nohup: ignoring input
Loading configuration from: config_cproxy.py
Target score methods for this run: ['c_proxy']
============================================================
Total experiments to run in this process: 8
============================================================

--- [1/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: c_proxy
  - unlearning_granularity: stage
  - forget_partition_ordering: easy_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[TRAIN] Original Model
    Epoch 1/30  34.16s
    Epoch 2/30  32.50s
    Epoch 3/30  31.57s
    Epoch 4/30  30.99s
    Epoch 5/30  31.19s
    Epoch 6/30  31.37s
    Epoch 7/30  30.86s
    Epoch 8/30  30.94s
    Epoch 9/30  31.30s
    Epoch 10/30  30.94s
    Epoch 11/30  30.83s
    Epoch 12/30  31.01s
    Epoch 13/30  31.37s
    Epoch 14/30  31.43s
    Epoch 15/30  30.94s
    Epoch 16/30  31.14s
    Epoch 17/30  31.04s
    Epoch 18/30  31.63s
    Epoch 19/30  30.79s
    Epoch 20/30  31.21s
    Epoch 21/30  30.92s
    Epoch 22/30  31.04s
    Epoch 23/30  30.49s
    Epoch 24/30  31.18s
    Epoch 25/30  30.43s
    Epoch 26/30  30.58s
    Epoch 27/30  31.51s
    Epoch 28/30  31.33s
    Epoch 29/30  30.86s
    Epoch 30/30  30.76s
[SAVE] saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'c_proxy' method...
Partition sizes for forget: [1666, 1666, 1668]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.001
Epoch: [0][99/189]	Loss 0.2640 (0.2861)	Accuracy 91.797 (90.074)	Time 17.52
train_accuracy 90.284
one epoch duration:33.02960777282715
Epoch #1, Learning rate: 0.001
Epoch: [1][99/189]	Loss 0.2774 (0.2775)	Accuracy 92.578 (90.301)	Time 17.86
train_accuracy 90.361
one epoch duration:33.20541596412659
Epoch #2, Learning rate: 0.001
Epoch: [2][99/189]	Loss 0.2415 (0.2755)	Accuracy 91.797 (90.336)	Time 17.47
train_accuracy 90.489
one epoch duration:33.18622326850891
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/189]	Loss 0.2985 (0.2696)	Accuracy 89.062 (90.547)	Time 17.76
train_accuracy 90.626
one epoch duration:33.16929006576538
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/189]	Loss 0.1650 (0.2705)	Accuracy 94.141 (90.559)	Time 17.40
train_accuracy 90.584
one epoch duration:32.73047471046448
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/189]	Loss 0.2937 (0.2662)	Accuracy 89.453 (90.625)	Time 17.05
train_accuracy 90.661
one epoch duration:32.33581495285034
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/189]	Loss 0.2677 (0.2668)	Accuracy 92.578 (90.676)	Time 17.28
train_accuracy 90.675
one epoch duration:32.56596040725708
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/189]	Loss 0.2294 (0.2713)	Accuracy 92.188 (90.594)	Time 17.21
train_accuracy 90.739
one epoch duration:32.508707761764526
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/189]	Loss 0.2837 (0.2591)	Accuracy 90.234 (91.148)	Time 17.15
train_accuracy 90.841
one epoch duration:32.49879550933838
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/189]	Loss 0.2336 (0.2619)	Accuracy 92.578 (90.988)	Time 17.15
train_accuracy 90.959
one epoch duration:32.502562046051025
[TRAIN] Retrain on 48334 samples
    Epoch 1/30  29.85s
    Epoch 2/30  29.76s
    Epoch 3/30  30.08s
    Epoch 4/30  29.80s
    Epoch 5/30  30.61s
    Epoch 6/30  30.35s
    Epoch 7/30  30.05s
    Epoch 8/30  29.95s
    Epoch 9/30  30.14s
    Epoch 10/30  29.70s
    Epoch 11/30  30.02s
    Epoch 12/30  31.41s
    Epoch 13/30  30.89s
    Epoch 14/30  30.31s
    Epoch 15/30  30.20s
    Epoch 16/30  29.69s
    Epoch 17/30  29.71s
    Epoch 18/30  29.77s
    Epoch 19/30  30.79s
    Epoch 20/30  30.39s
    Epoch 21/30  31.21s
    Epoch 22/30  31.00s
    Epoch 23/30  30.35s
    Epoch 24/30  30.89s
    Epoch 25/30  29.98s
    Epoch 26/30  29.58s
    Epoch 27/30  29.79s
    Epoch 28/30  29.47s
    Epoch 29/30  29.12s
    Epoch 30/30  30.52s
[SAVE] saved_models/retrain_35ecf4bdb13d611dd6e83f169dd5dfa96c8b0dc9_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S1 | Ftot=1666 | ΔF:+1.32 ΔR: 0.07 ΔT: 0.19 | MIA:0.4566 PredDiff:8.05%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.001
Epoch: [0][99/183]	Loss 0.2680 (0.2771)	Accuracy 88.281 (90.199)	Time 17.68
train_accuracy 90.297
one epoch duration:32.20920991897583
Epoch #1, Learning rate: 0.001
Epoch: [1][99/183]	Loss 0.3859 (0.2725)	Accuracy 88.672 (90.465)	Time 17.69
train_accuracy 90.469
one epoch duration:32.10583257675171
Epoch #2, Learning rate: 0.001
Epoch: [2][99/183]	Loss 0.2599 (0.2665)	Accuracy 90.625 (90.746)	Time 17.69
train_accuracy 90.602
one epoch duration:32.36805820465088
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/183]	Loss 0.2811 (0.2615)	Accuracy 88.672 (90.812)	Time 17.89
train_accuracy 90.773
one epoch duration:32.382328510284424
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/183]	Loss 0.2505 (0.2661)	Accuracy 91.406 (90.770)	Time 17.63
train_accuracy 90.855
one epoch duration:32.31236267089844
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/183]	Loss 0.2286 (0.2670)	Accuracy 91.016 (90.473)	Time 17.54
train_accuracy 90.602
one epoch duration:31.89552927017212
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/183]	Loss 0.2408 (0.2691)	Accuracy 91.797 (90.672)	Time 17.43
train_accuracy 90.657
one epoch duration:31.824661254882812
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/183]	Loss 0.2409 (0.2628)	Accuracy 91.406 (90.789)	Time 17.66
train_accuracy 90.782
one epoch duration:32.33910870552063
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/183]	Loss 0.2428 (0.2649)	Accuracy 91.406 (90.805)	Time 17.32
train_accuracy 91.011
one epoch duration:31.53842806816101
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/183]	Loss 0.3287 (0.2604)	Accuracy 90.234 (91.074)	Time 17.25
train_accuracy 91.022
one epoch duration:31.38000512123108
[TRAIN] Retrain on 46668 samples
    Epoch 1/30  29.38s
    Epoch 2/30  29.16s
    Epoch 3/30  29.44s
    Epoch 4/30  29.86s
    Epoch 5/30  29.06s
    Epoch 6/30  29.72s
    Epoch 7/30  30.01s
    Epoch 8/30  29.30s
    Epoch 9/30  29.25s
    Epoch 10/30  28.84s
    Epoch 11/30  29.22s
    Epoch 12/30  29.14s
    Epoch 13/30  29.03s
    Epoch 14/30  29.27s
    Epoch 15/30  29.30s
    Epoch 16/30  29.00s
    Epoch 17/30  29.27s
    Epoch 18/30  28.89s
    Epoch 19/30  29.10s
    Epoch 20/30  29.23s
    Epoch 21/30  29.00s
    Epoch 22/30  29.23s
    Epoch 23/30  29.48s
    Epoch 24/30  28.74s
    Epoch 25/30  28.85s
    Epoch 26/30  28.76s
    Epoch 27/30  29.42s
    Epoch 28/30  28.90s
    Epoch 29/30  29.07s
    Epoch 30/30  29.30s
[SAVE] saved_models/retrain_00980bb67d5861a4a3693ec2bec26edb0cf8b691_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S2 | Ftot=3332 | ΔF:+12.64 ΔR: 0.90 ΔT: 1.01 | MIA:0.4552 PredDiff:9.33%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2263 (0.2544)	Accuracy 91.016 (91.184)	Time 17.42
train_accuracy 91.393
one epoch duration:30.56117820739746
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.2931 (0.2395)	Accuracy 91.797 (91.578)	Time 17.37
train_accuracy 91.447
one epoch duration:30.574950218200684
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2275 (0.2402)	Accuracy 92.188 (91.484)	Time 17.29
train_accuracy 91.478
one epoch duration:30.342445611953735
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2947 (0.2348)	Accuracy 88.672 (91.824)	Time 17.22
train_accuracy 91.811
one epoch duration:30.305737257003784
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.1774 (0.2372)	Accuracy 93.359 (91.707)	Time 17.23
train_accuracy 91.816
one epoch duration:30.21712017059326
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.1739 (0.2347)	Accuracy 92.188 (91.852)	Time 17.30
train_accuracy 91.836
one epoch duration:30.420402765274048
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.2342 (0.2389)	Accuracy 91.016 (91.504)	Time 17.24
train_accuracy 91.722
one epoch duration:30.48606276512146
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.2506 (0.2339)	Accuracy 92.188 (91.801)	Time 17.44
train_accuracy 91.742
one epoch duration:30.521594047546387
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2486 (0.2301)	Accuracy 90.234 (91.871)	Time 17.26
train_accuracy 91.842
one epoch duration:30.469470024108887
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.2793 (0.2308)	Accuracy 89.062 (91.805)	Time 17.38
train_accuracy 91.902
one epoch duration:30.460238695144653
[TRAIN] Retrain on 45000 samples
    Epoch 1/30  27.89s
    Epoch 2/30  28.12s
    Epoch 3/30  27.77s
    Epoch 4/30  27.70s
    Epoch 5/30  28.15s
    Epoch 6/30  28.02s
    Epoch 7/30  27.87s
    Epoch 8/30  27.73s
    Epoch 9/30  28.09s
    Epoch 10/30  27.48s
    Epoch 11/30  27.65s
    Epoch 12/30  27.67s
    Epoch 13/30  28.24s
    Epoch 14/30  28.23s
    Epoch 15/30  28.01s
    Epoch 16/30  27.98s
    Epoch 17/30  27.93s
    Epoch 18/30  27.53s
    Epoch 19/30  27.84s
    Epoch 20/30  28.15s
    Epoch 21/30  28.18s
    Epoch 22/30  28.18s
    Epoch 23/30  28.13s
    Epoch 24/30  28.03s
    Epoch 25/30  27.61s
    Epoch 26/30  28.15s
    Epoch 27/30  28.32s
    Epoch 28/30  28.02s
    Epoch 29/30  28.20s
    Epoch 30/30  27.71s
[SAVE] saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S3 | Ftot=5000 | ΔF:+61.60 ΔR: 0.91 ΔT: 5.15 | MIA:0.4559 PredDiff:14.40%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/189]	Loss 0.9230 (0.9480)	Accuracy 89.453 (90.387)	Time 17.59
train_accuracy 90.400
one epoch duration:33.18537473678589
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/189]	Loss 0.7610 (0.8045)	Accuracy 91.406 (90.344)	Time 17.44
train_accuracy 90.431
one epoch duration:32.97048234939575
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/189]	Loss 0.6476 (0.6645)	Accuracy 90.625 (90.566)	Time 17.69
train_accuracy 90.487
one epoch duration:33.22640943527222
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/189]	Loss 0.5550 (0.5359)	Accuracy 89.844 (90.621)	Time 17.59
train_accuracy 90.489
one epoch duration:33.20110559463501
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/189]	Loss 0.3601 (0.4027)	Accuracy 91.016 (90.504)	Time 17.55
train_accuracy 90.481
one epoch duration:32.96830463409424
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/189]	Loss 0.2051 (0.2686)	Accuracy 93.750 (90.566)	Time 17.38
train_accuracy 90.439
one epoch duration:32.82090783119202
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/189]	Loss 0.2384 (0.2698)	Accuracy 90.625 (90.551)	Time 17.60
train_accuracy 90.632
one epoch duration:33.289491415023804
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/189]	Loss 0.3020 (0.2670)	Accuracy 89.844 (90.543)	Time 17.62
train_accuracy 90.530
one epoch duration:33.3749463558197
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/189]	Loss 0.3259 (0.2691)	Accuracy 90.234 (90.637)	Time 17.62
train_accuracy 90.636
one epoch duration:33.41490817070007
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/189]	Loss 0.3114 (0.2705)	Accuracy 91.016 (90.434)	Time 17.57
train_accuracy 90.574
one epoch duration:33.15212035179138
[LOAD] Retrain from saved_models/retrain_35ecf4bdb13d611dd6e83f169dd5dfa96c8b0dc9_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S1 | Ftot=1666 | ΔF:+1.32 ΔR: 0.12 ΔT: 0.43 | MIA:0.4609 PredDiff:8.14%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/183]	Loss 0.9261 (0.9375)	Accuracy 89.062 (90.422)	Time 18.04
train_accuracy 90.379
one epoch duration:32.853780031204224
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/183]	Loss 0.9252 (0.7973)	Accuracy 86.328 (90.582)	Time 17.64
train_accuracy 90.540
one epoch duration:32.09631562232971
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/183]	Loss 0.6890 (0.6704)	Accuracy 89.453 (90.480)	Time 17.39
train_accuracy 90.428
one epoch duration:31.708890438079834
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/183]	Loss 0.5645 (0.5315)	Accuracy 89.062 (90.535)	Time 17.12
train_accuracy 90.259
one epoch duration:31.745927095413208
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/183]	Loss 0.3769 (0.4068)	Accuracy 91.016 (90.387)	Time 17.34
train_accuracy 90.392
one epoch duration:31.72809910774231
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/183]	Loss 0.2492 (0.2702)	Accuracy 89.062 (90.512)	Time 17.47
train_accuracy 90.602
one epoch duration:31.95142960548401
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/183]	Loss 0.2919 (0.2656)	Accuracy 90.625 (90.688)	Time 17.44
train_accuracy 90.591
one epoch duration:31.967159509658813
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/183]	Loss 0.2436 (0.2681)	Accuracy 91.406 (90.520)	Time 17.53
train_accuracy 90.535
one epoch duration:32.05116534233093
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/183]	Loss 0.2515 (0.2670)	Accuracy 90.625 (90.621)	Time 17.53
train_accuracy 90.561
one epoch duration:31.7838134765625
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/183]	Loss 0.2912 (0.2710)	Accuracy 89.844 (90.508)	Time 17.33
train_accuracy 90.580
one epoch duration:31.924018621444702
[LOAD] Retrain from saved_models/retrain_00980bb67d5861a4a3693ec2bec26edb0cf8b691_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S2 | Ftot=3332 | ΔF:+12.64 ΔR: 0.58 ΔT: 0.96 | MIA:0.4586 PredDiff:9.43%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.8809 (0.9190)	Accuracy 92.578 (90.934)	Time 17.62
train_accuracy 91.082
one epoch duration:30.76797318458557
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.8360 (0.7779)	Accuracy 87.500 (91.180)	Time 17.70
train_accuracy 91.347
one epoch duration:30.982184171676636
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.6140 (0.6428)	Accuracy 91.797 (91.418)	Time 17.46
train_accuracy 91.589
one epoch duration:30.73371982574463
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.4898 (0.5082)	Accuracy 90.234 (91.348)	Time 17.56
train_accuracy 91.489
one epoch duration:30.845908641815186
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.3395 (0.3733)	Accuracy 92.969 (91.414)	Time 17.51
train_accuracy 91.533
one epoch duration:30.828701972961426
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2153 (0.2431)	Accuracy 91.797 (91.367)	Time 17.52
train_accuracy 91.442
one epoch duration:30.833842754364014
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2777 (0.2390)	Accuracy 88.281 (91.793)	Time 17.50
train_accuracy 91.673
one epoch duration:30.87501358985901
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.1932 (0.2414)	Accuracy 94.922 (91.488)	Time 17.52
train_accuracy 91.567
one epoch duration:30.846789360046387
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2496 (0.2406)	Accuracy 90.625 (91.648)	Time 17.61
train_accuracy 91.418
one epoch duration:30.977314949035645
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2506 (0.2420)	Accuracy 90.625 (91.609)	Time 17.60
train_accuracy 91.502
one epoch duration:30.980003118515015
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S3 | Ftot=5000 | ΔF:+70.70 ΔR: 0.46 ΔT: 5.73 | MIA:0.4541 PredDiff:15.07%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 14.346
one epoch duration:1.1426782608032227
Epoch #1, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 14.406
one epoch duration:1.1446640491485596
Epoch #2, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 15.366
one epoch duration:1.1226160526275635
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 13.685
one epoch duration:1.1212377548217773
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.286
one epoch duration:1.1331379413604736
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.346
one epoch duration:1.1354234218597412
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.766
one epoch duration:1.1226890087127686
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 13.806
one epoch duration:1.1183242797851562
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.586
one epoch duration:1.1182153224945068
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 13.866
one epoch duration:1.118645191192627
[LOAD] Retrain from saved_models/retrain_35ecf4bdb13d611dd6e83f169dd5dfa96c8b0dc9_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S1 | Ftot=1666 | ΔF:-79.35 ΔR:17.02 ΔT:15.20 | MIA:0.4196 PredDiff:26.21%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 15.216
one epoch duration:2.2990026473999023
Epoch #1, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 15.246
one epoch duration:2.288405179977417
Epoch #2, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 14.946
one epoch duration:2.2888376712799072
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 15.066
one epoch duration:2.2843661308288574
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 15.816
one epoch duration:2.288883686065674
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.826
one epoch duration:2.2927730083465576
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.406
one epoch duration:2.29020357131958
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.646
one epoch duration:2.2837116718292236
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 15.186
one epoch duration:2.290708541870117
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 15.186
one epoch duration:2.2804007530212402
[LOAD] Retrain from saved_models/retrain_00980bb67d5861a4a3693ec2bec26edb0cf8b691_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S2 | Ftot=3332 | ΔF:-69.03 ΔR:12.53 ΔT:12.72 | MIA:0.4183 PredDiff:24.44%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.720
one epoch duration:3.380723237991333
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.020
one epoch duration:3.3778584003448486
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 14.440
one epoch duration:3.387441635131836
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.200
one epoch duration:3.3420755863189697
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.800
one epoch duration:3.361311912536621
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.580
one epoch duration:3.3522398471832275
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.620
one epoch duration:3.3299717903137207
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.320
one epoch duration:3.333651065826416
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.140
one epoch duration:3.334129810333252
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.880
one epoch duration:3.3334784507751465
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S3 | Ftot=5000 | ΔF:+16.42 ΔR:10.08 ΔT: 5.22 | MIA:0.4146 PredDiff:22.41%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [0][99/189]	Loss -0.3778 (-0.3317)	Accuracy 92.188 (90.379)	Time 21.79
train_accuracy 90.361
one epoch duration:40.40634489059448
Epoch #1, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [1][99/189]	Loss -0.4294 (-0.4120)	Accuracy 89.844 (90.555)	Time 21.76
train_accuracy 90.392
one epoch duration:40.437501192092896
Epoch #2, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [2][99/189]	Loss -0.4837 (-0.4855)	Accuracy 90.234 (90.383)	Time 21.55
train_accuracy 90.233
one epoch duration:39.8975772857666
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [3][99/189]	Loss -0.6361 (-0.5590)	Accuracy 92.188 (90.047)	Time 21.51
train_accuracy 90.028
one epoch duration:39.878785133361816
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [4][99/189]	Loss -0.6853 (-0.6310)	Accuracy 89.844 (89.758)	Time 21.44
train_accuracy 89.860
one epoch duration:39.79904222488403
[LOAD] Retrain from saved_models/retrain_35ecf4bdb13d611dd6e83f169dd5dfa96c8b0dc9_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S1 | Ftot=1666 | ΔF:-31.51 ΔR: 9.77 ΔT: 8.21 | MIA:0.4577 PredDiff:17.84%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [0][99/183]	Loss -0.7868 (-0.7059)	Accuracy 91.016 (90.055)	Time 22.41
train_accuracy 89.817
one epoch duration:39.53997325897217
Epoch #1, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [1][99/183]	Loss -0.7362 (-0.7830)	Accuracy 88.281 (89.781)	Time 22.36
train_accuracy 89.912
one epoch duration:39.490134477615356
Epoch #2, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [2][99/183]	Loss -0.8748 (-0.8744)	Accuracy 90.234 (89.734)	Time 22.38
train_accuracy 89.522
one epoch duration:39.5194046497345
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [3][99/183]	Loss -1.0196 (-0.9504)	Accuracy 89.062 (89.535)	Time 22.61
train_accuracy 89.550
one epoch duration:40.11065411567688
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [4][99/183]	Loss -1.1114 (-1.0470)	Accuracy 91.797 (89.922)	Time 22.75
train_accuracy 89.633
one epoch duration:40.135276317596436
[LOAD] Retrain from saved_models/retrain_00980bb67d5861a4a3693ec2bec26edb0cf8b691_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S2 | Ftot=3332 | ΔF:-49.76 ΔR: 7.49 ΔT: 7.98 | MIA:0.4564 PredDiff:18.63%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -1.1662 (-1.1453)	Accuracy 92.188 (90.527)	Time 23.63
train_accuracy 90.627
one epoch duration:39.66928553581238
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -1.3249 (-1.2630)	Accuracy 90.625 (90.766)	Time 23.64
train_accuracy 90.589
one epoch duration:39.76464629173279
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -1.3722 (-1.3578)	Accuracy 91.797 (90.406)	Time 23.59
train_accuracy 90.484
one epoch duration:39.5995934009552
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -1.5297 (-1.4767)	Accuracy 90.625 (90.711)	Time 23.15
train_accuracy 90.709
one epoch duration:38.93400573730469
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -1.6782 (-1.5811)	Accuracy 93.750 (90.758)	Time 23.23
train_accuracy 90.580
one epoch duration:39.01834177970886
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S3 | Ftot=5000 | ΔF:+3.54 ΔR: 5.03 ΔT: 3.07 | MIA:0.4528 PredDiff:16.32%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.3575 (0.5021)	Accuracy 91.406 (87.445)	Time 16.20
Epoch: [0][199/196]	Loss 0.4598 (0.4512)	Accuracy 88.281 (87.350)	Time 17.37
one epoch duration:46.896939754486084
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.4345 (0.3945)	Accuracy 85.547 (87.462)	Time 16.21
Epoch: [1][199/196]	Loss 0.2822 (0.3899)	Accuracy 91.406 (87.613)	Time 17.34
one epoch duration:34.089807748794556
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.4210 (0.3810)	Accuracy 86.719 (87.664)	Time 15.97
Epoch: [2][199/196]	Loss 0.4311 (0.3819)	Accuracy 85.156 (87.694)	Time 18.11
one epoch duration:34.62274479866028
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.3573 (0.3728)	Accuracy 87.891 (87.723)	Time 16.16
Epoch: [3][199/196]	Loss 0.3679 (0.3739)	Accuracy 87.891 (87.834)	Time 17.41
one epoch duration:34.10792112350464
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.4263 (0.3779)	Accuracy 87.109 (88.063)	Time 16.20
Epoch: [4][199/196]	Loss 0.3455 (0.3748)	Accuracy 89.453 (88.073)	Time 17.34
one epoch duration:34.077271461486816
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.4040 (0.3700)	Accuracy 87.500 (88.214)	Time 16.14
Epoch: [5][199/196]	Loss 0.3398 (0.3751)	Accuracy 88.281 (87.994)	Time 17.35
one epoch duration:34.02945566177368
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.4248 (0.3721)	Accuracy 85.547 (88.046)	Time 16.15
Epoch: [6][199/196]	Loss 0.3493 (0.3740)	Accuracy 88.281 (87.885)	Time 17.39
one epoch duration:34.07719612121582
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.3290 (0.3715)	Accuracy 89.844 (88.260)	Time 16.18
Epoch: [7][199/196]	Loss 0.3798 (0.3771)	Accuracy 84.766 (87.978)	Time 17.32
one epoch duration:34.03113317489624
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.3317 (0.3706)	Accuracy 90.234 (88.122)	Time 15.97
Epoch: [8][199/196]	Loss 0.2974 (0.3712)	Accuracy 88.281 (88.109)	Time 17.15
one epoch duration:33.64508652687073
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.3680 (0.3819)	Accuracy 87.109 (87.660)	Time 16.13
Epoch: [9][199/196]	Loss 0.3268 (0.3732)	Accuracy 90.234 (87.868)	Time 17.39
one epoch duration:34.06705570220947
[LOAD] Retrain from saved_models/retrain_35ecf4bdb13d611dd6e83f169dd5dfa96c8b0dc9_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S1 | Ftot=1666 | ΔF:+1.26 ΔR: 0.08 ΔT: 0.37 | MIA:0.7853 PredDiff:8.22%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.001
Epoch: [0][99/197]	Loss 0.4113 (0.4636)	Accuracy 87.500 (84.902)	Time 15.04
Epoch: [0][199/197]	Loss 0.4413 (0.4500)	Accuracy 84.766 (85.047)	Time 17.39
one epoch duration:34.183765172958374
Epoch #1, Learning rate: 0.001
Epoch: [1][99/197]	Loss 0.5156 (0.4426)	Accuracy 82.422 (84.902)	Time 14.89
Epoch: [1][199/197]	Loss 0.3693 (0.4433)	Accuracy 87.891 (84.908)	Time 17.39
one epoch duration:34.04198431968689
Epoch #2, Learning rate: 0.001
Epoch: [2][99/197]	Loss 0.4133 (0.4398)	Accuracy 85.938 (85.383)	Time 14.98
Epoch: [2][199/197]	Loss 0.4173 (0.4438)	Accuracy 86.328 (85.118)	Time 17.40
one epoch duration:34.14741516113281
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/197]	Loss 0.4136 (0.4429)	Accuracy 86.328 (84.956)	Time 14.91
Epoch: [3][199/197]	Loss 0.4771 (0.4415)	Accuracy 83.203 (84.875)	Time 17.35
one epoch duration:34.037436962127686
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/197]	Loss 0.4017 (0.4380)	Accuracy 85.938 (85.152)	Time 14.97
Epoch: [4][199/197]	Loss 0.4102 (0.4363)	Accuracy 84.375 (85.173)	Time 17.39
one epoch duration:34.11549997329712
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/197]	Loss 0.3961 (0.4332)	Accuracy 86.719 (85.415)	Time 14.95
Epoch: [5][199/197]	Loss 0.5556 (0.4351)	Accuracy 79.688 (85.257)	Time 17.47
one epoch duration:34.18743586540222
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/197]	Loss 0.4155 (0.4309)	Accuracy 85.547 (85.288)	Time 14.97
Epoch: [6][199/197]	Loss 0.3911 (0.4344)	Accuracy 86.719 (85.173)	Time 17.59
one epoch duration:34.32523036003113
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/197]	Loss 0.3685 (0.4472)	Accuracy 87.109 (84.961)	Time 14.93
Epoch: [7][199/197]	Loss 0.3877 (0.4349)	Accuracy 86.328 (85.299)	Time 17.15
one epoch duration:33.8089804649353
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/197]	Loss 0.5246 (0.4269)	Accuracy 82.422 (85.420)	Time 14.77
Epoch: [8][199/197]	Loss 0.5311 (0.4316)	Accuracy 80.078 (85.253)	Time 17.18
one epoch duration:33.683786153793335
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/197]	Loss 0.3688 (0.4361)	Accuracy 87.891 (85.174)	Time 14.72
Epoch: [9][199/197]	Loss 0.3962 (0.4361)	Accuracy 85.156 (85.156)	Time 17.16
one epoch duration:33.641191482543945
[LOAD] Retrain from saved_models/retrain_00980bb67d5861a4a3693ec2bec26edb0cf8b691_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S2 | Ftot=3332 | ΔF:+5.73 ΔR: 0.52 ΔT: 0.54 | MIA:0.7950 PredDiff:9.93%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.4503 (0.4996)	Accuracy 84.766 (83.096)	Time 14.05
Epoch: [0][199/196]	Loss 0.4480 (0.4886)	Accuracy 83.984 (83.251)	Time 17.48
one epoch duration:35.3121395111084
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.4226 (0.4842)	Accuracy 84.766 (83.257)	Time 14.01
Epoch: [1][199/196]	Loss 0.4730 (0.4782)	Accuracy 85.547 (83.383)	Time 17.25
one epoch duration:35.26839900016785
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.4019 (0.4672)	Accuracy 87.500 (83.457)	Time 13.85
Epoch: [2][199/196]	Loss 0.4840 (0.4690)	Accuracy 82.422 (83.479)	Time 17.57
one epoch duration:35.18907403945923
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.4032 (0.4740)	Accuracy 85.547 (83.481)	Time 13.97
Epoch: [3][199/196]	Loss 0.4417 (0.4719)	Accuracy 84.766 (83.474)	Time 17.43
one epoch duration:34.202903747558594
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.5386 (0.4659)	Accuracy 78.906 (83.560)	Time 14.17
Epoch: [4][199/196]	Loss 0.5378 (0.4646)	Accuracy 81.250 (83.757)	Time 17.46
one epoch duration:34.458889961242676
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.4959 (0.4654)	Accuracy 83.984 (83.594)	Time 13.92
Epoch: [5][199/196]	Loss 0.5070 (0.4654)	Accuracy 82.422 (83.609)	Time 17.52
one epoch duration:34.25369668006897
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.4985 (0.4708)	Accuracy 82.422 (83.511)	Time 13.97
Epoch: [6][199/196]	Loss 0.4262 (0.4680)	Accuracy 83.594 (83.581)	Time 17.47
one epoch duration:34.25303268432617
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.4881 (0.4597)	Accuracy 82.812 (83.789)	Time 13.97
Epoch: [7][199/196]	Loss 0.4593 (0.4645)	Accuracy 81.641 (83.709)	Time 17.46
one epoch duration:34.2300283908844
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.4521 (0.4654)	Accuracy 82.031 (83.599)	Time 14.05
Epoch: [8][199/196]	Loss 0.4148 (0.4630)	Accuracy 86.328 (83.687)	Time 17.43
one epoch duration:34.28209590911865
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.4625 (0.4577)	Accuracy 83.984 (84.004)	Time 13.92
Epoch: [9][199/196]	Loss 0.4960 (0.4601)	Accuracy 82.812 (83.900)	Time 17.43
one epoch duration:34.15779519081116
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S3 | Ftot=5000 | ΔF:+1.82 ΔR: 0.80 ΔT: 0.19 | MIA:0.4515 PredDiff:12.97%

===== Running Method: Wfisher =====
  > Applied specific params for Wfisher: {'alpha': 10.0}

[UNLEARN] Stage 1: |Forget Total|=1666
[LOAD] Retrain from saved_models/retrain_35ecf4bdb13d611dd6e83f169dd5dfa96c8b0dc9_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S1 | Ftot=1666 | ΔF:-92.68 ΔR:26.87 ΔT:24.11 | MIA:0.4510 PredDiff:36.12%

[UNLEARN] Stage 2: |Forget Total|=3332
[LOAD] Retrain from saved_models/retrain_00980bb67d5861a4a3693ec2bec26edb0cf8b691_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S2 | Ftot=3332 | ΔF:-87.36 ΔR:81.28 ΔT:72.86 | MIA:0.5000 PredDiff:89.72%

[UNLEARN] Stage 3: |Forget Total|=5000
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S3 | Ftot=5000 | ΔF:+100.00 ΔR:92.92 ΔT:66.20 | MIA:0.0000 PredDiff:100.00%

===== Running Method: SCRUB =====
  > Applied specific params for SCRUB: {'unlearn_epochs': 10, 'kd_T': 4.0, 'gamma': 1.0, 'beta': 1.0, 'msteps': 5, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [0][6/7]	Time 0.103 (0.166)	Data 0.067 (0.121)	Loss -17.5195 (-15.3413)	Forget_Acc@1 8.462 (12.485)
*** Minimize step ***
Epoch: [0][188/189]	Time 0.149 (0.176)	Data 0.108 (0.129)	Loss 0.4192 (0.4424)	Retain_Acc@1 86.893 (89.817)
Epoch: [0]	 train-acc:	89.8166921829247	 train-loss: 0.44238254486979295
one epoch duration:34.438340187072754
Epoch #1, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [1][6/7]	Time 0.099 (0.160)	Data 0.066 (0.117)	Loss -19.8106 (-18.0414)	Forget_Acc@1 9.231 (10.204)
*** Minimize step ***
Epoch: [1][188/189]	Time 0.155 (0.172)	Data 0.110 (0.126)	Loss 0.3969 (0.4914)	Retain_Acc@1 94.660 (90.135)
Epoch: [1]	 train-acc:	90.13530847945083	 train-loss: 0.49143405440364996
one epoch duration:33.70057511329651
Epoch #2, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [2][6/7]	Time 0.097 (0.162)	Data 0.065 (0.116)	Loss -20.6367 (-19.0415)	Forget_Acc@1 10.769 (9.244)
*** Minimize step ***
Epoch: [2][188/189]	Time 0.149 (0.176)	Data 0.107 (0.129)	Loss 0.3689 (0.4964)	Retain_Acc@1 87.864 (89.951)
Epoch: [2]	 train-acc:	89.95117307463997	 train-loss: 0.49637389227439005
one epoch duration:34.387407302856445
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [3][6/7]	Time 0.102 (0.165)	Data 0.069 (0.120)	Loss -19.0222 (-19.1525)	Forget_Acc@1 9.231 (9.244)
*** Minimize step ***
Epoch: [3][188/189]	Time 0.158 (0.176)	Data 0.110 (0.128)	Loss 0.3120 (0.3601)	Retain_Acc@1 94.175 (90.549)
Epoch: [3]	 train-acc:	90.54909588148494	 train-loss: 0.36014060120857433
one epoch duration:34.47699952125549
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [4][6/7]	Time 0.107 (0.168)	Data 0.069 (0.120)	Loss -19.5802 (-19.7373)	Forget_Acc@1 6.154 (9.964)
*** Minimize step ***
Epoch: [4][188/189]	Time 0.153 (0.177)	Data 0.107 (0.128)	Loss 0.3249 (0.3694)	Retain_Acc@1 91.748 (90.357)
Epoch: [4]	 train-acc:	90.35668473980269	 train-loss: 0.3693817859649579
one epoch duration:34.608088970184326
Epoch #5, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [5][6/7]	Time 0.103 (0.164)	Data 0.067 (0.118)	Loss -20.1514 (-20.2104)	Forget_Acc@1 6.154 (8.523)
*** Minimize step ***
Epoch: [5][188/189]	Time 0.166 (0.174)	Data 0.123 (0.127)	Loss 0.3933 (0.3775)	Retain_Acc@1 91.262 (90.586)
Epoch: [5]	 train-acc:	90.58633675060082	 train-loss: 0.3775024139704049
one epoch duration:33.99498414993286
Epoch #6, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [6][188/189]	Time 0.152 (0.174)	Data 0.107 (0.127)	Loss 0.3204 (0.3588)	Retain_Acc@1 93.689 (90.568)
Epoch: [6]	 train-acc:	90.5677162877882	 train-loss: 0.3587887432310141
one epoch duration:32.915369510650635
Epoch #7, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [7][188/189]	Time 0.151 (0.173)	Data 0.106 (0.126)	Loss 0.3389 (0.3458)	Retain_Acc@1 90.777 (90.653)
Epoch: [7]	 train-acc:	90.6525427074482	 train-loss: 0.3458163191901541
one epoch duration:32.63823747634888
Epoch #8, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [8][188/189]	Time 0.155 (0.174)	Data 0.109 (0.127)	Loss 0.3420 (0.3353)	Retain_Acc@1 89.806 (90.729)
Epoch: [8]	 train-acc:	90.72909338771126	 train-loss: 0.33533618052167463
one epoch duration:32.85456705093384
Epoch #9, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [9][188/189]	Time 0.155 (0.175)	Data 0.109 (0.128)	Loss 0.3882 (0.3389)	Retain_Acc@1 89.806 (90.694)
Epoch: [9]	 train-acc:	90.6939214590482	 train-loss: 0.33887464837027204
one epoch duration:33.06409978866577
[LOAD] Retrain from saved_models/retrain_35ecf4bdb13d611dd6e83f169dd5dfa96c8b0dc9_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S1 | Ftot=1666 | ΔF:+1.32 ΔR: 0.13 ΔT: 0.35 | MIA:0.4594 PredDiff:8.24%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [0][13/14]	Time 0.046 (0.168)	Data 0.015 (0.121)	Loss -29.3700 (-19.6732)	Forget_Acc@1 0.000 (8.313)
*** Minimize step ***
Epoch: [0][182/183]	Time 0.079 (0.177)	Data 0.046 (0.129)	Loss 0.6962 (0.8837)	Retain_Acc@1 88.158 (88.922)
Epoch: [0]	 train-acc:	88.92174508580412	 train-loss: 0.8836689357474744
one epoch duration:34.7452507019043
Epoch #1, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [1][13/14]	Time 0.048 (0.166)	Data 0.016 (0.120)	Loss -38.5973 (-26.8532)	Forget_Acc@1 0.000 (6.182)
*** Minimize step ***
Epoch: [1][182/183]	Time 0.077 (0.177)	Data 0.046 (0.129)	Loss 0.5756 (1.4273)	Retain_Acc@1 93.421 (87.730)
Epoch: [1]	 train-acc:	87.73035055879681	 train-loss: 1.4273033424761052
one epoch duration:34.650473833084106
Epoch #2, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [2][13/14]	Time 0.045 (0.172)	Data 0.011 (0.125)	Loss -46.6015 (-32.9041)	Forget_Acc@1 0.000 (6.573)
*** Minimize step ***
Epoch: [2][182/183]	Time 0.081 (0.178)	Data 0.047 (0.130)	Loss 0.6413 (1.5248)	Retain_Acc@1 88.158 (87.578)
Epoch: [2]	 train-acc:	87.57821204389103	 train-loss: 1.5248087298300963
one epoch duration:34.915807247161865
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [3][13/14]	Time 0.048 (0.167)	Data 0.017 (0.121)	Loss -36.5020 (-34.5851)	Forget_Acc@1 25.000 (7.473)
*** Minimize step ***
Epoch: [3][182/183]	Time 0.076 (0.176)	Data 0.045 (0.129)	Loss 0.7205 (0.7619)	Retain_Acc@1 88.158 (89.100)
Epoch: [3]	 train-acc:	89.0995971471738	 train-loss: 0.7618615344873051
one epoch duration:34.55174541473389
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [4][13/14]	Time 0.054 (0.168)	Data 0.012 (0.120)	Loss -46.5754 (-40.0567)	Forget_Acc@1 0.000 (6.423)
*** Minimize step ***
Epoch: [4][182/183]	Time 0.076 (0.173)	Data 0.044 (0.126)	Loss 0.8663 (0.7569)	Retain_Acc@1 92.105 (89.468)
Epoch: [4]	 train-acc:	89.46815805197313	 train-loss: 0.7569497245345291
one epoch duration:33.999353647232056
Epoch #5, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [5][13/14]	Time 0.044 (0.162)	Data 0.014 (0.118)	Loss -49.0001 (-44.1170)	Forget_Acc@1 0.000 (5.492)
*** Minimize step ***
Epoch: [5][182/183]	Time 0.077 (0.173)	Data 0.045 (0.127)	Loss 0.7805 (0.7930)	Retain_Acc@1 81.579 (89.074)
Epoch: [5]	 train-acc:	89.07388360594133	 train-loss: 0.7929695433492147
one epoch duration:33.95614838600159
Epoch #6, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [6][182/183]	Time 0.079 (0.173)	Data 0.046 (0.127)	Loss 0.8947 (0.6416)	Retain_Acc@1 90.789 (89.472)
Epoch: [6]	 train-acc:	89.47244364577516	 train-loss: 0.641629237263023
one epoch duration:31.67422604560852
Epoch #7, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [7][182/183]	Time 0.078 (0.174)	Data 0.044 (0.128)	Loss 0.5770 (0.5832)	Retain_Acc@1 94.737 (89.742)
Epoch: [7]	 train-acc:	89.74243592582448	 train-loss: 0.5831537981199668
one epoch duration:31.929516315460205
Epoch #8, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [8][182/183]	Time 0.076 (0.175)	Data 0.044 (0.128)	Loss 0.5743 (0.5429)	Retain_Acc@1 89.474 (89.757)
Epoch: [8]	 train-acc:	89.75743550511245	 train-loss: 0.5428823081480667
one epoch duration:31.971163988113403
Epoch #9, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [9][182/183]	Time 0.076 (0.174)	Data 0.044 (0.127)	Loss 0.5804 (0.5165)	Retain_Acc@1 92.105 (89.888)
Epoch: [9]	 train-acc:	89.88814605231599	 train-loss: 0.5165142407298364
one epoch duration:31.898574590682983
[LOAD] Retrain from saved_models/retrain_00980bb67d5861a4a3693ec2bec26edb0cf8b691_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S2 | Ftot=3332 | ΔF:+11.82 ΔR: 0.26 ΔT: 0.94 | MIA:0.4540 PredDiff:9.96%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [0][19/20]	Time 0.103 (0.179)	Data 0.070 (0.131)	Loss -82.8840 (-51.8240)	Forget_Acc@1 0.000 (2.780)
*** Minimize step ***
Epoch: [0][175/176]	Time 0.148 (0.184)	Data 0.104 (0.135)	Loss 0.7106 (3.6595)	Retain_Acc@1 90.500 (83.407)
Epoch: [0]	 train-acc:	83.40666666666667	 train-loss: 3.6595195072068107
one epoch duration:36.0322949886322
Epoch #1, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [1][19/20]	Time 0.102 (0.168)	Data 0.068 (0.122)	Loss -128.5922 (-79.9748)	Forget_Acc@1 0.000 (0.920)
*** Minimize step ***
Epoch: [1][175/176]	Time 0.151 (0.175)	Data 0.105 (0.128)	Loss 1.5858 (7.2667)	Retain_Acc@1 88.500 (76.882)
Epoch: [1]	 train-acc:	76.88222222222223	 train-loss: 7.266659877522787
one epoch duration:34.196463108062744
Epoch #2, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [2][19/20]	Time 0.103 (0.169)	Data 0.070 (0.124)	Loss -179.0157 (-108.6672)	Forget_Acc@1 0.000 (0.060)
*** Minimize step ***
Epoch: [2][175/176]	Time 0.163 (0.180)	Data 0.115 (0.132)	Loss 2.6229 (8.3420)	Retain_Acc@1 81.500 (72.067)
Epoch: [2]	 train-acc:	72.06666666666666	 train-loss: 8.342015179994371
one epoch duration:35.14821219444275
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [3][19/20]	Time 0.104 (0.184)	Data 0.070 (0.135)	Loss -163.2171 (-133.6699)	Forget_Acc@1 0.000 (0.100)
*** Minimize step ***
Epoch: [3][175/176]	Time 0.150 (0.176)	Data 0.107 (0.128)	Loss 3.0086 (6.0963)	Retain_Acc@1 86.000 (77.844)
Epoch: [3]	 train-acc:	77.84444444444445	 train-loss: 6.096297526465522
one epoch duration:34.57258105278015
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [4][19/20]	Time 0.103 (0.170)	Data 0.070 (0.123)	Loss -178.9903 (-158.0593)	Forget_Acc@1 0.000 (0.220)
*** Minimize step ***
Epoch: [4][175/176]	Time 0.147 (0.175)	Data 0.103 (0.128)	Loss 3.4138 (4.6494)	Retain_Acc@1 83.500 (80.753)
Epoch: [4]	 train-acc:	80.75333333333333	 train-loss: 4.649386176808675
one epoch duration:34.24752068519592
Epoch #5, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [5][19/20]	Time 0.100 (0.165)	Data 0.068 (0.122)	Loss -189.2111 (-180.6129)	Forget_Acc@1 0.000 (0.180)
*** Minimize step ***
Epoch: [5][175/176]	Time 0.148 (0.173)	Data 0.104 (0.128)	Loss 3.4306 (4.1379)	Retain_Acc@1 80.500 (81.584)
Epoch: [5]	 train-acc:	81.58444444444444	 train-loss: 4.137895200305515
one epoch duration:33.755828857421875
Epoch #6, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [6][175/176]	Time 0.148 (0.175)	Data 0.104 (0.128)	Loss 3.0418 (3.0197)	Retain_Acc@1 89.500 (83.122)
Epoch: [6]	 train-acc:	83.12222222222222	 train-loss: 3.01968994462755
one epoch duration:30.736992120742798
Epoch #7, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [7][175/176]	Time 0.151 (0.172)	Data 0.105 (0.127)	Loss 2.6158 (2.7328)	Retain_Acc@1 86.000 (83.322)
Epoch: [7]	 train-acc:	83.32222222222222	 train-loss: 2.7327907012091743
one epoch duration:30.316409587860107
Epoch #8, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [8][175/176]	Time 0.143 (0.171)	Data 0.103 (0.125)	Loss 2.2581 (2.5054)	Retain_Acc@1 84.500 (83.673)
Epoch: [8]	 train-acc:	83.67333333333333	 train-loss: 2.5053904758877223
one epoch duration:30.15393042564392
Epoch #9, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [9][175/176]	Time 0.146 (0.174)	Data 0.103 (0.127)	Loss 2.7557 (2.3419)	Retain_Acc@1 83.500 (84.016)
Epoch: [9]	 train-acc:	84.01555555555555	 train-loss: 2.3419372457292345
one epoch duration:30.565670490264893
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S3 | Ftot=5000 | ΔF:+3.22 ΔR: 6.45 ΔT: 2.81 | MIA:0.4566 PredDiff:16.69%

===== Full Results =====
 method  stage  forget_total  Retrain_F  Retrain_R  Retrain_T  Unlearn_F  Unlearn_R  Unlearn_T         ΔF        ΔR    ΔT      MIA  PredDiff(%)
     FT      1          1666  98.679472  92.785617      84.18 100.000000  92.860098      83.99   1.320528  0.074482  0.19 0.456635        8.050
     FT      2          3332  87.364946  91.996657      82.86 100.000000  92.894489      83.87  12.635054  0.897831  1.01 0.455151        9.334
     FT      3          5000   0.000000  92.920000      76.20  61.600000  93.831111      81.35  61.600000  0.911111  5.15 0.455867       14.400
  FT_l1      1          1666  98.679472  92.785617      84.18 100.000000  92.665618      83.75   1.320528  0.119998  0.43 0.460949        8.142
  FT_l1      2          3332  87.364946  91.996657      82.86 100.000000  92.573069      83.82  12.635054  0.576412  0.96 0.458590        9.434
  FT_l1      3          5000   0.000000  92.920000      76.20  70.700000  93.382222      81.93  70.700000  0.462222  5.73 0.454067       15.072
     GA      1          1666  98.679472  92.785617      84.18  19.327731  75.766541      68.98 -79.351741 17.019076 15.20 0.419572       26.208
     GA      2          3332  87.364946  91.996657      82.86  18.337335  79.467729      70.14 -69.027611 12.528928 12.72 0.418253       24.436
     GA      3          5000   0.000000  92.920000      76.20  16.420000  82.842222      70.98  16.420000 10.077778  5.22 0.414611       22.408
     NG      1          1666  98.679472  92.785617      84.18  67.166867  83.011958      75.97 -31.512605  9.773658  8.21 0.457701       17.838
     NG      2          3332  87.364946  91.996657      82.86  37.605042  84.505443      74.88 -49.759904  7.491215  7.98 0.456384       18.628
     NG      3          5000   0.000000  92.920000      76.20   3.540000  87.893333      73.13   3.540000  5.026667  3.07 0.452822       16.318
     RL      1          1666  98.679472  92.785617      84.18  99.939976  92.702859      83.81   1.260504  0.082757  0.37 0.785314        8.220
     RL      2          3332  87.364946  91.996657      82.86  93.097239  92.517357      83.40   5.732293  0.520699  0.54 0.794953        9.930
     RL      3          5000   0.000000  92.920000      76.20   1.820000  93.720000      76.39   1.820000  0.800000  0.19 0.451500       12.974
Wfisher      1          1666  98.679472  92.785617      84.18   6.002401  65.916332      60.07 -92.677071 26.869285 24.11 0.451041       36.120
Wfisher      2          3332  87.364946  91.996657      82.86   0.000000  10.713980      10.00 -87.364946 81.282678 72.86 0.500000       89.720
Wfisher      3          5000   0.000000  92.920000      76.20 100.000000   0.000000      10.00 100.000000 92.920000 66.20 0.000000      100.000
  SCRUB      1          1666  98.679472  92.785617      84.18 100.000000  92.657343      83.83   1.320528  0.128274  0.35 0.459428        8.236
  SCRUB      2          3332  87.364946  91.996657      82.86  99.189676  91.737379      83.80  11.824730  0.259278  0.94 0.454037        9.958
  SCRUB      3          5000   0.000000  92.920000      76.20   3.220000  86.468889      73.39   3.220000  6.451111  2.81 0.456567       16.692

Results saved to saved_models/results_class_c_proxy_stage_easy_first.csv

--- [1/8] Experiment FINISHED ---
============================================================

--- [2/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: c_proxy
  - unlearning_granularity: stage
  - forget_partition_ordering: hard_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[LOAD] Original model from saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'c_proxy' method...
Partition sizes for forget: [1668, 1666, 1666]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.001
Epoch: [0][99/189]	Loss 0.2263 (0.2617)	Accuracy 92.188 (90.980)	Time 17.32
train_accuracy 91.159
one epoch duration:33.39550304412842
Epoch #1, Learning rate: 0.001
Epoch: [1][99/189]	Loss 0.2758 (0.2574)	Accuracy 91.406 (91.070)	Time 17.59
train_accuracy 91.157
one epoch duration:33.28477382659912
Epoch #2, Learning rate: 0.001
Epoch: [2][99/189]	Loss 0.2287 (0.2508)	Accuracy 91.797 (91.262)	Time 17.36
train_accuracy 91.225
one epoch duration:32.81970143318176
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/189]	Loss 0.2279 (0.2418)	Accuracy 92.969 (91.746)	Time 17.26
train_accuracy 91.482
one epoch duration:32.317821741104126
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/189]	Loss 0.2779 (0.2421)	Accuracy 91.406 (91.523)	Time 17.44
train_accuracy 91.494
one epoch duration:32.888700008392334
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/189]	Loss 0.2165 (0.2410)	Accuracy 91.016 (91.473)	Time 17.37
train_accuracy 91.445
one epoch duration:32.729684829711914
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/189]	Loss 0.1914 (0.2428)	Accuracy 94.531 (91.582)	Time 17.92
train_accuracy 91.685
one epoch duration:33.540099143981934
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/189]	Loss 0.2942 (0.2458)	Accuracy 91.797 (91.629)	Time 17.28
train_accuracy 91.616
one epoch duration:32.97329044342041
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/189]	Loss 0.1968 (0.2452)	Accuracy 93.359 (91.449)	Time 17.34
train_accuracy 91.554
one epoch duration:32.5990993976593
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/189]	Loss 0.3116 (0.2442)	Accuracy 89.844 (91.461)	Time 17.21
train_accuracy 91.540
one epoch duration:32.46038794517517
[TRAIN] Retrain on 48332 samples
    Epoch 1/30  29.94s
    Epoch 2/30  29.72s
    Epoch 3/30  30.26s
    Epoch 4/30  30.15s
    Epoch 5/30  30.00s
    Epoch 6/30  30.10s
    Epoch 7/30  29.78s
    Epoch 8/30  30.13s
    Epoch 9/30  30.25s
    Epoch 10/30  29.93s
    Epoch 11/30  29.41s
    Epoch 12/30  29.81s
    Epoch 13/30  29.75s
    Epoch 14/30  29.69s
    Epoch 15/30  29.28s
    Epoch 16/30  30.62s
    Epoch 17/30  30.76s
    Epoch 18/30  30.04s
    Epoch 19/30  30.29s
    Epoch 20/30  29.51s
    Epoch 21/30  29.67s
    Epoch 22/30  30.38s
    Epoch 23/30  29.75s
    Epoch 24/30  29.26s
    Epoch 25/30  29.79s
    Epoch 26/30  29.79s
    Epoch 27/30  29.56s
    Epoch 28/30  30.21s
    Epoch 29/30  29.78s
    Epoch 30/30  29.99s
[SAVE] saved_models/retrain_7775916b40d20a5345b8760351dc4f7a984bfb38_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S1 | Ftot=1668 | ΔF:+20.56 ΔR: 0.35 ΔT: 0.37 | MIA:0.9454 PredDiff:8.88%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.001
Epoch: [0][99/183]	Loss 0.2397 (0.2479)	Accuracy 91.016 (91.379)	Time 18.72
train_accuracy 91.411
one epoch duration:33.828683376312256
Epoch #1, Learning rate: 0.001
Epoch: [1][99/183]	Loss 0.2641 (0.2384)	Accuracy 90.625 (91.879)	Time 17.19
train_accuracy 91.583
one epoch duration:31.354469537734985
Epoch #2, Learning rate: 0.001
Epoch: [2][99/183]	Loss 0.2901 (0.2451)	Accuracy 88.672 (91.266)	Time 17.31
train_accuracy 91.469
one epoch duration:31.68889045715332
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/183]	Loss 0.2338 (0.2381)	Accuracy 91.797 (91.566)	Time 17.43
train_accuracy 91.617
one epoch duration:31.836936712265015
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/183]	Loss 0.1543 (0.2379)	Accuracy 94.531 (91.539)	Time 17.32
train_accuracy 91.641
one epoch duration:31.41066551208496
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/183]	Loss 0.2142 (0.2364)	Accuracy 91.797 (91.668)	Time 17.87
train_accuracy 91.838
one epoch duration:32.06939888000488
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/183]	Loss 0.2220 (0.2345)	Accuracy 90.234 (91.535)	Time 17.22
train_accuracy 91.561
one epoch duration:31.54890203475952
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/183]	Loss 0.1528 (0.2370)	Accuracy 94.141 (91.695)	Time 17.37
train_accuracy 91.754
one epoch duration:31.551329374313354
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/183]	Loss 0.2158 (0.2304)	Accuracy 91.797 (91.875)	Time 17.32
train_accuracy 91.829
one epoch duration:31.767123460769653
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/183]	Loss 0.1455 (0.2338)	Accuracy 96.094 (91.910)	Time 17.66
train_accuracy 91.915
one epoch duration:31.909795999526978
[TRAIN] Retrain on 46666 samples
    Epoch 1/30  29.05s
    Epoch 2/30  28.67s
    Epoch 3/30  29.77s
    Epoch 4/30  29.19s
    Epoch 5/30  28.93s
    Epoch 6/30  29.06s
    Epoch 7/30  29.61s
    Epoch 8/30  29.59s
    Epoch 9/30  29.16s
    Epoch 10/30  28.85s
    Epoch 11/30  29.52s
    Epoch 12/30  29.10s
    Epoch 13/30  29.37s
    Epoch 14/30  29.52s
    Epoch 15/30  29.23s
    Epoch 16/30  29.05s
    Epoch 17/30  29.57s
    Epoch 18/30  29.27s
    Epoch 19/30  29.03s
    Epoch 20/30  29.12s
    Epoch 21/30  29.35s
    Epoch 22/30  29.29s
    Epoch 23/30  28.77s
    Epoch 24/30  29.09s
    Epoch 25/30  29.38s
    Epoch 26/30  29.30s
    Epoch 27/30  28.96s
    Epoch 28/30  29.07s
    Epoch 29/30  29.11s
    Epoch 30/30  29.13s
[SAVE] saved_models/retrain_031685d557d10a22a93b8df704d56e4fe54084af_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S2 | Ftot=3334 | ΔF:+30.26 ΔR: 0.60 ΔT: 1.45 | MIA:0.8303 PredDiff:10.31%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2931 (0.2420)	Accuracy 88.281 (91.570)	Time 17.37
train_accuracy 91.498
one epoch duration:30.63827610015869
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.1705 (0.2369)	Accuracy 94.141 (91.633)	Time 17.48
train_accuracy 91.384
one epoch duration:30.733282566070557
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2658 (0.2371)	Accuracy 91.016 (91.582)	Time 17.41
train_accuracy 91.618
one epoch duration:30.43634843826294
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2414 (0.2326)	Accuracy 90.625 (91.727)	Time 17.16
train_accuracy 91.651
one epoch duration:30.11299705505371
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.1933 (0.2333)	Accuracy 93.359 (91.566)	Time 17.36
train_accuracy 91.800
one epoch duration:30.478330373764038
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.3100 (0.2313)	Accuracy 89.453 (91.930)	Time 17.17
train_accuracy 91.933
one epoch duration:30.11419129371643
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.1615 (0.2279)	Accuracy 92.969 (92.047)	Time 17.16
train_accuracy 92.022
one epoch duration:30.138278484344482
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.2284 (0.2294)	Accuracy 92.969 (92.082)	Time 17.09
train_accuracy 91.964
one epoch duration:30.164207458496094
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2367 (0.2316)	Accuracy 92.969 (91.957)	Time 17.31
train_accuracy 91.856
one epoch duration:30.72714638710022
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.2638 (0.2240)	Accuracy 91.406 (92.309)	Time 17.16
train_accuracy 92.062
one epoch duration:30.2988498210907
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S3 | Ftot=5000 | ΔF:+60.02 ΔR: 0.87 ΔT: 4.87 | MIA:0.4605 PredDiff:14.36%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/189]	Loss 1.0271 (0.9245)	Accuracy 87.891 (90.973)	Time 17.45
train_accuracy 91.101
one epoch duration:33.150246143341064
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/189]	Loss 0.8226 (0.7879)	Accuracy 90.625 (91.020)	Time 17.66
train_accuracy 91.312
one epoch duration:33.30098247528076
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/189]	Loss 0.6651 (0.6536)	Accuracy 92.969 (91.199)	Time 17.74
train_accuracy 91.147
one epoch duration:33.43923544883728
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/189]	Loss 0.4671 (0.5162)	Accuracy 91.406 (91.180)	Time 17.55
train_accuracy 91.219
one epoch duration:33.273319721221924
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/189]	Loss 0.3798 (0.3817)	Accuracy 91.406 (91.285)	Time 17.66
train_accuracy 91.354
one epoch duration:33.068204402923584
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/189]	Loss 0.1822 (0.2481)	Accuracy 94.141 (91.453)	Time 17.28
train_accuracy 91.519
one epoch duration:32.66571640968323
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/189]	Loss 0.2451 (0.2480)	Accuracy 92.969 (91.352)	Time 17.36
train_accuracy 91.422
one epoch duration:32.73617219924927
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/189]	Loss 0.2344 (0.2466)	Accuracy 91.016 (91.402)	Time 17.26
train_accuracy 91.316
one epoch duration:32.82092618942261
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/189]	Loss 0.2226 (0.2472)	Accuracy 93.359 (91.508)	Time 17.50
train_accuracy 91.434
one epoch duration:33.06365728378296
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/189]	Loss 0.2884 (0.2467)	Accuracy 91.797 (91.527)	Time 17.51
train_accuracy 91.486
one epoch duration:33.081117391586304
[LOAD] Retrain from saved_models/retrain_7775916b40d20a5345b8760351dc4f7a984bfb38_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S1 | Ftot=1668 | ΔF:+23.62 ΔR: 0.23 ΔT: 0.39 | MIA:0.9319 PredDiff:8.97%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/183]	Loss 0.8922 (0.9127)	Accuracy 92.188 (91.238)	Time 17.29
train_accuracy 91.131
one epoch duration:31.66253638267517
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/183]	Loss 0.7550 (0.7785)	Accuracy 92.188 (91.355)	Time 17.57
train_accuracy 91.347
one epoch duration:32.07137656211853
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/183]	Loss 0.6100 (0.6360)	Accuracy 91.406 (91.707)	Time 17.55
train_accuracy 91.416
one epoch duration:31.997173309326172
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/183]	Loss 0.4823 (0.4965)	Accuracy 92.188 (91.746)	Time 17.73
train_accuracy 91.527
one epoch duration:32.27212309837341
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/183]	Loss 0.2948 (0.3749)	Accuracy 93.750 (91.551)	Time 17.57
train_accuracy 91.533
one epoch duration:32.10215973854065
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/183]	Loss 0.1915 (0.2413)	Accuracy 94.141 (91.672)	Time 17.65
train_accuracy 91.673
one epoch duration:32.27646088600159
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/183]	Loss 0.2381 (0.2427)	Accuracy 92.578 (91.703)	Time 17.70
train_accuracy 91.630
one epoch duration:32.29185080528259
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/183]	Loss 0.2189 (0.2366)	Accuracy 91.406 (91.621)	Time 17.66
train_accuracy 91.598
one epoch duration:32.2969970703125
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/183]	Loss 0.2054 (0.2436)	Accuracy 92.578 (91.457)	Time 17.61
train_accuracy 91.587
one epoch duration:32.09858679771423
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/183]	Loss 0.2076 (0.2394)	Accuracy 93.359 (91.734)	Time 17.51
train_accuracy 91.739
one epoch duration:32.00521802902222
[LOAD] Retrain from saved_models/retrain_031685d557d10a22a93b8df704d56e4fe54084af_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S2 | Ftot=3334 | ΔF:+33.98 ΔR: 0.36 ΔT: 1.64 | MIA:0.8577 PredDiff:10.52%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.8678 (0.9117)	Accuracy 92.188 (91.422)	Time 17.61
train_accuracy 91.347
one epoch duration:31.10071110725403
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.7221 (0.7786)	Accuracy 94.531 (91.129)	Time 17.66
train_accuracy 91.036
one epoch duration:31.153093338012695
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.5552 (0.6410)	Accuracy 95.703 (91.359)	Time 17.43
train_accuracy 91.318
one epoch duration:30.62643575668335
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.5786 (0.5112)	Accuracy 90.625 (91.484)	Time 17.34
train_accuracy 91.427
one epoch duration:30.5259952545166
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.4885 (0.3767)	Accuracy 86.719 (91.410)	Time 17.42
train_accuracy 91.578
one epoch duration:30.677900314331055
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2429 (0.2410)	Accuracy 91.016 (91.547)	Time 17.51
train_accuracy 91.560
one epoch duration:30.800698041915894
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2829 (0.2433)	Accuracy 90.625 (91.520)	Time 17.50
train_accuracy 91.551
one epoch duration:30.805332899093628
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.1781 (0.2436)	Accuracy 94.531 (91.512)	Time 17.55
train_accuracy 91.578
one epoch duration:30.878463983535767
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2637 (0.2404)	Accuracy 88.672 (91.656)	Time 17.52
train_accuracy 91.649
one epoch duration:30.652689218521118
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2570 (0.2351)	Accuracy 90.234 (91.840)	Time 17.25
train_accuracy 91.762
one epoch duration:30.356802940368652
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S3 | Ftot=5000 | ΔF:+68.50 ΔR: 0.50 ΔT: 5.61 | MIA:0.4526 PredDiff:14.92%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 11.331
one epoch duration:1.1234171390533447
Epoch #1, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 10.731
one epoch duration:1.123492956161499
Epoch #2, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 10.671
one epoch duration:1.1020417213439941
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 10.971
one epoch duration:1.106283187866211
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 11.691
one epoch duration:1.1165411472320557
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 10.971
one epoch duration:1.101987361907959
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 11.631
one epoch duration:1.1056876182556152
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 10.911
one epoch duration:1.0986378192901611
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 11.451
one epoch duration:1.101365327835083
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 10.612
one epoch duration:1.1019136905670166
[LOAD] Retrain from saved_models/retrain_7775916b40d20a5345b8760351dc4f7a984bfb38_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S1 | Ftot=1668 | ΔF:-32.79 ΔR:11.25 ΔT: 9.24 | MIA:0.7097 PredDiff:20.09%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 13.407
one epoch duration:2.291262149810791
Epoch #1, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 13.677
one epoch duration:2.290008068084717
Epoch #2, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 13.617
one epoch duration:2.295029401779175
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 13.287
one epoch duration:2.3214972019195557
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 12.897
one epoch duration:2.290590763092041
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 13.587
one epoch duration:2.2887067794799805
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 13.287
one epoch duration:2.2924695014953613
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 13.137
one epoch duration:2.302506446838379
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 13.287
one epoch duration:2.2933671474456787
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 13.257
one epoch duration:2.2941722869873047
[LOAD] Retrain from saved_models/retrain_031685d557d10a22a93b8df704d56e4fe54084af_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S2 | Ftot=3334 | ΔF:-24.51 ΔR:10.11 ΔT: 8.95 | MIA:0.7253 PredDiff:20.38%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.060
one epoch duration:3.393170118331909
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.420
one epoch duration:3.4003939628601074
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.060
one epoch duration:3.3803322315216064
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.200
one epoch duration:3.3380625247955322
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.980
one epoch duration:3.323709726333618
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.500
one epoch duration:3.327266216278076
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.040
one epoch duration:3.316420316696167
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.920
one epoch duration:3.3255455493927
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.200
one epoch duration:3.2923457622528076
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.860
one epoch duration:3.2813146114349365
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S3 | Ftot=5000 | ΔF:+16.54 ΔR:10.03 ΔT: 5.15 | MIA:0.4141 PredDiff:22.33%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [0][99/189]	Loss -0.4379 (-0.3891)	Accuracy 91.797 (91.039)	Time 21.48
train_accuracy 90.998
one epoch duration:39.80919575691223
Epoch #1, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [1][99/189]	Loss -0.4804 (-0.4758)	Accuracy 90.234 (91.012)	Time 21.43
train_accuracy 91.122
one epoch duration:39.75415515899658
Epoch #2, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [2][99/189]	Loss -0.6398 (-0.5685)	Accuracy 94.922 (91.270)	Time 21.44
train_accuracy 91.136
one epoch duration:39.75449562072754
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [3][99/189]	Loss -0.6958 (-0.6486)	Accuracy 91.406 (91.281)	Time 21.70
train_accuracy 91.095
one epoch duration:40.61660194396973
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [4][99/189]	Loss -0.7841 (-0.7272)	Accuracy 91.797 (91.152)	Time 21.80
train_accuracy 90.816
one epoch duration:40.46091556549072
[LOAD] Retrain from saved_models/retrain_7775916b40d20a5345b8760351dc4f7a984bfb38_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S1 | Ftot=1668 | ΔF:-34.11 ΔR: 5.69 ΔT: 5.09 | MIA:0.8164 PredDiff:14.63%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [0][99/183]	Loss -0.8154 (-0.7874)	Accuracy 89.453 (91.027)	Time 22.67
train_accuracy 90.985
one epoch duration:39.96653747558594
Epoch #1, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [1][99/183]	Loss -0.9061 (-0.8758)	Accuracy 88.281 (91.082)	Time 22.60
train_accuracy 90.886
one epoch duration:39.751187562942505
Epoch #2, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [2][99/183]	Loss -0.9703 (-0.9600)	Accuracy 91.797 (91.070)	Time 22.30
train_accuracy 90.841
one epoch duration:39.37408900260925
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [3][99/183]	Loss -0.9786 (-1.0741)	Accuracy 86.719 (90.613)	Time 22.32
train_accuracy 90.614
one epoch duration:39.41251277923584
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [4][99/183]	Loss -1.0711 (-1.1426)	Accuracy 89.453 (90.785)	Time 22.33
train_accuracy 90.584
one epoch duration:39.629693269729614
[LOAD] Retrain from saved_models/retrain_031685d557d10a22a93b8df704d56e4fe54084af_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S2 | Ftot=3334 | ΔF:-31.58 ΔR: 6.56 ΔT: 6.50 | MIA:0.8304 PredDiff:16.95%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -1.2579 (-1.2228)	Accuracy 91.406 (90.902)	Time 23.56
train_accuracy 90.669
one epoch duration:39.58872151374817
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -1.2323 (-1.3267)	Accuracy 86.719 (90.684)	Time 23.45
train_accuracy 90.620
one epoch duration:39.551708698272705
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -1.5003 (-1.4391)	Accuracy 92.578 (90.668)	Time 23.47
train_accuracy 90.718
one epoch duration:39.471344232559204
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -1.4346 (-1.5442)	Accuracy 87.109 (90.586)	Time 23.51
train_accuracy 90.573
one epoch duration:39.44456887245178
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -1.6059 (-1.6593)	Accuracy 85.547 (90.332)	Time 23.44
train_accuracy 90.329
one epoch duration:39.417433977127075
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S3 | Ftot=5000 | ΔF:+1.16 ΔR: 5.18 ΔT: 3.43 | MIA:0.4477 PredDiff:15.93%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.5021 (0.4342)	Accuracy 87.500 (88.508)	Time 16.17
Epoch: [0][199/196]	Loss 0.4348 (0.4171)	Accuracy 89.844 (88.417)	Time 17.33
one epoch duration:56.77370834350586
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.3938 (0.3918)	Accuracy 86.719 (88.470)	Time 16.11
Epoch: [1][199/196]	Loss 0.3194 (0.3915)	Accuracy 88.672 (88.510)	Time 17.31
one epoch duration:57.814642906188965
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.4333 (0.3870)	Accuracy 85.938 (88.336)	Time 16.13
Epoch: [2][199/196]	Loss 0.3694 (0.3803)	Accuracy 89.453 (88.546)	Time 17.46
one epoch duration:50.12374401092529
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.3742 (0.3715)	Accuracy 89.453 (88.731)	Time 16.22
Epoch: [3][199/196]	Loss 0.3590 (0.3777)	Accuracy 87.891 (88.672)	Time 17.38
one epoch duration:34.14527249336243
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.4191 (0.3782)	Accuracy 88.281 (88.571)	Time 16.21
Epoch: [4][199/196]	Loss 0.2853 (0.3769)	Accuracy 91.016 (88.571)	Time 17.39
one epoch duration:34.140531063079834
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.4080 (0.3763)	Accuracy 89.062 (88.567)	Time 16.18
Epoch: [5][199/196]	Loss 0.4690 (0.3705)	Accuracy 86.719 (88.745)	Time 17.38
one epoch duration:34.10153365135193
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.4176 (0.3737)	Accuracy 88.281 (88.899)	Time 16.23
Epoch: [6][199/196]	Loss 0.4097 (0.3780)	Accuracy 86.328 (88.642)	Time 17.24
one epoch duration:34.00594115257263
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.3769 (0.3721)	Accuracy 87.891 (88.642)	Time 16.03
Epoch: [7][199/196]	Loss 0.3954 (0.3768)	Accuracy 89.062 (88.532)	Time 17.18
one epoch duration:33.740251779556274
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.3531 (0.3694)	Accuracy 89.453 (88.781)	Time 15.98
Epoch: [8][199/196]	Loss 0.3046 (0.3713)	Accuracy 89.844 (88.814)	Time 17.14
one epoch duration:33.661065101623535
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.3891 (0.3560)	Accuracy 89.453 (89.214)	Time 15.99
Epoch: [9][199/196]	Loss 0.3752 (0.3698)	Accuracy 89.844 (88.710)	Time 17.34
one epoch duration:33.870784759521484
[LOAD] Retrain from saved_models/retrain_7775916b40d20a5345b8760351dc4f7a984bfb38_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S1 | Ftot=1668 | ΔF:+17.81 ΔR: 0.31 ΔT: 0.39 | MIA:0.9249 PredDiff:8.89%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.001
Epoch: [0][99/197]	Loss 0.4358 (0.4724)	Accuracy 86.328 (85.897)	Time 14.94
Epoch: [0][199/197]	Loss 0.3764 (0.4631)	Accuracy 88.281 (85.837)	Time 17.38
one epoch duration:44.94683504104614
Epoch #1, Learning rate: 0.001
Epoch: [1][99/197]	Loss 0.4553 (0.4512)	Accuracy 86.328 (85.833)	Time 14.91
Epoch: [1][199/197]	Loss 0.4764 (0.4475)	Accuracy 84.766 (85.931)	Time 17.32
one epoch duration:42.01470685005188
Epoch #2, Learning rate: 0.001
Epoch: [2][99/197]	Loss 0.4605 (0.4391)	Accuracy 85.938 (86.165)	Time 14.91
Epoch: [2][199/197]	Loss 0.3848 (0.4430)	Accuracy 87.500 (85.881)	Time 17.35
one epoch duration:34.006184101104736
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/197]	Loss 0.3498 (0.4291)	Accuracy 89.453 (86.337)	Time 14.91
Epoch: [3][199/197]	Loss 0.3703 (0.4327)	Accuracy 87.500 (86.194)	Time 17.33
one epoch duration:33.99491333961487
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/197]	Loss 0.3763 (0.4332)	Accuracy 87.891 (86.210)	Time 14.94
Epoch: [4][199/197]	Loss 0.5320 (0.4317)	Accuracy 83.203 (86.234)	Time 17.34
one epoch duration:34.03881812095642
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/197]	Loss 0.3957 (0.4286)	Accuracy 86.328 (86.296)	Time 14.94
Epoch: [5][199/197]	Loss 0.4969 (0.4349)	Accuracy 84.766 (86.087)	Time 17.35
one epoch duration:34.04619789123535
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/197]	Loss 0.4864 (0.4360)	Accuracy 84.375 (86.092)	Time 14.96
Epoch: [6][199/197]	Loss 0.5206 (0.4334)	Accuracy 81.641 (86.267)	Time 17.43
one epoch duration:34.15363359451294
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/197]	Loss 0.3984 (0.4255)	Accuracy 86.328 (86.519)	Time 14.76
Epoch: [7][199/197]	Loss 0.4517 (0.4280)	Accuracy 85.938 (86.316)	Time 17.12
one epoch duration:33.60968279838562
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/197]	Loss 0.4681 (0.4281)	Accuracy 84.375 (86.346)	Time 14.93
Epoch: [8][199/197]	Loss 0.4822 (0.4292)	Accuracy 84.375 (86.387)	Time 17.40
one epoch duration:34.0866756439209
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/197]	Loss 0.3448 (0.4329)	Accuracy 87.891 (86.115)	Time 15.02
Epoch: [9][199/197]	Loss 0.5600 (0.4326)	Accuracy 80.469 (86.261)	Time 17.38
one epoch duration:34.16281723976135
[LOAD] Retrain from saved_models/retrain_031685d557d10a22a93b8df704d56e4fe54084af_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S2 | Ftot=3334 | ΔF:+24.78 ΔR: 0.59 ΔT: 1.20 | MIA:0.8246 PredDiff:10.31%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.4403 (0.4906)	Accuracy 83.984 (83.438)	Time 14.07
Epoch: [0][199/196]	Loss 0.4138 (0.4893)	Accuracy 85.156 (83.231)	Time 17.49
one epoch duration:44.73640727996826
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.4132 (0.4763)	Accuracy 85.938 (83.320)	Time 13.85
Epoch: [1][199/196]	Loss 0.4618 (0.4785)	Accuracy 84.375 (83.268)	Time 17.26
one epoch duration:44.20014452934265
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.4605 (0.4768)	Accuracy 81.641 (83.506)	Time 13.94
Epoch: [2][199/196]	Loss 0.4730 (0.4706)	Accuracy 83.203 (83.557)	Time 17.40
one epoch duration:34.14619851112366
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.5021 (0.4747)	Accuracy 81.641 (83.174)	Time 13.99
Epoch: [3][199/196]	Loss 0.4936 (0.4705)	Accuracy 83.203 (83.435)	Time 17.58
one epoch duration:34.40881276130676
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.4921 (0.4674)	Accuracy 83.203 (83.467)	Time 13.98
Epoch: [4][199/196]	Loss 0.4579 (0.4649)	Accuracy 83.984 (83.566)	Time 17.47
one epoch duration:34.26750087738037
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.4218 (0.4617)	Accuracy 87.109 (83.638)	Time 13.93
Epoch: [5][199/196]	Loss 0.4453 (0.4645)	Accuracy 83.594 (83.615)	Time 17.46
one epoch duration:34.171290159225464
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.4041 (0.4620)	Accuracy 85.938 (83.628)	Time 13.91
Epoch: [6][199/196]	Loss 0.4704 (0.4660)	Accuracy 82.812 (83.624)	Time 17.34
one epoch duration:34.05444288253784
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.4564 (0.4690)	Accuracy 85.156 (83.701)	Time 13.98
Epoch: [7][199/196]	Loss 0.5199 (0.4661)	Accuracy 82.812 (83.652)	Time 17.39
one epoch duration:34.1932954788208
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.5408 (0.4706)	Accuracy 81.250 (83.511)	Time 14.29
Epoch: [8][199/196]	Loss 0.4770 (0.4675)	Accuracy 80.859 (83.587)	Time 17.50
one epoch duration:34.60851168632507
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.4155 (0.4597)	Accuracy 83.594 (83.921)	Time 13.91
Epoch: [9][199/196]	Loss 0.4735 (0.4650)	Accuracy 83.984 (83.709)	Time 17.18
one epoch duration:33.85033297538757
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S3 | Ftot=5000 | ΔF:+14.36 ΔR: 0.82 ΔT: 1.13 | MIA:0.4590 PredDiff:13.15%

===== Running Method: Wfisher =====
  > Applied specific params for Wfisher: {'alpha': 10.0}

[UNLEARN] Stage 1: |Forget Total|=1668
[LOAD] Retrain from saved_models/retrain_7775916b40d20a5345b8760351dc4f7a984bfb38_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S1 | Ftot=1668 | ΔF:-41.91 ΔR:82.89 ΔT:73.14 | MIA:0.5000 PredDiff:89.17%

[UNLEARN] Stage 2: |Forget Total|=3334
[LOAD] Retrain from saved_models/retrain_031685d557d10a22a93b8df704d56e4fe54084af_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S2 | Ftot=3334 | ΔF:+62.03 ΔR:89.50 ΔT:71.49 | MIA:0.0000 PredDiff:94.11%

[UNLEARN] Stage 3: |Forget Total|=5000
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S3 | Ftot=5000 | ΔF:+100.00 ΔR:92.92 ΔT:66.20 | MIA:0.0000 PredDiff:100.00%

===== Running Method: SCRUB =====
  > Applied specific params for SCRUB: {'unlearn_epochs': 10, 'kd_T': 4.0, 'gamma': 1.0, 'beta': 1.0, 'msteps': 5, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [0][6/7]	Time 0.101 (0.165)	Data 0.068 (0.119)	Loss -7.9034 (-6.3724)	Forget_Acc@1 6.818 (10.612)
*** Minimize step ***
Epoch: [0][188/189]	Time 0.149 (0.174)	Data 0.107 (0.128)	Loss 0.3557 (0.3525)	Retain_Acc@1 88.725 (90.832)
Epoch: [0]	 train-acc:	90.83216087267434	 train-loss: 0.352517127739143
one epoch duration:34.06794261932373
Epoch #1, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [1][6/7]	Time 0.099 (0.163)	Data 0.067 (0.119)	Loss -8.4243 (-7.4062)	Forget_Acc@1 11.364 (8.453)
*** Minimize step ***
Epoch: [1][188/189]	Time 0.148 (0.173)	Data 0.105 (0.127)	Loss 0.4086 (0.3736)	Retain_Acc@1 89.706 (90.797)
Epoch: [1]	 train-acc:	90.7969874898438	 train-loss: 0.3735808669721647
one epoch duration:33.91556930541992
Epoch #2, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [2][6/7]	Time 0.097 (0.161)	Data 0.065 (0.117)	Loss -8.0014 (-7.8300)	Forget_Acc@1 6.061 (8.573)
*** Minimize step ***
Epoch: [2][188/189]	Time 0.148 (0.171)	Data 0.105 (0.125)	Loss 0.3792 (0.3669)	Retain_Acc@1 88.725 (91.083)
Epoch: [2]	 train-acc:	91.08251260651527	 train-loss: 0.36685316514112515
one epoch duration:33.53369951248169
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [3][6/7]	Time 0.101 (0.161)	Data 0.066 (0.117)	Loss -8.3969 (-7.6294)	Forget_Acc@1 11.364 (8.693)
*** Minimize step ***
Epoch: [3][188/189]	Time 0.149 (0.172)	Data 0.105 (0.125)	Loss 0.4214 (0.3098)	Retain_Acc@1 87.255 (91.308)
Epoch: [3]	 train-acc:	91.30803609953942	 train-loss: 0.3097594387358825
one epoch duration:33.62275791168213
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [4][6/7]	Time 0.098 (0.161)	Data 0.066 (0.117)	Loss -7.9307 (-7.9742)	Forget_Acc@1 9.091 (8.273)
*** Minimize step ***
Epoch: [4][188/189]	Time 0.150 (0.172)	Data 0.105 (0.125)	Loss 0.4093 (0.3193)	Retain_Acc@1 88.725 (91.180)
Epoch: [4]	 train-acc:	91.17975666842042	 train-loss: 0.3192779305290428
one epoch duration:33.576714515686035
Epoch #5, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [5][6/7]	Time 0.100 (0.162)	Data 0.066 (0.117)	Loss -8.9800 (-8.2348)	Forget_Acc@1 9.091 (8.153)
*** Minimize step ***
Epoch: [5][188/189]	Time 0.149 (0.172)	Data 0.106 (0.126)	Loss 0.3814 (0.3150)	Retain_Acc@1 90.686 (91.411)
Epoch: [5]	 train-acc:	91.41148720144348	 train-loss: 0.3150246886043648
one epoch duration:33.725579500198364
Epoch #6, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [6][188/189]	Time 0.151 (0.172)	Data 0.106 (0.125)	Loss 0.2599 (0.3090)	Retain_Acc@1 93.137 (91.341)
Epoch: [6]	 train-acc:	91.34114043641384	 train-loss: 0.3089655086510349
one epoch duration:32.46865129470825
Epoch #7, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [7][188/189]	Time 0.150 (0.172)	Data 0.105 (0.125)	Loss 0.2660 (0.3035)	Retain_Acc@1 91.667 (91.368)
Epoch: [7]	 train-acc:	91.36803772823805	 train-loss: 0.3035477770486731
one epoch duration:32.435149908065796
Epoch #8, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [8][188/189]	Time 0.146 (0.171)	Data 0.105 (0.126)	Loss 0.3161 (0.3031)	Retain_Acc@1 92.157 (91.380)
Epoch: [8]	 train-acc:	91.38045186443182	 train-loss: 0.3030749116350974
one epoch duration:32.3770227432251
Epoch #9, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [9][188/189]	Time 0.151 (0.171)	Data 0.108 (0.125)	Loss 0.3568 (0.3095)	Retain_Acc@1 87.255 (91.167)
Epoch: [9]	 train-acc:	91.167342563166	 train-loss: 0.3094811493283144
one epoch duration:32.25273251533508
[LOAD] Retrain from saved_models/retrain_7775916b40d20a5345b8760351dc4f7a984bfb38_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S1 | Ftot=1668 | ΔF:+37.05 ΔR: 0.14 ΔT: 0.64 | MIA:0.8918 PredDiff:9.14%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [0][13/14]	Time 0.042 (0.168)	Data 0.014 (0.120)	Loss -15.1516 (-11.5714)	Forget_Acc@1 0.000 (8.728)
*** Minimize step ***
Epoch: [0][182/183]	Time 0.077 (0.172)	Data 0.045 (0.125)	Loss 0.6180 (0.6774)	Retain_Acc@1 90.541 (89.858)
Epoch: [0]	 train-acc:	89.85771225630198	 train-loss: 0.677418474237463
one epoch duration:33.91229486465454
Epoch #1, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [1][13/14]	Time 0.045 (0.164)	Data 0.015 (0.118)	Loss -22.3033 (-14.2707)	Forget_Acc@1 16.667 (6.569)
*** Minimize step ***
Epoch: [1][182/183]	Time 0.076 (0.172)	Data 0.045 (0.125)	Loss 0.3257 (0.8192)	Retain_Acc@1 95.946 (89.558)
Epoch: [1]	 train-acc:	89.5577079669297	 train-loss: 0.8192060138431981
one epoch duration:33.74728989601135
Epoch #2, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [2][13/14]	Time 0.048 (0.167)	Data 0.016 (0.121)	Loss -29.8744 (-16.4659)	Forget_Acc@1 0.000 (5.909)
*** Minimize step ***
Epoch: [2][182/183]	Time 0.075 (0.174)	Data 0.044 (0.128)	Loss 0.3007 (0.9693)	Retain_Acc@1 94.595 (89.468)
Epoch: [2]	 train-acc:	89.46770668512079	 train-loss: 0.9692689879495526
one epoch duration:34.218660831451416
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [3][13/14]	Time 0.041 (0.164)	Data 0.011 (0.120)	Loss -12.8450 (-14.8679)	Forget_Acc@1 0.000 (6.389)
*** Minimize step ***
Epoch: [3][182/183]	Time 0.081 (0.175)	Data 0.045 (0.129)	Loss 0.6752 (0.4652)	Retain_Acc@1 81.081 (90.777)
Epoch: [3]	 train-acc:	90.77701110669815	 train-loss: 0.46515931690448875
one epoch duration:34.34808111190796
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [4][13/14]	Time 0.049 (0.167)	Data 0.015 (0.120)	Loss -13.9396 (-16.1797)	Forget_Acc@1 0.000 (6.239)
*** Minimize step ***
Epoch: [4][182/183]	Time 0.077 (0.176)	Data 0.044 (0.128)	Loss 0.5274 (0.4936)	Retain_Acc@1 90.541 (90.933)
Epoch: [4]	 train-acc:	90.93344190958274	 train-loss: 0.4935632799112428
one epoch duration:34.4956738948822
Epoch #5, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [5][13/14]	Time 0.044 (0.165)	Data 0.014 (0.121)	Loss -16.3815 (-17.3570)	Forget_Acc@1 16.667 (5.999)
*** Minimize step ***
Epoch: [5][182/183]	Time 0.077 (0.176)	Data 0.044 (0.129)	Loss 0.5114 (0.5146)	Retain_Acc@1 89.189 (90.910)
Epoch: [5]	 train-acc:	90.90987014819555	 train-loss: 0.5145698898896062
one epoch duration:34.46052432060242
Epoch #6, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [6][182/183]	Time 0.074 (0.172)	Data 0.043 (0.126)	Loss 0.4591 (0.4305)	Retain_Acc@1 91.892 (91.060)
Epoch: [6]	 train-acc:	91.05987228323582	 train-loss: 0.43054865660651276
one epoch duration:31.49020290374756
Epoch #7, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [7][182/183]	Time 0.072 (0.171)	Data 0.042 (0.125)	Loss 0.8648 (0.4028)	Retain_Acc@1 79.730 (90.985)
Epoch: [7]	 train-acc:	90.984871210811	 train-loss: 0.4028464737749997
one epoch duration:31.29282307624817
Epoch #8, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [8][182/183]	Time 0.075 (0.172)	Data 0.044 (0.125)	Loss 0.6650 (0.3828)	Retain_Acc@1 91.892 (91.223)
Epoch: [8]	 train-acc:	91.2227317526568	 train-loss: 0.3828218460941327
one epoch duration:31.427950143814087
Epoch #9, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [9][182/183]	Time 0.076 (0.173)	Data 0.043 (0.126)	Loss 0.3866 (0.3770)	Retain_Acc@1 90.541 (91.161)
Epoch: [9]	 train-acc:	91.1605880116699	 train-loss: 0.37699615320043345
one epoch duration:31.581990480422974
[LOAD] Retrain from saved_models/retrain_031685d557d10a22a93b8df704d56e4fe54084af_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S2 | Ftot=3334 | ΔF:+41.18 ΔR: 0.05 ΔT: 1.80 | MIA:0.8395 PredDiff:11.10%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [0][19/20]	Time 0.101 (0.168)	Data 0.069 (0.123)	Loss -49.0080 (-29.8178)	Forget_Acc@1 0.735 (3.500)
*** Minimize step ***
Epoch: [0][175/176]	Time 0.146 (0.172)	Data 0.103 (0.126)	Loss 0.5652 (1.9665)	Retain_Acc@1 90.500 (87.384)
Epoch: [0]	 train-acc:	87.38444444444444	 train-loss: 1.9665012183295356
one epoch duration:33.675628662109375
Epoch #1, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [1][19/20]	Time 0.103 (0.169)	Data 0.068 (0.123)	Loss -86.7448 (-52.5073)	Forget_Acc@1 0.000 (0.600)
*** Minimize step ***
Epoch: [1][175/176]	Time 0.146 (0.173)	Data 0.105 (0.126)	Loss 1.2019 (4.9571)	Retain_Acc@1 88.500 (82.924)
Epoch: [1]	 train-acc:	82.92444444444445	 train-loss: 4.957091348542107
one epoch duration:33.74793744087219
Epoch #2, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [2][19/20]	Time 0.101 (0.167)	Data 0.069 (0.123)	Loss -106.0184 (-62.0395)	Forget_Acc@1 0.000 (0.080)
*** Minimize step ***
Epoch: [2][175/176]	Time 0.144 (0.172)	Data 0.103 (0.125)	Loss 1.8025 (5.0811)	Retain_Acc@1 88.000 (82.676)
Epoch: [2]	 train-acc:	82.67555555555556	 train-loss: 5.0810785557428995
one epoch duration:33.55612850189209
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [3][19/20]	Time 0.104 (0.168)	Data 0.069 (0.123)	Loss -76.5705 (-57.7326)	Forget_Acc@1 0.735 (0.520)
*** Minimize step ***
Epoch: [3][175/176]	Time 0.144 (0.171)	Data 0.103 (0.125)	Loss 2.2953 (2.9033)	Retain_Acc@1 89.000 (86.227)
Epoch: [3]	 train-acc:	86.22666666666667	 train-loss: 2.903326573138767
one epoch duration:33.47074890136719
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [4][19/20]	Time 0.101 (0.168)	Data 0.069 (0.123)	Loss -84.2354 (-77.1688)	Forget_Acc@1 1.471 (0.640)
*** Minimize step ***
Epoch: [4][175/176]	Time 0.144 (0.171)	Data 0.102 (0.126)	Loss 2.2644 (2.8429)	Retain_Acc@1 92.500 (86.936)
Epoch: [4]	 train-acc:	86.93555555555555	 train-loss: 2.842887828233507
one epoch duration:33.53156042098999
Epoch #5, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [5][19/20]	Time 0.104 (0.168)	Data 0.072 (0.123)	Loss -94.0714 (-87.2804)	Forget_Acc@1 0.000 (0.380)
*** Minimize step ***
Epoch: [5][175/176]	Time 0.147 (0.172)	Data 0.103 (0.126)	Loss 2.3480 (2.9478)	Retain_Acc@1 87.500 (86.627)
Epoch: [5]	 train-acc:	86.62666666666667	 train-loss: 2.9477920021480983
one epoch duration:33.62145471572876
Epoch #6, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [6][175/176]	Time 0.147 (0.171)	Data 0.104 (0.125)	Loss 1.8920 (2.2422)	Retain_Acc@1 90.000 (87.744)
Epoch: [6]	 train-acc:	87.74444444444444	 train-loss: 2.2421942608303493
one epoch duration:30.059533834457397
Epoch #7, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [7][175/176]	Time 0.166 (0.173)	Data 0.121 (0.126)	Loss 1.6933 (2.0103)	Retain_Acc@1 92.500 (88.073)
Epoch: [7]	 train-acc:	88.07333333333334	 train-loss: 2.010271078660753
one epoch duration:30.461843967437744
Epoch #8, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [8][175/176]	Time 0.150 (0.175)	Data 0.106 (0.128)	Loss 1.7466 (1.8207)	Retain_Acc@1 91.000 (87.953)
Epoch: [8]	 train-acc:	87.95333333333333	 train-loss: 1.8207263447655573
one epoch duration:30.761452198028564
Epoch #9, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [9][175/176]	Time 0.147 (0.174)	Data 0.105 (0.128)	Loss 1.5107 (1.6657)	Retain_Acc@1 88.000 (88.367)
Epoch: [9]	 train-acc:	88.36666666666666	 train-loss: 1.6657261253145006
one epoch duration:30.6359920501709
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S3 | Ftot=5000 | ΔF:+22.24 ΔR: 2.38 ΔT: 0.97 | MIA:0.4532 PredDiff:14.39%

===== Full Results =====
 method  stage  forget_total  Retrain_F  Retrain_R  Retrain_T  Unlearn_F  Unlearn_R  Unlearn_T         ΔF        ΔR    ΔT      MIA  PredDiff(%)
     FT      1          1668  41.906475  93.236365      83.14  62.470024  93.581892      83.51  20.563549  0.345527  0.37 0.945432        8.878
     FT      2          3334  37.972406  93.065615      81.49  68.236353  93.665624      82.94  30.263947  0.600009  1.45 0.830288       10.312
     FT      3          5000   0.000000  92.920000      76.20  60.020000  93.793333      81.07  60.020000  0.873333  4.87 0.460533       14.360
  FT_l1      1          1668  41.906475  93.236365      83.14  65.527578  93.470165      83.53  23.621103  0.233800  0.39 0.931949        8.972
  FT_l1      2          3334  37.972406  93.065615      81.49  71.955609  93.423477      83.13  33.983203  0.357862  1.64 0.857660       10.522
  FT_l1      3          5000   0.000000  92.920000      76.20  68.500000  93.415556      81.81  68.500000  0.495556  5.61 0.452600       14.922
     GA      1          1668  41.906475  93.236365      83.14   9.112710  81.989158      73.90 -32.793765 11.247207  9.24 0.709705       20.088
     GA      2          3334  37.972406  93.065615      81.49  13.467307  82.951185      72.54 -24.505099 10.114430  8.95 0.725276       20.384
     GA      3          5000   0.000000  92.920000      76.20  16.540000  82.893333      71.05  16.540000 10.026667  5.15 0.414122       22.326
     NG      1          1668  41.906475  93.236365      83.14   7.793765  87.546553      78.05 -34.112710  5.689812  5.09 0.816441       14.634
     NG      2          3334  37.972406  93.065615      81.49   6.388722  86.501950      74.99 -31.583683  6.563665  6.50 0.830426       16.948
     NG      3          5000   0.000000  92.920000      76.20   1.160000  87.740000      72.77   1.160000  5.180000  3.43 0.447733       15.930
     RL      1          1668  41.906475  93.236365      83.14  59.712230  93.542580      83.53  17.805755  0.306215  0.39 0.924917        8.892
     RL      2          3334  37.972406  93.065615      81.49  62.747451  93.659195      82.69  24.775045  0.593580  1.20 0.824599       10.308
     RL      3          5000   0.000000  92.920000      76.20  14.360000  93.735556      77.33  14.360000  0.815556  1.13 0.459000       13.154
Wfisher      1          1668  41.906475  93.236365      83.14   0.000000  10.345113      10.00 -41.906475 82.891252 73.14 0.500000       89.172
Wfisher      2          3334  37.972406  93.065615      81.49 100.000000   3.570051      10.00  62.027594 89.495564 71.49 0.000000       94.108
Wfisher      3          5000   0.000000  92.920000      76.20 100.000000   0.000000      10.00 100.000000 92.920000 66.20 0.000000      100.000
  SCRUB      1          1668  41.906475  93.236365      83.14  78.956835  93.381197      83.78  37.050360  0.144832  0.64 0.891824        9.142
  SCRUB      2          3334  37.972406  93.065615      81.49  79.154169  93.016329      83.29  41.181764  0.049286  1.80 0.839524       11.096
  SCRUB      3          5000   0.000000  92.920000      76.20  22.240000  90.544444      77.17  22.240000  2.375556  0.97 0.453167       14.390

Results saved to saved_models/results_class_c_proxy_stage_hard_first.csv

--- [2/8] Experiment FINISHED ---
============================================================

--- [3/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: c_proxy
  - unlearning_granularity: batch
  - forget_partition_ordering: easy_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[LOAD] Original model from saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'c_proxy' method...
Partition sizes for forget: [1666, 1666, 1668]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for FT
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2698 (0.2817)	Accuracy 91.016 (90.184)	Time 17.28
train_accuracy 90.364
one epoch duration:30.375766277313232
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.2619 (0.2558)	Accuracy 89.844 (91.133)	Time 17.28
train_accuracy 90.918
one epoch duration:30.481303930282593
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2393 (0.2550)	Accuracy 90.234 (90.824)	Time 17.36
train_accuracy 90.784
one epoch duration:30.279496669769287
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2585 (0.2503)	Accuracy 92.578 (91.270)	Time 17.06
train_accuracy 91.164
one epoch duration:30.01764988899231
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.2473 (0.2493)	Accuracy 92.188 (91.168)	Time 17.05
train_accuracy 91.249
one epoch duration:29.984514713287354
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.2547 (0.2522)	Accuracy 92.578 (91.199)	Time 17.07
train_accuracy 91.331
one epoch duration:30.03023886680603
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.2669 (0.2474)	Accuracy 91.406 (91.496)	Time 17.06
train_accuracy 91.382
one epoch duration:30.00938391685486
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.1872 (0.2477)	Accuracy 93.750 (91.383)	Time 17.08
train_accuracy 91.531
one epoch duration:30.067282915115356
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2831 (0.2474)	Accuracy 89.844 (91.316)	Time 17.08
train_accuracy 91.373
one epoch duration:30.19880199432373
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.3051 (0.2453)	Accuracy 88.672 (91.371)	Time 17.37
train_accuracy 91.160
one epoch duration:30.5215744972229
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         FT | Ftot=5000 | ΔF:+60.40 ΔR: 0.38 ΔT: 4.77 | MIA:0.4581 PredDiff:14.41%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for FT_l1
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.9340 (0.9505)	Accuracy 89.453 (90.125)	Time 17.45
train_accuracy 90.204
one epoch duration:30.773784160614014
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.7687 (0.7944)	Accuracy 91.797 (90.824)	Time 17.57
train_accuracy 90.644
one epoch duration:30.909114599227905
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.6955 (0.6520)	Accuracy 91.016 (91.223)	Time 17.61
train_accuracy 91.040
one epoch duration:30.958028316497803
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.5748 (0.5188)	Accuracy 89.062 (91.078)	Time 17.60
train_accuracy 91.211
one epoch duration:30.94238543510437
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.3371 (0.3862)	Accuracy 94.141 (91.250)	Time 17.49
train_accuracy 91.116
one epoch duration:30.527742385864258
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2327 (0.2522)	Accuracy 92.969 (91.234)	Time 17.21
train_accuracy 91.213
one epoch duration:30.267348051071167
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2227 (0.2503)	Accuracy 92.188 (91.336)	Time 17.23
train_accuracy 91.293
one epoch duration:30.290788888931274
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.2996 (0.2567)	Accuracy 89.844 (91.094)	Time 17.19
train_accuracy 91.127
one epoch duration:30.273380517959595
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2254 (0.2500)	Accuracy 91.016 (91.297)	Time 17.50
train_accuracy 91.151
one epoch duration:30.795719623565674
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2553 (0.2570)	Accuracy 91.797 (90.848)	Time 17.51
train_accuracy 90.916
one epoch duration:30.836949110031128
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)      FT_l1 | Ftot=5000 | ΔF:+70.28 ΔR: 0.11 ΔT: 5.65 | MIA:0.4525 PredDiff:15.10%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for GA
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 16.200
one epoch duration:3.4089746475219727
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 16.580
one epoch duration:3.3975162506103516
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.440
one epoch duration:3.4189703464508057
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.500
one epoch duration:3.4016194343566895
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.580
one epoch duration:3.4482805728912354
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.600
one epoch duration:3.375744104385376
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.500
one epoch duration:3.4203498363494873
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.560
one epoch duration:3.3895981311798096
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.860
one epoch duration:3.3969898223876953
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.200
one epoch duration:3.3933138847351074
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         GA | Ftot=5000 | ΔF:+17.54 ΔR: 9.81 ΔT: 5.01 | MIA:0.4087 PredDiff:22.20%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}
[UNLEARN MODE] Batch-wise curriculum for NG
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -0.3014 (-0.3289)	Accuracy 87.891 (89.336)	Time 23.72
train_accuracy 89.733
one epoch duration:39.88579058647156
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -0.5152 (-0.4388)	Accuracy 91.797 (90.324)	Time 23.69
train_accuracy 90.304
one epoch duration:39.92910861968994
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -0.5950 (-0.5159)	Accuracy 91.797 (90.582)	Time 23.61
train_accuracy 90.638
one epoch duration:39.715811014175415
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -0.5370 (-0.6060)	Accuracy 89.453 (90.793)	Time 23.69
train_accuracy 90.942
one epoch duration:39.83302593231201
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -0.7288 (-0.6842)	Accuracy 91.406 (90.664)	Time 23.48
train_accuracy 90.673
one epoch duration:39.66238045692444
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         NG | Ftot=5000 | ΔF:+23.96 ΔR: 3.90 ΔT: 0.33 | MIA:0.4531 PredDiff:15.93%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for RL
Epoch #0, Learning rate: 0.001

--- [3/8] Experiment FAILED ---
============================================================

--- [4/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: c_proxy
  - unlearning_granularity: sample
  - forget_partition_ordering: easy_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[LOAD] Original model from saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'c_proxy' method...
Partition sizes for forget: [5000]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for FT
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2698 (0.2817)	Accuracy 91.016 (90.184)	Time 17.43
train_accuracy 90.364
one epoch duration:30.65204644203186
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.2619 (0.2558)	Accuracy 89.844 (91.133)	Time 17.36
train_accuracy 90.918
one epoch duration:30.517858743667603
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2393 (0.2550)	Accuracy 90.234 (90.824)	Time 17.36
train_accuracy 90.784
one epoch duration:30.518901586532593
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2585 (0.2503)	Accuracy 92.578 (91.270)	Time 17.49
train_accuracy 91.164
one epoch duration:30.69742727279663
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.2473 (0.2493)	Accuracy 92.188 (91.168)	Time 17.44
train_accuracy 91.249
one epoch duration:30.60663890838623
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.2547 (0.2522)	Accuracy 92.578 (91.199)	Time 17.46
train_accuracy 91.331
one epoch duration:30.599950551986694
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.2669 (0.2474)	Accuracy 91.406 (91.496)	Time 17.61
train_accuracy 91.382
one epoch duration:30.828542470932007
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.1872 (0.2477)	Accuracy 93.750 (91.383)	Time 17.30
train_accuracy 91.531
one epoch duration:30.32419776916504
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2831 (0.2474)	Accuracy 89.844 (91.316)	Time 17.08
train_accuracy 91.373
one epoch duration:30.001234531402588
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.3051 (0.2453)	Accuracy 88.672 (91.371)	Time 17.09
train_accuracy 91.160
one epoch duration:30.02784776687622
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         FT | Ftot=5000 | ΔF:+60.40 ΔR: 0.38 ΔT: 4.77 | MIA:0.4581 PredDiff:14.41%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for FT_l1
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.9340 (0.9505)	Accuracy 89.453 (90.125)	Time 17.41
train_accuracy 90.204
one epoch duration:30.61863088607788
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.7687 (0.7944)	Accuracy 91.797 (90.824)	Time 17.40
train_accuracy 90.644
one epoch duration:30.617287158966064
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.6955 (0.6520)	Accuracy 91.016 (91.223)	Time 17.42
train_accuracy 91.040
one epoch duration:30.719663858413696
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.5748 (0.5188)	Accuracy 89.062 (91.078)	Time 17.60
train_accuracy 91.211
one epoch duration:30.92990803718567
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.3371 (0.3862)	Accuracy 94.141 (91.250)	Time 17.53
train_accuracy 91.116
one epoch duration:30.859251260757446
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2327 (0.2522)	Accuracy 92.969 (91.234)	Time 17.60
train_accuracy 91.213
one epoch duration:30.962855339050293
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2227 (0.2503)	Accuracy 92.188 (91.336)	Time 17.60
train_accuracy 91.293
one epoch duration:30.998311519622803
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.2996 (0.2567)	Accuracy 89.844 (91.094)	Time 17.56
train_accuracy 91.127
one epoch duration:30.848795652389526
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2254 (0.2500)	Accuracy 91.016 (91.297)	Time 17.53
train_accuracy 91.151
one epoch duration:30.796478509902954
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2553 (0.2570)	Accuracy 91.797 (90.848)	Time 17.51
train_accuracy 90.916
one epoch duration:30.83694815635681
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)      FT_l1 | Ftot=5000 | ΔF:+70.28 ΔR: 0.11 ΔT: 5.65 | MIA:0.4525 PredDiff:15.10%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for GA
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 12.580
one epoch duration:3.3809103965759277
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 13.000
one epoch duration:3.3787052631378174
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 12.240
one epoch duration:3.335254192352295
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 12.080
one epoch duration:3.3243141174316406
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 12.660
one epoch duration:3.318828821182251
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 12.140
one epoch duration:3.324768543243408
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 12.280
one epoch duration:3.3178179264068604
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 11.920
one epoch duration:3.320152997970581
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 12.480
one epoch duration:3.3255417346954346
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 11.180
one epoch duration:3.3248114585876465
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         GA | Ftot=5000 | ΔF:+29.34 ΔR: 9.08 ΔT: 3.43 | MIA:0.4170 PredDiff:21.90%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}
[UNLEARN MODE] Sample-wise curriculum for NG
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -0.4210 (-0.3327)	Accuracy 90.234 (89.285)	Time 23.48
train_accuracy 89.522
one epoch duration:39.37348651885986
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -0.4470 (-0.4393)	Accuracy 88.672 (90.648)	Time 23.37
train_accuracy 90.518
one epoch duration:39.27474117279053
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -0.6639 (-0.5284)	Accuracy 93.750 (90.574)	Time 23.32
train_accuracy 90.598
one epoch duration:39.1080482006073
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -0.7276 (-0.6169)	Accuracy 92.578 (90.961)	Time 23.14
train_accuracy 90.813
one epoch duration:38.87209153175354
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -0.8012 (-0.7069)	Accuracy 89.453 (90.910)	Time 23.06
train_accuracy 90.991
one epoch duration:38.775020360946655
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         NG | Ftot=5000 | ΔF:+25.26 ΔR: 4.39 ΔT: 0.53 | MIA:0.4547 PredDiff:16.48%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for RL
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.6539 (0.7592)	Accuracy 81.641 (82.510)	Time 13.91
Epoch: [0][199/196]	Loss 0.5357 (0.6380)	Accuracy 82.812 (82.804)	Time 17.36
one epoch duration:34.07779097557068
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.5452 (0.5151)	Accuracy 80.859 (82.729)	Time 13.90
Epoch: [1][199/196]	Loss 0.5320 (0.5100)	Accuracy 80.859 (82.806)	Time 17.35
one epoch duration:34.03802180290222
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.3536 (0.4925)	Accuracy 87.109 (82.988)	Time 13.88
Epoch: [2][199/196]	Loss 0.4335 (0.4933)	Accuracy 85.547 (82.958)	Time 17.35
one epoch duration:34.020745515823364
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.5046 (0.4769)	Accuracy 82.422 (83.530)	Time 13.88
Epoch: [3][199/196]	Loss 0.4713 (0.4890)	Accuracy 83.203 (82.999)	Time 17.34
one epoch duration:34.02252221107483
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.6059 (0.4930)	Accuracy 77.734 (82.900)	Time 13.82
Epoch: [4][199/196]	Loss 0.5334 (0.4851)	Accuracy 80.469 (83.158)	Time 17.14
one epoch duration:33.731786251068115
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.6506 (0.4856)	Accuracy 76.172 (83.101)	Time 13.71
Epoch: [5][199/196]	Loss 0.5012 (0.4852)	Accuracy 82.812 (83.108)	Time 17.11
one epoch duration:33.57362627983093
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.5288 (0.4935)	Accuracy 82.031 (82.788)	Time 13.73
Epoch: [6][199/196]	Loss 0.4352 (0.4881)	Accuracy 85.938 (82.914)	Time 17.15
one epoch duration:33.65959405899048
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.4261 (0.4813)	Accuracy 84.375 (83.228)	Time 14.03
Epoch: [7][199/196]	Loss 0.4521 (0.4849)	Accuracy 84.766 (83.108)	Time 17.40
one epoch duration:34.26007342338562
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.4762 (0.4872)	Accuracy 83.594 (83.027)	Time 13.95
Epoch: [8][199/196]	Loss 0.5186 (0.4839)	Accuracy 80.078 (83.097)	Time 17.39
one epoch duration:34.15074181556702
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.5453 (0.4759)	Accuracy 80.078 (83.394)	Time 13.92
Epoch: [9][199/196]	Loss 0.4592 (0.4854)	Accuracy 84.375 (83.008)	Time 17.41
one epoch duration:34.16979455947876
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         RL | Ftot=5000 | ΔF:+5.34 ΔR: 0.31 ΔT: 0.18 | MIA:0.4477 PredDiff:12.61%

===== Running Method: Wfisher =====
  > Applied specific params for Wfisher: {'alpha': 10.0}
[UNLEARN MODE] Sample-wise curriculum for Wfisher
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)    Wfisher | Ftot=5000 | ΔF:+0.00 ΔR:81.81 ΔT:66.20 | MIA:0.5000 PredDiff:86.74%

===== Running Method: SCRUB =====
  > Applied specific params for SCRUB: {'unlearn_epochs': 10, 'kd_T': 4.0, 'gamma': 1.0, 'beta': 1.0, 'msteps': 5, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for SCRUB
Epoch #0, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [0][19/20]	Time 0.105 (0.171)	Data 0.070 (0.124)	Loss -14.7018 (-16.0775)	Forget_Acc@1 0.000 (6.940)
*** Minimize step ***
Epoch: [0][175/176]	Time 0.150 (0.174)	Data 0.107 (0.127)	Loss 0.3789 (1.2761)	Retain_Acc@1 90.000 (88.784)
Epoch: [0]	 train-acc:	88.78444444444445	 train-loss: 1.2760763013415866
one epoch duration:34.06794214248657
Epoch #1, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [1][19/20]	Time 0.100 (0.167)	Data 0.068 (0.122)	Loss -20.9548 (-24.0996)	Forget_Acc@1 0.000 (1.940)
*** Minimize step ***
Epoch: [1][175/176]	Time 0.149 (0.171)	Data 0.105 (0.125)	Loss 0.6160 (1.9126)	Retain_Acc@1 92.500 (88.887)
Epoch: [1]	 train-acc:	88.88666666666667	 train-loss: 1.9125882211897107
one epoch duration:33.49558997154236
Epoch #2, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [2][19/20]	Time 0.100 (0.168)	Data 0.068 (0.122)	Loss -26.7933 (-30.6138)	Forget_Acc@1 0.000 (1.240)
*** Minimize step ***
Epoch: [2][175/176]	Time 0.145 (0.172)	Data 0.102 (0.125)	Loss 0.7370 (2.3377)	Retain_Acc@1 92.500 (88.216)
Epoch: [2]	 train-acc:	88.21555555555555	 train-loss: 2.337736943774753
one epoch duration:33.61712622642517
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [3][19/20]	Time 0.100 (0.169)	Data 0.068 (0.122)	Loss -10.8457 (-27.7194)	Forget_Acc@1 0.000 (1.300)
*** Minimize step ***
Epoch: [3][175/176]	Time 0.148 (0.172)	Data 0.103 (0.126)	Loss 0.8731 (1.0020)	Retain_Acc@1 90.000 (89.642)
Epoch: [3]	 train-acc:	89.64222222222222	 train-loss: 1.0019833856900533
one epoch duration:33.6316351890564
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [4][19/20]	Time 0.102 (0.169)	Data 0.068 (0.123)	Loss -12.4141 (-31.7380)	Forget_Acc@1 0.735 (1.080)
*** Minimize step ***
Epoch: [4][175/176]	Time 0.146 (0.168)	Data 0.102 (0.123)	Loss 0.8179 (1.0536)	Retain_Acc@1 92.000 (89.731)
Epoch: [4]	 train-acc:	89.7311111111111	 train-loss: 1.053591651058197
one epoch duration:32.998783826828
Epoch #5, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [5][19/20]	Time 0.096 (0.166)	Data 0.065 (0.121)	Loss -14.3339 (-34.8127)	Forget_Acc@1 0.735 (0.940)
*** Minimize step ***
Epoch: [5][175/176]	Time 0.141 (0.168)	Data 0.098 (0.123)	Loss 0.9888 (1.1111)	Retain_Acc@1 90.500 (89.873)
Epoch: [5]	 train-acc:	89.87333333333333	 train-loss: 1.111060152498881
one epoch duration:32.904338359832764
Epoch #6, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [6][175/176]	Time 0.145 (0.167)	Data 0.102 (0.123)	Loss 0.8927 (0.8550)	Retain_Acc@1 87.500 (90.113)
Epoch: [6]	 train-acc:	90.11333333333333	 train-loss: 0.8550008764902751
one epoch duration:29.32953453063965
Epoch #7, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [7][175/176]	Time 0.144 (0.168)	Data 0.103 (0.122)	Loss 0.6870 (0.7556)	Retain_Acc@1 89.500 (90.087)
Epoch: [7]	 train-acc:	90.08666666666667	 train-loss: 0.7555652742915683
one epoch duration:29.57543635368347
Epoch #8, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [8][175/176]	Time 0.142 (0.167)	Data 0.100 (0.122)	Loss 0.6337 (0.6879)	Retain_Acc@1 90.000 (90.198)
Epoch: [8]	 train-acc:	90.19777777777777	 train-loss: 0.6878610482851665
one epoch duration:29.465519905090332
Epoch #9, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [9][175/176]	Time 0.141 (0.167)	Data 0.098 (0.122)	Loss 0.5892 (0.6304)	Retain_Acc@1 89.500 (90.351)
Epoch: [9]	 train-acc:	90.35111111111111	 train-loss: 0.6304168408075969
one epoch duration:29.440483331680298
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)      SCRUB | Ftot=5000 | ΔF:+79.82 ΔR: 0.71 ΔT: 6.53 | MIA:0.4502 PredDiff:16.44%

===== Full Results =====
 method stage  forget_total  Retrain_F  Retrain_R  Retrain_T  Unlearn_F  Unlearn_R  Unlearn_T    ΔF        ΔR    ΔT      MIA  PredDiff(%)
     FT final          5000        0.0      92.92       76.2      60.40  93.304444      80.97 60.40  0.384444  4.77 0.458067       14.410
  FT_l1 final          5000        0.0      92.92       76.2      70.28  93.028889      81.85 70.28  0.108889  5.65 0.452544       15.104
     GA final          5000        0.0      92.92       76.2      29.34  83.840000      72.77 29.34  9.080000  3.43 0.416978       21.900
     NG final          5000        0.0      92.92       76.2      25.26  88.533333      75.67 25.26  4.386667  0.53 0.454689       16.478
     RL final          5000        0.0      92.92       76.2       5.34  93.228889      76.38  5.34  0.308889  0.18 0.447667       12.606
Wfisher final          5000        0.0      92.92       76.2       0.00  11.111111      10.00  0.00 81.808889 66.20 0.500000       86.742
  SCRUB final          5000        0.0      92.92       76.2      79.82  92.213333      82.73 79.82  0.706667  6.53 0.450156       16.442

Results saved to saved_models/results_class_c_proxy_sample_easy_first.csv

--- [4/8] Experiment FINISHED ---
============================================================
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_all_conditions.py", line 91, in main
    run_experiment(run_config)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_experiment.py", line 166, in run_experiment
    model_u = unlearn_fn(model_u, loaders, method_config)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/methods.py", line 95, in unlearn_rl
    return _run_iterative_unlearn(unlearn_method, m, loaders, cfg)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/methods.py", line 38, in _run_iterative_unlearn
    unlearn_fn(
  File "/workspace/machine-unlearning/curriculum_unlearning/official/unlearn_methods/impl.py", line 284, in _wrapped
    train_acc = unlearn_iter_func(
  File "/workspace/machine-unlearning/curriculum_unlearning/official/unlearn_methods/RL_original.py", line 24, in RL_og
    original_dataset = forget_dataset.dataset
AttributeError: 'InterleavedDataset' object has no attribute 'dataset'
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
Traceback (most recent call last):
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_all_conditions.py", line 106, in <module>
    main(args)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_all_conditions.py", line 81, in main
    fname_parts.append(f"paired_{exp_params['retain_ordering']}")
KeyError: 'retain_ordering'

ERROR conda.cli.main_run:execute(127): `conda run python run_all_conditions.py --config_module config_cproxy` failed. (See above for error)

