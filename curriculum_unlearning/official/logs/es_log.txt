nohup: ignoring input
Loading configuration from: config_es.py
Target score methods for this run: ['es']
============================================================
Total experiments to run in this process: 8
============================================================

--- [1/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: es
  - unlearning_granularity: stage
  - forget_partition_ordering: easy_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[TRAIN] Original Model
    Epoch 1/30  30.07s
    Epoch 2/30  30.37s
    Epoch 3/30  30.46s
    Epoch 4/30  30.57s
    Epoch 5/30  30.91s
    Epoch 6/30  30.54s
    Epoch 7/30  31.36s
    Epoch 8/30  30.84s
    Epoch 9/30  31.74s
    Epoch 10/30  31.43s
    Epoch 11/30  30.53s
    Epoch 12/30  30.65s
    Epoch 13/30  30.48s
    Epoch 14/30  30.56s
    Epoch 15/30  30.92s
    Epoch 16/30  30.09s
    Epoch 17/30  30.66s
    Epoch 18/30  30.76s
    Epoch 19/30  30.42s
    Epoch 20/30  30.66s
    Epoch 21/30  30.63s
    Epoch 22/30  30.79s
    Epoch 23/30  30.60s
    Epoch 24/30  30.57s
    Epoch 25/30  30.55s
    Epoch 26/30  30.61s
    Epoch 27/30  30.46s
    Epoch 28/30  30.24s
    Epoch 29/30  30.15s
    Epoch 30/30  30.43s
[SAVE] saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'es' method...
Partition sizes for forget: [1666, 1666, 1668]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.001
Epoch: [0][99/189]	Loss 0.2383 (0.2721)	Accuracy 92.578 (90.516)	Time 17.42
train_accuracy 90.344
one epoch duration:32.61902093887329
Epoch #1, Learning rate: 0.001
Epoch: [1][99/189]	Loss 0.2450 (0.2684)	Accuracy 89.844 (90.629)	Time 17.59
train_accuracy 90.655
one epoch duration:32.96618962287903
Epoch #2, Learning rate: 0.001
Epoch: [2][99/189]	Loss 0.3126 (0.2705)	Accuracy 89.453 (90.645)	Time 17.00
train_accuracy 90.704
one epoch duration:32.138872146606445
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/189]	Loss 0.2503 (0.2611)	Accuracy 92.188 (90.887)	Time 17.09
train_accuracy 90.739
one epoch duration:32.17230582237244
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/189]	Loss 0.2190 (0.2644)	Accuracy 91.016 (90.793)	Time 17.10
train_accuracy 90.777
one epoch duration:32.19640350341797
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/189]	Loss 0.2198 (0.2662)	Accuracy 92.969 (90.598)	Time 16.99
train_accuracy 90.764
one epoch duration:32.23680830001831
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/189]	Loss 0.2361 (0.2601)	Accuracy 89.844 (90.934)	Time 17.19
train_accuracy 90.959
one epoch duration:32.44500923156738
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/189]	Loss 0.2438 (0.2666)	Accuracy 90.234 (90.559)	Time 17.00
train_accuracy 90.661
one epoch duration:32.20203900337219
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/189]	Loss 0.3572 (0.2677)	Accuracy 86.719 (90.586)	Time 17.16
train_accuracy 90.698
one epoch duration:32.234336376190186
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/189]	Loss 0.2637 (0.2567)	Accuracy 90.625 (90.922)	Time 16.69
train_accuracy 90.982
one epoch duration:31.958093404769897
[TRAIN] Retrain on 48334 samples
    Epoch 1/30  29.83s
    Epoch 2/30  31.05s
    Epoch 3/30  30.35s
    Epoch 4/30  30.21s
    Epoch 5/30  30.15s
    Epoch 6/30  30.12s
    Epoch 7/30  30.34s
    Epoch 8/30  30.11s
    Epoch 9/30  30.48s
    Epoch 10/30  30.69s
    Epoch 11/30  30.69s
    Epoch 12/30  30.00s
    Epoch 13/30  29.86s
    Epoch 14/30  29.48s
    Epoch 15/30  29.65s
    Epoch 16/30  29.68s
    Epoch 17/30  30.29s
    Epoch 18/30  30.44s
    Epoch 19/30  29.97s
    Epoch 20/30  30.44s
    Epoch 21/30  30.26s
    Epoch 22/30  30.07s
    Epoch 23/30  29.85s
    Epoch 24/30  29.40s
    Epoch 25/30  29.71s
    Epoch 26/30  29.42s
    Epoch 27/30  29.51s
    Epoch 28/30  29.69s
    Epoch 29/30  30.06s
    Epoch 30/30  29.58s
[SAVE] saved_models/retrain_412af551830acc6c4ee8553f289c9261db4c9403_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S1 | Ftot=1666 | ΔF:+11.58 ΔR: 0.32 ΔT: 0.21 | MIA:0.4561 PredDiff:8.39%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.001
Epoch: [0][99/183]	Loss 0.2378 (0.2628)	Accuracy 94.531 (90.797)	Time 17.10
train_accuracy 90.595
one epoch duration:31.1574125289917
Epoch #1, Learning rate: 0.001
Epoch: [1][99/183]	Loss 0.2568 (0.2607)	Accuracy 90.234 (91.016)	Time 17.13
train_accuracy 90.889
one epoch duration:31.23405361175537
Epoch #2, Learning rate: 0.001
Epoch: [2][99/183]	Loss 0.2289 (0.2589)	Accuracy 92.969 (91.098)	Time 17.16
train_accuracy 90.861
one epoch duration:31.34734869003296
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/183]	Loss 0.2518 (0.2528)	Accuracy 91.406 (91.160)	Time 17.10
train_accuracy 90.940
one epoch duration:31.22407817840576
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/183]	Loss 0.2541 (0.2560)	Accuracy 93.359 (91.129)	Time 17.25
train_accuracy 91.052
one epoch duration:31.07593011856079
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/183]	Loss 0.2376 (0.2496)	Accuracy 90.625 (91.316)	Time 16.71
train_accuracy 91.105
one epoch duration:30.672061681747437
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/183]	Loss 0.3283 (0.2493)	Accuracy 89.062 (91.250)	Time 17.06
train_accuracy 91.180
one epoch duration:31.084187984466553
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/183]	Loss 0.1486 (0.2537)	Accuracy 92.969 (90.855)	Time 17.08
train_accuracy 90.887
one epoch duration:31.13376545906067
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/183]	Loss 0.3179 (0.2556)	Accuracy 91.016 (91.047)	Time 17.15
train_accuracy 91.058
one epoch duration:31.120315074920654
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/183]	Loss 0.2366 (0.2571)	Accuracy 91.797 (90.840)	Time 17.10
train_accuracy 91.127
one epoch duration:31.351985931396484
[TRAIN] Retrain on 46668 samples
    Epoch 1/30  28.35s
    Epoch 2/30  29.42s
    Epoch 3/30  29.80s
    Epoch 4/30  29.78s
    Epoch 5/30  29.54s
    Epoch 6/30  29.17s
    Epoch 7/30  28.97s
    Epoch 8/30  28.39s
    Epoch 9/30  28.74s
    Epoch 10/30  28.91s
    Epoch 11/30  29.49s
    Epoch 12/30  29.42s
    Epoch 13/30  29.77s
    Epoch 14/30  29.17s
    Epoch 15/30  29.96s
    Epoch 16/30  28.93s
    Epoch 17/30  29.62s
    Epoch 18/30  29.17s
    Epoch 19/30  29.05s
    Epoch 20/30  29.89s
    Epoch 21/30  29.40s
    Epoch 22/30  28.88s
    Epoch 23/30  29.29s
    Epoch 24/30  30.09s
    Epoch 25/30  28.82s
    Epoch 26/30  29.07s
    Epoch 27/30  29.95s
    Epoch 28/30  29.39s
    Epoch 29/30  30.30s
    Epoch 30/30  29.13s
[SAVE] saved_models/retrain_2fc7fa8471d9532f062f5915fe46ce69f74174e1_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S2 | Ftot=3332 | ΔF:+31.72 ΔR: 0.99 ΔT: 1.83 | MIA:0.4609 PredDiff:10.48%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2284 (0.2526)	Accuracy 92.578 (91.227)	Time 16.94
train_accuracy 91.389
one epoch duration:30.00812554359436
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.2835 (0.2392)	Accuracy 91.797 (91.566)	Time 17.36
train_accuracy 91.438
one epoch duration:30.716511726379395
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2300 (0.2406)	Accuracy 92.188 (91.566)	Time 17.05
train_accuracy 91.504
one epoch duration:29.978812217712402
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2871 (0.2351)	Accuracy 89.453 (91.887)	Time 17.23
train_accuracy 91.880
one epoch duration:30.27006196975708
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.1783 (0.2370)	Accuracy 93.359 (91.582)	Time 17.32
train_accuracy 91.778
one epoch duration:30.33494472503662
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.1728 (0.2345)	Accuracy 92.578 (91.863)	Time 17.70
train_accuracy 91.791
one epoch duration:30.966604709625244
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.2278 (0.2390)	Accuracy 91.406 (91.617)	Time 17.49
train_accuracy 91.827
one epoch duration:30.878461599349976
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.2443 (0.2334)	Accuracy 91.016 (91.793)	Time 17.35
train_accuracy 91.789
one epoch duration:30.47359037399292
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2528 (0.2299)	Accuracy 90.625 (91.938)	Time 17.47
train_accuracy 91.891
one epoch duration:30.68321943283081
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.2879 (0.2308)	Accuracy 89.062 (91.754)	Time 17.58
train_accuracy 91.871
one epoch duration:30.71671724319458
[TRAIN] Retrain on 45000 samples
    Epoch 1/30  28.47s
    Epoch 2/30  28.61s
    Epoch 3/30  28.48s
    Epoch 4/30  28.42s
    Epoch 5/30  28.81s
    Epoch 6/30  28.42s
    Epoch 7/30  28.06s
    Epoch 8/30  27.80s
    Epoch 9/30  28.09s
    Epoch 10/30  27.82s
    Epoch 11/30  28.23s
    Epoch 12/30  28.08s
    Epoch 13/30  28.14s
    Epoch 14/30  28.94s
    Epoch 15/30  29.00s
    Epoch 16/30  28.18s
    Epoch 17/30  28.12s
    Epoch 18/30  28.49s
    Epoch 19/30  28.35s
    Epoch 20/30  28.28s
    Epoch 21/30  29.28s
    Epoch 22/30  28.35s
    Epoch 23/30  27.93s
    Epoch 24/30  28.09s
    Epoch 25/30  28.57s
    Epoch 26/30  28.50s
    Epoch 27/30  29.08s
    Epoch 28/30  28.84s
    Epoch 29/30  29.08s
    Epoch 30/30  28.44s
[SAVE] saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S3 | Ftot=5000 | ΔF:+61.00 ΔR: 0.91 ΔT: 5.30 | MIA:0.4534 PredDiff:14.41%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/189]	Loss 0.8847 (0.9411)	Accuracy 93.359 (90.430)	Time 17.37
train_accuracy 90.487
one epoch duration:32.82175874710083
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/189]	Loss 0.7829 (0.8026)	Accuracy 91.797 (90.633)	Time 17.45
train_accuracy 90.622
one epoch duration:32.74362540245056
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/189]	Loss 0.6971 (0.6727)	Accuracy 89.844 (90.309)	Time 17.33
train_accuracy 90.442
one epoch duration:32.64215850830078
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/189]	Loss 0.5431 (0.5350)	Accuracy 90.234 (90.473)	Time 17.37
train_accuracy 90.483
one epoch duration:32.869038820266724
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/189]	Loss 0.3927 (0.3996)	Accuracy 90.234 (90.703)	Time 17.70
train_accuracy 90.677
one epoch duration:33.199037075042725
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/189]	Loss 0.2727 (0.2646)	Accuracy 90.234 (90.742)	Time 17.36
train_accuracy 90.557
one epoch duration:33.022457122802734
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/189]	Loss 0.1910 (0.2649)	Accuracy 94.141 (90.797)	Time 17.87
train_accuracy 90.615
one epoch duration:33.93463635444641
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/189]	Loss 0.2059 (0.2667)	Accuracy 93.359 (90.812)	Time 18.36
train_accuracy 90.826
one epoch duration:34.290162563323975
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/189]	Loss 0.2964 (0.2697)	Accuracy 89.453 (90.633)	Time 17.95
train_accuracy 90.648
one epoch duration:33.54188394546509
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/189]	Loss 0.2353 (0.2664)	Accuracy 91.016 (90.613)	Time 17.55
train_accuracy 90.768
one epoch duration:33.29480338096619
[LOAD] Retrain from saved_models/retrain_412af551830acc6c4ee8553f289c9261db4c9403_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S1 | Ftot=1666 | ΔF:+11.52 ΔR: 0.45 ΔT: 0.15 | MIA:0.4599 PredDiff:8.41%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/183]	Loss 0.9600 (0.9343)	Accuracy 87.109 (90.645)	Time 17.38
train_accuracy 90.735
one epoch duration:31.553710460662842
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/183]	Loss 0.8102 (0.7904)	Accuracy 90.625 (90.898)	Time 17.44
train_accuracy 90.855
one epoch duration:31.727996110916138
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/183]	Loss 0.7098 (0.6621)	Accuracy 89.453 (90.727)	Time 17.29
train_accuracy 90.636
one epoch duration:31.491280794143677
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/183]	Loss 0.6152 (0.5258)	Accuracy 86.719 (90.855)	Time 17.59
train_accuracy 90.855
one epoch duration:31.94228196144104
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/183]	Loss 0.3231 (0.3948)	Accuracy 93.359 (90.879)	Time 17.52
train_accuracy 90.799
one epoch duration:31.98053002357483
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/183]	Loss 0.2610 (0.2661)	Accuracy 92.578 (90.668)	Time 17.85
train_accuracy 90.773
one epoch duration:32.4120237827301
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/183]	Loss 0.2173 (0.2557)	Accuracy 92.969 (91.207)	Time 17.83
train_accuracy 91.097
one epoch duration:32.48086619377136
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/183]	Loss 0.2912 (0.2595)	Accuracy 91.406 (91.098)	Time 17.93
train_accuracy 91.028
one epoch duration:32.282946825027466
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/183]	Loss 0.2515 (0.2635)	Accuracy 91.797 (90.754)	Time 17.57
train_accuracy 90.829
one epoch duration:32.06706166267395
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/183]	Loss 0.2745 (0.2598)	Accuracy 90.625 (90.863)	Time 17.48
train_accuracy 90.775
one epoch duration:32.84001612663269
[LOAD] Retrain from saved_models/retrain_2fc7fa8471d9532f062f5915fe46ce69f74174e1_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S2 | Ftot=3332 | ΔF:+32.20 ΔR: 0.67 ΔT: 1.79 | MIA:0.7510 PredDiff:10.41%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.8746 (0.9163)	Accuracy 91.797 (90.996)	Time 17.50
train_accuracy 91.129
one epoch duration:30.74438238143921
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.8305 (0.7772)	Accuracy 87.500 (91.250)	Time 17.45
train_accuracy 91.358
one epoch duration:30.65401792526245
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.6101 (0.6424)	Accuracy 92.188 (91.441)	Time 17.41
train_accuracy 91.622
one epoch duration:30.585963487625122
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.4939 (0.5087)	Accuracy 90.234 (91.277)	Time 17.37
train_accuracy 91.447
one epoch duration:30.581679821014404
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.3405 (0.3735)	Accuracy 93.750 (91.430)	Time 17.38
train_accuracy 91.553
one epoch duration:30.57530975341797
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2187 (0.2431)	Accuracy 91.406 (91.309)	Time 17.39
train_accuracy 91.436
one epoch duration:30.59281039237976
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2802 (0.2390)	Accuracy 87.891 (91.789)	Time 17.25
train_accuracy 91.651
one epoch duration:30.264885902404785
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.1911 (0.2410)	Accuracy 94.531 (91.512)	Time 17.07
train_accuracy 91.536
one epoch duration:29.98671865463257
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2462 (0.2408)	Accuracy 91.016 (91.609)	Time 17.03
train_accuracy 91.384
one epoch duration:29.98550295829773
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2456 (0.2419)	Accuracy 91.406 (91.613)	Time 17.11
train_accuracy 91.513
one epoch duration:30.09185791015625
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S3 | Ftot=5000 | ΔF:+70.96 ΔR: 0.43 ΔT: 5.78 | MIA:0.4516 PredDiff:15.11%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 14.466
one epoch duration:1.106435775756836
Epoch #1, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 14.286
one epoch duration:1.1156055927276611
Epoch #2, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 14.766
one epoch duration:1.1053643226623535
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.466
one epoch duration:1.0854065418243408
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 15.006
one epoch duration:1.0863382816314697
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.886
one epoch duration:1.1055889129638672
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.166
one epoch duration:1.0887486934661865
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 15.306
one epoch duration:1.0955686569213867
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.286
one epoch duration:1.0939130783081055
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 14.466
one epoch duration:1.1055090427398682
[LOAD] Retrain from saved_models/retrain_412af551830acc6c4ee8553f289c9261db4c9403_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S1 | Ftot=1666 | ΔF:-72.87 ΔR:15.04 ΔT:13.29 | MIA:0.7338 PredDiff:24.01%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 14.736
one epoch duration:2.2444493770599365
Epoch #1, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 14.346
one epoch duration:2.21681809425354
Epoch #2, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 14.286
one epoch duration:2.2150375843048096
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.376
one epoch duration:2.2232601642608643
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.436
one epoch duration:2.2175395488739014
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.346
one epoch duration:2.2177441120147705
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.526
one epoch duration:2.217945098876953
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.526
one epoch duration:2.215581178665161
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 14.496
one epoch duration:2.2436001300811768
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 13.806
one epoch duration:2.2127418518066406
[LOAD] Retrain from saved_models/retrain_2fc7fa8471d9532f062f5915fe46ce69f74174e1_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S2 | Ftot=3332 | ΔF:-46.55 ΔR:11.37 ΔT:10.83 | MIA:0.7877 PredDiff:22.69%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.780
one epoch duration:3.2964820861816406
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.320
one epoch duration:3.2551372051239014
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 14.560
one epoch duration:3.2995786666870117
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.300
one epoch duration:3.303318977355957
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.760
one epoch duration:3.2961676120758057
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.640
one epoch duration:3.2862563133239746
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.780
one epoch duration:3.2667384147644043
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.300
one epoch duration:3.2969417572021484
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.220
one epoch duration:3.2958595752716064
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.940
one epoch duration:3.293010950088501
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S3 | Ftot=5000 | ΔF:+16.66 ΔR:10.12 ΔT: 5.27 | MIA:0.4111 PredDiff:22.48%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [0][99/189]	Loss -0.3834 (-0.3518)	Accuracy 91.016 (90.277)	Time 21.44
train_accuracy 90.270
one epoch duration:39.85735273361206
Epoch #1, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [1][99/189]	Loss -0.4394 (-0.4260)	Accuracy 89.844 (90.559)	Time 21.47
train_accuracy 90.489
one epoch duration:39.88351011276245
Epoch #2, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [2][99/189]	Loss -0.5309 (-0.4979)	Accuracy 88.672 (90.031)	Time 21.51
train_accuracy 90.262
one epoch duration:39.858386516571045
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [3][99/189]	Loss -0.6210 (-0.5753)	Accuracy 91.016 (90.375)	Time 21.50
train_accuracy 90.164
one epoch duration:39.86820363998413
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [4][99/189]	Loss -0.6636 (-0.6449)	Accuracy 90.625 (90.141)	Time 21.50
train_accuracy 90.255
one epoch duration:39.82714629173279
[LOAD] Retrain from saved_models/retrain_412af551830acc6c4ee8553f289c9261db4c9403_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S1 | Ftot=1666 | ΔF:-41.66 ΔR: 8.05 ΔT: 6.97 | MIA:0.4623 PredDiff:16.29%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [0][99/183]	Loss -0.6320 (-0.7231)	Accuracy 89.453 (90.270)	Time 22.40
train_accuracy 90.360
one epoch duration:39.486347913742065
Epoch #1, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [1][99/183]	Loss -0.8807 (-0.8089)	Accuracy 90.234 (90.207)	Time 22.11
train_accuracy 90.184
one epoch duration:39.13556528091431
Epoch #2, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [2][99/183]	Loss -0.8405 (-0.8732)	Accuracy 87.891 (89.891)	Time 22.23
train_accuracy 89.982
one epoch duration:39.275558948516846
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [3][99/183]	Loss -0.9045 (-0.9620)	Accuracy 88.281 (90.035)	Time 22.45
train_accuracy 90.023
one epoch duration:39.63754868507385
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [4][99/183]	Loss -1.0786 (-1.0577)	Accuracy 89.453 (89.504)	Time 22.41
train_accuracy 89.592
one epoch duration:39.59971809387207
[LOAD] Retrain from saved_models/retrain_2fc7fa8471d9532f062f5915fe46ce69f74174e1_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S2 | Ftot=3332 | ΔF:-38.00 ΔR: 5.47 ΔT: 5.98 | MIA:0.8122 PredDiff:16.83%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -1.1796 (-1.1534)	Accuracy 90.625 (90.684)	Time 23.34
train_accuracy 90.753
one epoch duration:39.23444199562073
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -1.3191 (-1.2713)	Accuracy 89.844 (90.840)	Time 23.32
train_accuracy 90.713
one epoch duration:39.19618535041809
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -1.3806 (-1.3660)	Accuracy 91.406 (90.461)	Time 23.06
train_accuracy 90.520
one epoch duration:38.74827575683594
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -1.5310 (-1.4861)	Accuracy 90.625 (90.781)	Time 22.99
train_accuracy 90.756
one epoch duration:38.659634590148926
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -1.6963 (-1.5894)	Accuracy 94.141 (90.820)	Time 23.03
train_accuracy 90.664
one epoch duration:38.769365549087524
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S3 | Ftot=5000 | ΔF:+2.82 ΔR: 5.16 ΔT: 3.19 | MIA:0.4498 PredDiff:16.30%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.4051 (0.4851)	Accuracy 88.281 (87.357)	Time 16.16
Epoch: [0][199/196]	Loss 0.5456 (0.4558)	Accuracy 87.891 (87.318)	Time 17.17
one epoch duration:35.479142904281616
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.4137 (0.4093)	Accuracy 89.062 (87.391)	Time 16.10
Epoch: [1][199/196]	Loss 0.3263 (0.4012)	Accuracy 89.844 (87.526)	Time 17.29
one epoch duration:33.921963691711426
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.4165 (0.3833)	Accuracy 87.109 (88.105)	Time 16.16
Epoch: [2][199/196]	Loss 0.4040 (0.3861)	Accuracy 88.281 (87.966)	Time 17.31
one epoch duration:34.01086688041687
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.3833 (0.3832)	Accuracy 86.328 (88.067)	Time 16.13
Epoch: [3][199/196]	Loss 0.3606 (0.3815)	Accuracy 90.625 (88.030)	Time 17.15
one epoch duration:33.817699909210205
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.4345 (0.3845)	Accuracy 85.156 (87.815)	Time 16.15
Epoch: [4][199/196]	Loss 0.3460 (0.3784)	Accuracy 91.797 (88.083)	Time 17.24
one epoch duration:33.92495250701904
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.3995 (0.3730)	Accuracy 84.375 (88.185)	Time 15.93
Epoch: [5][199/196]	Loss 0.3786 (0.3759)	Accuracy 87.891 (87.974)	Time 17.05
one epoch duration:33.508970737457275
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.3810 (0.3871)	Accuracy 89.453 (87.836)	Time 15.97
Epoch: [6][199/196]	Loss 0.3569 (0.3789)	Accuracy 91.406 (88.051)	Time 17.15
one epoch duration:33.65972709655762
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.3176 (0.3759)	Accuracy 90.625 (88.075)	Time 15.92
Epoch: [7][199/196]	Loss 0.3856 (0.3791)	Accuracy 87.891 (88.038)	Time 17.13
one epoch duration:33.58681106567383
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.2942 (0.3725)	Accuracy 89.062 (88.151)	Time 15.97
Epoch: [8][199/196]	Loss 0.3833 (0.3751)	Accuracy 86.719 (87.984)	Time 17.17
one epoch duration:33.678823471069336
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.3194 (0.3789)	Accuracy 90.625 (87.954)	Time 15.97
Epoch: [9][199/196]	Loss 0.3612 (0.3759)	Accuracy 88.281 (88.170)	Time 17.14
one epoch duration:33.649784326553345
[LOAD] Retrain from saved_models/retrain_412af551830acc6c4ee8553f289c9261db4c9403_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S1 | Ftot=1666 | ΔF:+11.10 ΔR: 0.49 ΔT: 0.03 | MIA:0.8390 PredDiff:8.50%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.001
Epoch: [0][99/197]	Loss 0.3999 (0.4547)	Accuracy 86.328 (85.392)	Time 14.57
Epoch: [0][199/197]	Loss 0.4780 (0.4472)	Accuracy 84.766 (85.419)	Time 16.97
one epoch duration:36.195767879486084
Epoch #1, Learning rate: 0.001
Epoch: [1][99/197]	Loss 0.4173 (0.4457)	Accuracy 85.938 (85.411)	Time 14.60
Epoch: [1][199/197]	Loss 0.4290 (0.4457)	Accuracy 85.547 (85.263)	Time 16.94
one epoch duration:33.26400685310364
Epoch #2, Learning rate: 0.001
Epoch: [2][99/197]	Loss 0.4537 (0.4439)	Accuracy 85.156 (85.265)	Time 14.61
Epoch: [2][199/197]	Loss 0.4434 (0.4439)	Accuracy 86.328 (85.324)	Time 16.96
one epoch duration:33.29279708862305
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/197]	Loss 0.3924 (0.4339)	Accuracy 87.500 (85.488)	Time 14.63
Epoch: [3][199/197]	Loss 0.4971 (0.4351)	Accuracy 81.641 (85.425)	Time 17.31
one epoch duration:33.67655611038208
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/197]	Loss 0.3632 (0.4408)	Accuracy 88.281 (85.383)	Time 14.87
Epoch: [4][199/197]	Loss 0.4631 (0.4355)	Accuracy 85.547 (85.538)	Time 17.23
one epoch duration:33.85101628303528
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/197]	Loss 0.3550 (0.4314)	Accuracy 87.891 (85.606)	Time 14.86
Epoch: [5][199/197]	Loss 0.4288 (0.4352)	Accuracy 84.375 (85.482)	Time 17.20
one epoch duration:33.80294346809387
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/197]	Loss 0.3613 (0.4323)	Accuracy 87.500 (85.533)	Time 14.83
Epoch: [6][199/197]	Loss 0.3497 (0.4304)	Accuracy 88.672 (85.610)	Time 17.19
one epoch duration:33.7637403011322
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/197]	Loss 0.4277 (0.4406)	Accuracy 87.891 (85.356)	Time 14.80
Epoch: [7][199/197]	Loss 0.4189 (0.4319)	Accuracy 83.203 (85.627)	Time 17.31
one epoch duration:33.853761196136475
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/197]	Loss 0.4876 (0.4299)	Accuracy 83.984 (85.828)	Time 14.88
Epoch: [8][199/197]	Loss 0.4510 (0.4305)	Accuracy 83.203 (85.595)	Time 17.21
one epoch duration:33.82295060157776
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/197]	Loss 0.4497 (0.4366)	Accuracy 82.422 (85.556)	Time 14.63
Epoch: [9][199/197]	Loss 0.3593 (0.4310)	Accuracy 87.500 (85.631)	Time 17.24
one epoch duration:33.61284852027893
[LOAD] Retrain from saved_models/retrain_2fc7fa8471d9532f062f5915fe46ce69f74174e1_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S2 | Ftot=3332 | ΔF:+23.59 ΔR: 0.77 ΔT: 1.27 | MIA:0.8105 PredDiff:10.34%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.4576 (0.5015)	Accuracy 83.984 (83.237)	Time 13.83
Epoch: [0][199/196]	Loss 0.4369 (0.4884)	Accuracy 83.203 (83.362)	Time 17.23
one epoch duration:34.79671382904053
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.4246 (0.4838)	Accuracy 85.156 (83.096)	Time 13.67
Epoch: [1][199/196]	Loss 0.4740 (0.4777)	Accuracy 83.984 (83.301)	Time 17.23
one epoch duration:33.68774676322937
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.4113 (0.4673)	Accuracy 87.891 (83.354)	Time 13.86
Epoch: [2][199/196]	Loss 0.4888 (0.4689)	Accuracy 81.250 (83.429)	Time 17.23
one epoch duration:33.87481117248535
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.4043 (0.4725)	Accuracy 85.938 (83.594)	Time 13.78
Epoch: [3][199/196]	Loss 0.4336 (0.4709)	Accuracy 85.156 (83.490)	Time 17.35
one epoch duration:33.90835642814636
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.5330 (0.4656)	Accuracy 78.516 (83.535)	Time 13.98
Epoch: [4][199/196]	Loss 0.5457 (0.4645)	Accuracy 80.859 (83.700)	Time 17.29
one epoch duration:34.089789152145386
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.4927 (0.4645)	Accuracy 83.203 (83.496)	Time 13.81
Epoch: [5][199/196]	Loss 0.5040 (0.4650)	Accuracy 82.422 (83.583)	Time 17.29
one epoch duration:33.89449429512024
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.5042 (0.4707)	Accuracy 82.422 (83.438)	Time 13.84
Epoch: [6][199/196]	Loss 0.4249 (0.4674)	Accuracy 83.984 (83.555)	Time 17.23
one epoch duration:33.85126090049744
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.4831 (0.4599)	Accuracy 82.422 (83.667)	Time 13.79
Epoch: [7][199/196]	Loss 0.4612 (0.4648)	Accuracy 82.031 (83.561)	Time 17.27
one epoch duration:33.8413450717926
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.4629 (0.4642)	Accuracy 80.859 (83.740)	Time 13.87
Epoch: [8][199/196]	Loss 0.4127 (0.4623)	Accuracy 86.328 (83.722)	Time 17.35
one epoch duration:34.026262521743774
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.4563 (0.4574)	Accuracy 83.984 (83.979)	Time 13.82
Epoch: [9][199/196]	Loss 0.5056 (0.4600)	Accuracy 82.812 (83.869)	Time 17.50
one epoch duration:34.09094715118408
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S3 | Ftot=5000 | ΔF:+12.06 ΔR: 0.84 ΔT: 1.07 | MIA:0.4534 PredDiff:13.31%

===== Running Method: Wfisher =====
  > Applied specific params for Wfisher: {'alpha': 10.0}

[UNLEARN] Stage 1: |Forget Total|=1666
[LOAD] Retrain from saved_models/retrain_412af551830acc6c4ee8553f289c9261db4c9403_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S1 | Ftot=1666 | ΔF:-88.36 ΔR:78.93 ΔT:70.12 | MIA:0.6734 PredDiff:86.07%

[UNLEARN] Stage 2: |Forget Total|=3332
[LOAD] Retrain from saved_models/retrain_2fc7fa8471d9532f062f5915fe46ce69f74174e1_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S2 | Ftot=3332 | ΔF:-62.79 ΔR:81.31 ΔT:71.82 | MIA:0.5000 PredDiff:89.22%

[UNLEARN] Stage 3: |Forget Total|=5000
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S3 | Ftot=5000 | ΔF:+100.00 ΔR:92.92 ΔT:66.20 | MIA:0.0000 PredDiff:100.00%

===== Running Method: SCRUB =====
  > Applied specific params for SCRUB: {'unlearn_epochs': 10, 'kd_T': 4.0, 'gamma': 1.0, 'beta': 1.0, 'msteps': 5, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1666
Epoch #0, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [0][6/7]	Time 0.099 (0.159)	Data 0.064 (0.115)	Loss -12.7334 (-11.2960)	Forget_Acc@1 9.231 (12.425)
*** Minimize step ***
Epoch: [0][188/189]	Time 0.155 (0.175)	Data 0.108 (0.127)	Loss 0.3712 (0.4081)	Retain_Acc@1 90.291 (90.185)
Epoch: [0]	 train-acc:	90.18496295592583	 train-loss: 0.4080895385998394
one epoch duration:34.232903480529785
Epoch #1, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [1][6/7]	Time 0.100 (0.165)	Data 0.066 (0.117)	Loss -15.3349 (-13.2055)	Forget_Acc@1 6.923 (10.744)
*** Minimize step ***
Epoch: [1][188/189]	Time 0.149 (0.173)	Data 0.106 (0.125)	Loss 0.4166 (0.4438)	Retain_Acc@1 88.350 (90.069)
Epoch: [1]	 train-acc:	90.06910247651201	 train-loss: 0.4437867440580804
one epoch duration:33.820361614227295
Epoch #2, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [2][6/7]	Time 0.100 (0.161)	Data 0.065 (0.117)	Loss -16.6908 (-14.4619)	Forget_Acc@1 6.923 (8.703)
*** Minimize step ***
Epoch: [2][188/189]	Time 0.151 (0.173)	Data 0.106 (0.125)	Loss 0.4047 (0.4719)	Retain_Acc@1 88.350 (90.034)
Epoch: [2]	 train-acc:	90.03393054784895	 train-loss: 0.4718775431859349
one epoch duration:33.79386305809021
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [3][6/7]	Time 0.100 (0.161)	Data 0.066 (0.117)	Loss -14.7779 (-14.2550)	Forget_Acc@1 8.462 (10.204)
*** Minimize step ***
Epoch: [3][188/189]	Time 0.151 (0.172)	Data 0.106 (0.125)	Loss 0.4505 (0.3534)	Retain_Acc@1 88.350 (90.499)
Epoch: [3]	 train-acc:	90.49944136838936	 train-loss: 0.35343725243673113
one epoch duration:33.598474740982056
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [4][6/7]	Time 0.099 (0.161)	Data 0.065 (0.117)	Loss -14.8507 (-15.1436)	Forget_Acc@1 10.769 (9.064)
*** Minimize step ***
Epoch: [4][188/189]	Time 0.152 (0.172)	Data 0.106 (0.126)	Loss 0.3989 (0.3530)	Retain_Acc@1 88.835 (90.530)
Epoch: [4]	 train-acc:	90.5304754496104	 train-loss: 0.35296553068617387
one epoch duration:33.63963222503662
Epoch #5, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [5][6/7]	Time 0.104 (0.162)	Data 0.065 (0.117)	Loss -15.2649 (-15.3886)	Forget_Acc@1 8.462 (9.004)
*** Minimize step ***
Epoch: [5][188/189]	Time 0.153 (0.176)	Data 0.107 (0.128)	Loss 0.3280 (0.3643)	Retain_Acc@1 93.204 (90.514)
Epoch: [5]	 train-acc:	90.51392393230174	 train-loss: 0.36426130315265703
one epoch duration:34.44717860221863
Epoch #6, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [6][188/189]	Time 0.154 (0.174)	Data 0.107 (0.126)	Loss 0.5285 (0.3435)	Retain_Acc@1 82.524 (90.735)
Epoch: [6]	 train-acc:	90.73530019107513	 train-loss: 0.34349402589841777
one epoch duration:32.9286105632782
Epoch #7, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [7][188/189]	Time 0.150 (0.173)	Data 0.107 (0.125)	Loss 0.3216 (0.3400)	Retain_Acc@1 91.262 (90.694)
Epoch: [7]	 train-acc:	90.69392147357016	 train-loss: 0.3399509875436097
one epoch duration:32.70538258552551
Epoch #8, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [8][188/189]	Time 0.150 (0.174)	Data 0.107 (0.125)	Loss 0.2926 (0.3383)	Retain_Acc@1 92.233 (90.590)
Epoch: [8]	 train-acc:	90.59047461256478	 train-loss: 0.3382520784536866
one epoch duration:32.80984902381897
Epoch #9, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [9][188/189]	Time 0.151 (0.176)	Data 0.105 (0.128)	Loss 0.3302 (0.3292)	Retain_Acc@1 90.777 (90.733)
Epoch: [9]	 train-acc:	90.73323124967521	 train-loss: 0.329232917179925
one epoch duration:33.3373236656189
[LOAD] Retrain from saved_models/retrain_412af551830acc6c4ee8553f289c9261db4c9403_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S1 | Ftot=1666 | ΔF:+11.58 ΔR: 0.48 ΔT: 0.15 | MIA:0.4545 PredDiff:8.49%

[UNLEARN] Stage 2: |Forget Total|=3332
Epoch #0, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [0][13/14]	Time 0.046 (0.169)	Data 0.014 (0.121)	Loss -19.0307 (-17.2752)	Forget_Acc@1 0.000 (6.963)
*** Minimize step ***
Epoch: [0][182/183]	Time 0.078 (0.174)	Data 0.045 (0.126)	Loss 0.7359 (0.8656)	Retain_Acc@1 88.158 (88.639)
Epoch: [0]	 train-acc:	88.63889602434874	 train-loss: 0.8655978423050992
one epoch duration:34.266547441482544
Epoch #1, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [1][13/14]	Time 0.047 (0.166)	Data 0.012 (0.119)	Loss -36.7608 (-23.6445)	Forget_Acc@1 0.000 (4.142)
*** Minimize step ***
Epoch: [1][182/183]	Time 0.079 (0.174)	Data 0.044 (0.126)	Loss 0.6238 (1.4766)	Retain_Acc@1 93.421 (87.253)
Epoch: [1]	 train-acc:	87.25250706861083	 train-loss: 1.4765918805174771
one epoch duration:34.20241117477417
Epoch #2, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [2][13/14]	Time 0.046 (0.164)	Data 0.014 (0.118)	Loss -29.2626 (-22.3136)	Forget_Acc@1 0.000 (2.521)
*** Minimize step ***
Epoch: [2][182/183]	Time 0.078 (0.171)	Data 0.044 (0.124)	Loss 0.5233 (1.3456)	Retain_Acc@1 90.789 (87.857)
Epoch: [2]	 train-acc:	87.85677552200727	 train-loss: 1.345609847215144
one epoch duration:33.60628151893616
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [3][13/14]	Time 0.046 (0.160)	Data 0.015 (0.115)	Loss -18.7651 (-21.1489)	Forget_Acc@1 0.000 (4.292)
*** Minimize step ***
Epoch: [3][182/183]	Time 0.075 (0.172)	Data 0.044 (0.126)	Loss 0.5934 (0.6075)	Retain_Acc@1 90.789 (89.807)
Epoch: [3]	 train-acc:	89.80671980931335	 train-loss: 0.6074922424698382
one epoch duration:33.79196500778198
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [4][13/14]	Time 0.047 (0.166)	Data 0.013 (0.120)	Loss -25.2867 (-23.0849)	Forget_Acc@1 0.000 (4.262)
*** Minimize step ***
Epoch: [4][182/183]	Time 0.082 (0.176)	Data 0.046 (0.129)	Loss 0.5268 (0.6709)	Retain_Acc@1 92.105 (89.543)
Epoch: [4]	 train-acc:	89.54315590917722	 train-loss: 0.6709225403625984
one epoch duration:34.53452777862549
Epoch #5, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [5][13/14]	Time 0.044 (0.163)	Data 0.012 (0.118)	Loss -33.4111 (-24.8466)	Forget_Acc@1 0.000 (4.142)
*** Minimize step ***
Epoch: [5][182/183]	Time 0.079 (0.177)	Data 0.045 (0.127)	Loss 0.7693 (0.7240)	Retain_Acc@1 89.474 (89.475)
Epoch: [5]	 train-acc:	89.47458644365706	 train-loss: 0.7240498962444849
one epoch duration:34.64840579032898
Epoch #6, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [6][182/183]	Time 0.078 (0.173)	Data 0.043 (0.126)	Loss 0.6462 (0.5902)	Retain_Acc@1 94.737 (89.725)
Epoch: [6]	 train-acc:	89.72529355846355	 train-loss: 0.5902147081278994
one epoch duration:31.58704662322998
Epoch #7, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [7][182/183]	Time 0.078 (0.172)	Data 0.044 (0.126)	Loss 0.7858 (0.5344)	Retain_Acc@1 86.842 (89.764)
Epoch: [7]	 train-acc:	89.76386388437172	 train-loss: 0.534427647501948
one epoch duration:31.469469785690308
Epoch #8, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [8][182/183]	Time 0.077 (0.171)	Data 0.043 (0.123)	Loss 0.4395 (0.5008)	Retain_Acc@1 94.737 (89.828)
Epoch: [8]	 train-acc:	89.82814776262914	 train-loss: 0.5008109778183698
one epoch duration:31.211838960647583
Epoch #9, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [9][182/183]	Time 0.080 (0.172)	Data 0.045 (0.124)	Loss 0.4222 (0.4767)	Retain_Acc@1 93.421 (90.002)
Epoch: [9]	 train-acc:	90.00171423412037	 train-loss: 0.47672804166278115
one epoch duration:31.49763250350952
[LOAD] Retrain from saved_models/retrain_2fc7fa8471d9532f062f5915fe46ce69f74174e1_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S2 | Ftot=3332 | ΔF:+31.96 ΔR: 0.06 ΔT: 1.61 | MIA:0.4821 PredDiff:10.99%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [0][19/20]	Time 0.106 (0.175)	Data 0.072 (0.127)	Loss -50.3056 (-32.2221)	Forget_Acc@1 0.000 (3.040)
*** Minimize step ***
Epoch: [0][175/176]	Time 0.162 (0.176)	Data 0.115 (0.128)	Loss 0.4839 (1.9245)	Retain_Acc@1 91.500 (86.896)
Epoch: [0]	 train-acc:	86.89555555555556	 train-loss: 1.9244612225585513
one epoch duration:34.56045413017273
Epoch #1, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [1][19/20]	Time 0.102 (0.174)	Data 0.069 (0.126)	Loss -93.5999 (-59.4246)	Forget_Acc@1 0.000 (0.740)
*** Minimize step ***
Epoch: [1][175/176]	Time 0.149 (0.172)	Data 0.105 (0.126)	Loss 0.8981 (4.1224)	Retain_Acc@1 91.000 (83.211)
Epoch: [1]	 train-acc:	83.21111111111111	 train-loss: 4.122379638947381
one epoch duration:33.81029224395752
Epoch #2, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [2][19/20]	Time 0.105 (0.173)	Data 0.069 (0.126)	Loss -151.5840 (-95.3410)	Forget_Acc@1 0.000 (0.300)
*** Minimize step ***
Epoch: [2][175/176]	Time 0.145 (0.171)	Data 0.101 (0.125)	Loss 1.8999 (7.6258)	Retain_Acc@1 88.000 (77.691)
Epoch: [2]	 train-acc:	77.69111111111111	 train-loss: 7.625837748209635
one epoch duration:33.601418018341064
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [3][19/20]	Time 0.102 (0.166)	Data 0.067 (0.120)	Loss -107.6608 (-82.6641)	Forget_Acc@1 0.000 (0.020)
*** Minimize step ***
Epoch: [3][175/176]	Time 0.148 (0.174)	Data 0.104 (0.127)	Loss 2.1310 (3.5874)	Retain_Acc@1 88.000 (84.009)
Epoch: [3]	 train-acc:	84.00888888888889	 train-loss: 3.587382208082411
one epoch duration:34.00655484199524
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [4][19/20]	Time 0.105 (0.170)	Data 0.070 (0.123)	Loss -120.7527 (-109.2777)	Forget_Acc@1 0.735 (0.100)
*** Minimize step ***
Epoch: [4][175/176]	Time 0.157 (0.174)	Data 0.111 (0.126)	Loss 2.6624 (3.1705)	Retain_Acc@1 88.500 (84.507)
Epoch: [4]	 train-acc:	84.50666666666666	 train-loss: 3.1704854393853084
one epoch duration:33.97043824195862
Epoch #5, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [5][19/20]	Time 0.120 (0.182)	Data 0.079 (0.133)	Loss -131.0152 (-127.4339)	Forget_Acc@1 0.000 (0.120)
*** Minimize step ***
Epoch: [5][175/176]	Time 0.150 (0.176)	Data 0.106 (0.128)	Loss 2.6610 (3.0910)	Retain_Acc@1 85.000 (84.742)
Epoch: [5]	 train-acc:	84.74222222222222	 train-loss: 3.0910145726097955
one epoch duration:34.64302897453308
Epoch #6, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [6][175/176]	Time 0.143 (0.172)	Data 0.102 (0.125)	Loss 2.0675 (2.2507)	Retain_Acc@1 92.000 (86.027)
Epoch: [6]	 train-acc:	86.02666666666667	 train-loss: 2.2507010815090602
one epoch duration:30.242313861846924
Epoch #7, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [7][175/176]	Time 0.147 (0.170)	Data 0.103 (0.124)	Loss 1.8885 (1.9885)	Retain_Acc@1 86.500 (86.353)
Epoch: [7]	 train-acc:	86.35333333333334	 train-loss: 1.9884627685546874
one epoch duration:29.987441778182983
Epoch #8, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [8][175/176]	Time 0.147 (0.173)	Data 0.103 (0.126)	Loss 1.8689 (1.8064)	Retain_Acc@1 85.500 (86.882)
Epoch: [8]	 train-acc:	86.88222222222223	 train-loss: 1.8063987865024143
one epoch duration:30.407472133636475
Epoch #9, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [9][175/176]	Time 0.146 (0.174)	Data 0.102 (0.126)	Loss 2.0170 (1.6666)	Retain_Acc@1 85.000 (86.962)
Epoch: [9]	 train-acc:	86.96222222222222	 train-loss: 1.6666201999664307
one epoch duration:30.559903860092163
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S3 | Ftot=5000 | ΔF:+22.98 ΔR: 3.73 ΔT: 0.90 | MIA:0.4605 PredDiff:15.27%

===== Full Results =====
 method  stage  forget_total  Retrain_F  Retrain_R  Retrain_T  Unlearn_F  Unlearn_R  Unlearn_T         ΔF        ΔR    ΔT      MIA  PredDiff(%)
     FT      1          1666  88.355342  93.178715      84.01  99.939976  92.862167      83.80  11.584634  0.316547  0.21 0.456076        8.386
     FT      2          3332  62.785114  92.028799      81.82  94.507803  93.016628      83.65  31.722689  0.987829  1.83 0.460874       10.476
     FT      3          5000   0.000000  92.920000      76.20  61.000000  93.828889      81.50  61.000000  0.908889  5.30 0.453444       14.414
  FT_l1      1          1666  88.355342  93.178715      84.01  99.879952  92.725618      83.86  11.524610  0.453097  0.15 0.459852        8.410
  FT_l1      2          3332  62.785114  92.028799      81.82  94.987995  92.701637      83.61  32.202881  0.672838  1.79 0.750987       10.408
  FT_l1      3          5000   0.000000  92.920000      76.20  70.960000  93.353333      81.98  70.960000  0.433333  5.78 0.451633       15.114
     GA      1          1666  88.355342  93.178715      84.01  15.486194  78.135474      70.72 -72.869148 15.043241 13.29 0.733846       24.012
     GA      2          3332  62.785114  92.028799      81.82  16.236495  80.661267      70.99 -46.548619 11.367532 10.83 0.787725       22.688
     GA      3          5000   0.000000  92.920000      76.20  16.660000  82.804444      70.93  16.660000 10.115556  5.27 0.411111       22.480
     NG      1          1666  88.355342  93.178715      84.01  46.698679  85.132619      77.04 -41.656663  8.046096  6.97 0.462279       16.288
     NG      2          3332  62.785114  92.028799      81.82  24.789916  86.553956      75.84 -37.995198  5.474844  5.98 0.812234       16.828
     NG      3          5000   0.000000  92.920000      76.20   2.820000  87.757778      73.01   2.820000  5.162222  3.19 0.449778       16.298
     RL      1          1666  88.355342  93.178715      84.01  99.459784  92.686308      84.04  11.104442  0.492407  0.03 0.838975        8.502
     RL      2          3332  62.785114  92.028799      81.82  86.374550  92.795920      83.09  23.589436  0.767121  1.27 0.810548       10.344
     RL      3          5000   0.000000  92.920000      76.20  12.060000  93.762222      77.27  12.060000  0.842222  1.07 0.453378       13.312
Wfisher      1          1666  88.355342  93.178715      84.01   0.000000  14.246700      13.89 -88.355342 78.932015 70.12 0.673435       86.070
Wfisher      2          3332  62.785114  92.028799      81.82   0.000000  10.713980      10.00 -62.785114 81.314820 71.82 0.500000       89.216
Wfisher      3          5000   0.000000  92.920000      76.20 100.000000   0.000000      10.00 100.000000 92.920000 66.20 0.000000      100.000
  SCRUB      1          1666  88.355342  93.178715      84.01  99.939976  92.696652      83.86  11.584634  0.482062  0.15 0.454473        8.494
  SCRUB      2          3332  62.785114  92.028799      81.82  94.747899  91.964515      83.43  31.962785  0.064284  1.61 0.482081       10.994
  SCRUB      3          5000   0.000000  92.920000      76.20  22.980000  89.188889      77.10  22.980000  3.731111  0.90 0.460456       15.266

Results saved to saved_models/results_class_es_stage_easy_first.csv

--- [1/8] Experiment FINISHED ---
============================================================

--- [2/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: es
  - unlearning_granularity: stage
  - forget_partition_ordering: hard_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[LOAD] Original model from saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'es' method...
Partition sizes for forget: [1668, 1666, 1666]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.001
Epoch: [0][99/189]	Loss 0.3633 (0.2598)	Accuracy 86.328 (90.957)	Time 17.29
train_accuracy 90.704
one epoch duration:32.50040864944458
Epoch #1, Learning rate: 0.001
Epoch: [1][99/189]	Loss 0.2186 (0.2622)	Accuracy 92.578 (90.852)	Time 16.97
train_accuracy 90.863
one epoch duration:31.930434226989746
Epoch #2, Learning rate: 0.001
Epoch: [2][99/189]	Loss 0.4026 (0.2654)	Accuracy 85.938 (90.793)	Time 16.90
train_accuracy 90.931
one epoch duration:31.826383352279663
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/189]	Loss 0.3668 (0.2617)	Accuracy 89.453 (90.770)	Time 16.84
train_accuracy 90.938
one epoch duration:31.775614976882935
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/189]	Loss 0.2112 (0.2491)	Accuracy 92.188 (91.434)	Time 16.89
train_accuracy 91.358
one epoch duration:32.26455545425415
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/189]	Loss 0.2879 (0.2516)	Accuracy 91.406 (91.184)	Time 17.12
train_accuracy 91.085
one epoch duration:32.435521841049194
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/189]	Loss 0.2030 (0.2513)	Accuracy 94.141 (91.133)	Time 17.12
train_accuracy 91.101
one epoch duration:32.328941822052
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/189]	Loss 0.2525 (0.2499)	Accuracy 91.016 (91.344)	Time 17.28
train_accuracy 91.107
one epoch duration:32.484859466552734
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/189]	Loss 0.2961 (0.2477)	Accuracy 90.234 (91.430)	Time 17.33
train_accuracy 91.227
one epoch duration:32.519527196884155
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/189]	Loss 0.2300 (0.2570)	Accuracy 92.188 (91.098)	Time 17.16
train_accuracy 91.134
one epoch duration:32.90577793121338
[TRAIN] Retrain on 48332 samples
    Epoch 1/30  30.50s
    Epoch 2/30  30.75s
    Epoch 3/30  30.49s
    Epoch 4/30  31.28s
    Epoch 5/30  31.13s
    Epoch 6/30  30.01s
    Epoch 7/30  30.58s
    Epoch 8/30  29.88s
    Epoch 9/30  30.10s
    Epoch 10/30  29.80s
    Epoch 11/30  29.81s
    Epoch 12/30  30.09s
    Epoch 13/30  29.58s
    Epoch 14/30  31.34s
    Epoch 15/30  31.75s
    Epoch 16/30  32.81s
    Epoch 17/30  30.62s
    Epoch 18/30  29.85s
    Epoch 19/30  29.71s
    Epoch 20/30  30.18s
    Epoch 21/30  30.12s
    Epoch 22/30  30.08s
    Epoch 23/30  30.24s
    Epoch 24/30  31.08s
    Epoch 25/30  31.21s
    Epoch 26/30  30.06s
    Epoch 27/30  29.99s
    Epoch 28/30  29.54s
    Epoch 29/30  29.90s
    Epoch 30/30  29.28s
[SAVE] saved_models/retrain_3b7783e89077896f42f170365f982b2c528e3a07_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S1 | Ftot=1668 | ΔF:+16.67 ΔR: 1.52 ΔT: 0.92 | MIA:0.6053 PredDiff:9.52%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.001
Epoch: [0][99/183]	Loss 0.3127 (0.2566)	Accuracy 88.672 (91.031)	Time 18.06
train_accuracy 91.064
one epoch duration:33.05245876312256
Epoch #1, Learning rate: 0.001
Epoch: [1][99/183]	Loss 0.2449 (0.2484)	Accuracy 91.016 (91.211)	Time 17.83
train_accuracy 91.242
one epoch duration:32.870079040527344
Epoch #2, Learning rate: 0.001
Epoch: [2][99/183]	Loss 0.2310 (0.2477)	Accuracy 93.359 (91.258)	Time 17.58
train_accuracy 91.323
one epoch duration:31.868180513381958
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/183]	Loss 0.2643 (0.2518)	Accuracy 90.625 (91.266)	Time 17.19
train_accuracy 91.407
one epoch duration:31.334753274917603
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/183]	Loss 0.2661 (0.2405)	Accuracy 91.406 (91.480)	Time 17.12
train_accuracy 91.386
one epoch duration:30.966031789779663
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/183]	Loss 0.2776 (0.2442)	Accuracy 90.234 (91.543)	Time 16.95
train_accuracy 91.512
one epoch duration:30.792057514190674
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/183]	Loss 0.1978 (0.2394)	Accuracy 92.969 (91.645)	Time 17.00
train_accuracy 91.527
one epoch duration:31.129013538360596
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/183]	Loss 0.2984 (0.2389)	Accuracy 90.234 (91.602)	Time 17.35
train_accuracy 91.559
one epoch duration:31.44012761116028
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/183]	Loss 0.2693 (0.2407)	Accuracy 91.797 (91.664)	Time 16.99
train_accuracy 91.559
one epoch duration:31.069862127304077
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/183]	Loss 0.2190 (0.2428)	Accuracy 94.922 (91.445)	Time 16.92
train_accuracy 91.484
one epoch duration:31.077396631240845
[TRAIN] Retrain on 46666 samples
    Epoch 1/30  29.78s
    Epoch 2/30  30.08s
    Epoch 3/30  28.22s
    Epoch 4/30  28.35s
    Epoch 5/30  28.45s
    Epoch 6/30  30.35s
    Epoch 7/30  31.38s
    Epoch 8/30  29.54s
    Epoch 9/30  30.06s
    Epoch 10/30  29.22s
    Epoch 11/30  29.07s
    Epoch 12/30  29.80s
    Epoch 13/30  28.61s
    Epoch 14/30  29.66s
    Epoch 15/30  30.31s
    Epoch 16/30  28.87s
    Epoch 17/30  29.11s
    Epoch 18/30  28.78s
    Epoch 19/30  29.01s
    Epoch 20/30  29.31s
    Epoch 21/30  29.05s
    Epoch 22/30  28.81s
    Epoch 23/30  28.88s
    Epoch 24/30  29.00s
    Epoch 25/30  28.99s
    Epoch 26/30  29.19s
    Epoch 27/30  28.91s
    Epoch 28/30  29.24s
    Epoch 29/30  28.70s
    Epoch 30/30  28.38s
[SAVE] saved_models/retrain_8cbbe922167bbbd83c35bb584bb3817e9f05874b_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S2 | Ftot=3334 | ΔF:+24.33 ΔR: 0.87 ΔT: 0.48 | MIA:0.6241 PredDiff:10.16%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2853 (0.2441)	Accuracy 88.281 (91.477)	Time 17.37
train_accuracy 91.447
one epoch duration:30.512434244155884
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.1666 (0.2372)	Accuracy 94.141 (91.637)	Time 17.17
train_accuracy 91.380
one epoch duration:30.20588779449463
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2700 (0.2369)	Accuracy 90.625 (91.883)	Time 17.16
train_accuracy 91.791
one epoch duration:30.16045379638672
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2449 (0.2327)	Accuracy 89.844 (91.770)	Time 17.16
train_accuracy 91.687
one epoch duration:30.165027856826782
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.1981 (0.2336)	Accuracy 92.188 (91.590)	Time 17.13
train_accuracy 91.793
one epoch duration:30.120246410369873
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.3101 (0.2314)	Accuracy 89.062 (91.867)	Time 17.14
train_accuracy 91.882
one epoch duration:30.114869832992554
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.1555 (0.2274)	Accuracy 93.750 (92.098)	Time 17.13
train_accuracy 92.036
one epoch duration:30.123628854751587
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.2324 (0.2298)	Accuracy 93.359 (92.016)	Time 17.79
train_accuracy 91.956
one epoch duration:30.897796392440796
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2285 (0.2315)	Accuracy 92.578 (91.871)	Time 17.24
train_accuracy 91.796
one epoch duration:30.323862314224243
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.2543 (0.2237)	Accuracy 91.016 (92.238)	Time 17.24
train_accuracy 92.011
one epoch duration:30.308014631271362
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         FT | S3 | Ftot=5000 | ΔF:+58.10 ΔR: 0.89 ΔT: 4.78 | MIA:0.4597 PredDiff:14.19%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/189]	Loss 0.8967 (0.9361)	Accuracy 91.016 (90.453)	Time 17.86
train_accuracy 90.615
one epoch duration:33.43239212036133
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/189]	Loss 0.8403 (0.7917)	Accuracy 89.062 (90.969)	Time 17.25
train_accuracy 90.940
one epoch duration:32.56140375137329
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/189]	Loss 0.6739 (0.6603)	Accuracy 89.844 (91.062)	Time 17.61
train_accuracy 91.101
one epoch duration:33.16355013847351
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/189]	Loss 0.4791 (0.5223)	Accuracy 91.016 (90.699)	Time 17.49
train_accuracy 90.741
one epoch duration:32.96369409561157
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/189]	Loss 0.3554 (0.3819)	Accuracy 92.578 (91.156)	Time 17.35
train_accuracy 91.072
one epoch duration:32.75798201560974
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/189]	Loss 0.1551 (0.2552)	Accuracy 94.922 (91.082)	Time 17.92
train_accuracy 91.045
one epoch duration:33.11538219451904
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/189]	Loss 0.2891 (0.2570)	Accuracy 90.234 (90.910)	Time 17.20
train_accuracy 91.012
one epoch duration:32.69853854179382
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/189]	Loss 0.2485 (0.2549)	Accuracy 92.578 (91.137)	Time 17.53
train_accuracy 91.006
one epoch duration:33.05899453163147
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/189]	Loss 0.2620 (0.2578)	Accuracy 91.406 (91.121)	Time 17.49
train_accuracy 91.035
one epoch duration:32.98913264274597
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/189]	Loss 0.1999 (0.2541)	Accuracy 92.578 (91.059)	Time 17.36
train_accuracy 91.097
one epoch duration:32.807289123535156
[LOAD] Retrain from saved_models/retrain_3b7783e89077896f42f170365f982b2c528e3a07_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S1 | Ftot=1668 | ΔF:+18.65 ΔR: 1.34 ΔT: 0.79 | MIA:0.6412 PredDiff:9.52%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/183]	Loss 0.9045 (0.9208)	Accuracy 92.578 (91.168)	Time 17.45
train_accuracy 91.107
one epoch duration:31.748239278793335
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/183]	Loss 0.7615 (0.7868)	Accuracy 92.578 (91.176)	Time 17.32
train_accuracy 91.214
one epoch duration:31.616093158721924
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/183]	Loss 0.6762 (0.6510)	Accuracy 88.672 (91.070)	Time 17.36
train_accuracy 91.169
one epoch duration:31.664464473724365
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/183]	Loss 0.4268 (0.5158)	Accuracy 94.531 (91.270)	Time 17.30
train_accuracy 91.347
one epoch duration:31.540974855422974
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/183]	Loss 0.3956 (0.3808)	Accuracy 90.625 (91.168)	Time 17.34
train_accuracy 91.188
one epoch duration:31.64989185333252
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/183]	Loss 0.2317 (0.2522)	Accuracy 91.406 (91.328)	Time 17.61
train_accuracy 91.379
one epoch duration:31.903794765472412
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/183]	Loss 0.2408 (0.2469)	Accuracy 91.016 (91.277)	Time 17.33
train_accuracy 91.137
one epoch duration:31.68609070777893
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/183]	Loss 0.1900 (0.2430)	Accuracy 93.750 (91.520)	Time 17.67
train_accuracy 91.351
one epoch duration:32.063626289367676
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/183]	Loss 0.2774 (0.2482)	Accuracy 89.844 (91.473)	Time 17.29
train_accuracy 91.390
one epoch duration:31.48285174369812
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/183]	Loss 0.3114 (0.2463)	Accuracy 91.016 (91.543)	Time 17.10
train_accuracy 91.392
one epoch duration:31.19968581199646
[LOAD] Retrain from saved_models/retrain_8cbbe922167bbbd83c35bb584bb3817e9f05874b_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S2 | Ftot=3334 | ΔF:+24.69 ΔR: 0.60 ΔT: 0.33 | MIA:0.6621 PredDiff:10.22%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.8653 (0.9148)	Accuracy 92.188 (91.262)	Time 17.50
train_accuracy 91.271
one epoch duration:30.50421643257141
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.7209 (0.7798)	Accuracy 94.531 (91.070)	Time 17.16
train_accuracy 91.049
one epoch duration:30.10001826286316
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.5530 (0.6411)	Accuracy 95.703 (91.281)	Time 17.41
train_accuracy 91.233
one epoch duration:30.432772874832153
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.5812 (0.5117)	Accuracy 90.234 (91.426)	Time 17.05
train_accuracy 91.402
one epoch duration:30.020743131637573
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.4958 (0.3766)	Accuracy 86.719 (91.402)	Time 17.08
train_accuracy 91.522
one epoch duration:30.014400720596313
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2466 (0.2416)	Accuracy 90.625 (91.539)	Time 17.42
train_accuracy 91.558
one epoch duration:31.029204845428467
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2877 (0.2436)	Accuracy 90.234 (91.527)	Time 17.37
train_accuracy 91.511
one epoch duration:30.57118320465088
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.1808 (0.2442)	Accuracy 93.359 (91.516)	Time 17.80
train_accuracy 91.569
one epoch duration:31.04655170440674
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2662 (0.2405)	Accuracy 88.672 (91.609)	Time 17.42
train_accuracy 91.593
one epoch duration:30.617785930633545
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2567 (0.2357)	Accuracy 90.234 (91.812)	Time 17.40
train_accuracy 91.718
one epoch duration:30.613828420639038
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      FT_l1 | S3 | Ftot=5000 | ΔF:+67.34 ΔR: 0.43 ΔT: 5.48 | MIA:0.4497 PredDiff:14.82%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 18.046
one epoch duration:1.1672813892364502
Epoch #1, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 18.405
one epoch duration:1.1159651279449463
Epoch #2, Learning rate: 1e-05
len(train_loader):  7
train_accuracy 18.705
one epoch duration:1.0998876094818115
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 18.765
one epoch duration:1.1153314113616943
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 18.645
one epoch duration:1.1180336475372314
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 18.046
one epoch duration:1.1247575283050537
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 17.806
one epoch duration:1.1458172798156738
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 19.065
one epoch duration:1.1223022937774658
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 18.165
one epoch duration:1.1236481666564941
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  7
train_accuracy 18.046
one epoch duration:1.123948335647583
[LOAD] Retrain from saved_models/retrain_3b7783e89077896f42f170365f982b2c528e3a07_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S1 | Ftot=1668 | ΔF:-40.47 ΔR:13.20 ΔT:11.63 | MIA:0.5229 PredDiff:23.08%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 17.247
one epoch duration:2.2852344512939453
Epoch #1, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 16.947
one epoch duration:2.2575149536132812
Epoch #2, Learning rate: 1e-05
len(train_loader):  14
train_accuracy 16.557
one epoch duration:2.2562406063079834
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 17.067
one epoch duration:2.269244432449341
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 16.347
one epoch duration:2.2692759037017822
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 17.277
one epoch duration:2.257434606552124
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 16.107
one epoch duration:2.2546844482421875
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 16.617
one epoch duration:2.2584116458892822
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 16.407
one epoch duration:2.2605628967285156
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  14
train_accuracy 16.017
one epoch duration:2.269059419631958
[LOAD] Retrain from saved_models/retrain_8cbbe922167bbbd83c35bb584bb3817e9f05874b_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S2 | Ftot=3334 | ΔF:-36.17 ΔR:11.57 ΔT:11.22 | MIA:0.4892 PredDiff:21.99%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.080
one epoch duration:3.3489668369293213
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.560
one epoch duration:3.422380208969116
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.040
one epoch duration:3.3810040950775146
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.060
one epoch duration:3.3601491451263428
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.960
one epoch duration:3.3129262924194336
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.420
one epoch duration:3.3654515743255615
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.980
one epoch duration:3.3019731044769287
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.920
one epoch duration:3.2983038425445557
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.100
one epoch duration:3.3166589736938477
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.640
one epoch duration:3.2907698154449463
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         GA | S3 | Ftot=5000 | ΔF:+16.62 ΔR:10.04 ΔT: 5.16 | MIA:0.4139 PredDiff:22.35%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [0][99/189]	Loss -0.3958 (-0.3645)	Accuracy 90.625 (90.906)	Time 21.62
train_accuracy 90.745
one epoch duration:39.985079526901245
Epoch #1, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [1][99/189]	Loss -0.4618 (-0.4557)	Accuracy 89.062 (90.867)	Time 21.63
train_accuracy 90.650
one epoch duration:40.043755292892456
Epoch #2, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [2][99/189]	Loss -0.5272 (-0.5339)	Accuracy 92.578 (90.746)	Time 21.49
train_accuracy 90.807
one epoch duration:39.706066370010376
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [3][99/189]	Loss -0.5709 (-0.6122)	Accuracy 85.938 (90.520)	Time 21.23
train_accuracy 90.611
one epoch duration:39.396928787231445
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
Epoch: [4][99/189]	Loss -0.7415 (-0.6903)	Accuracy 91.406 (90.594)	Time 21.26
train_accuracy 90.431
one epoch duration:39.40888261795044
[LOAD] Retrain from saved_models/retrain_3b7783e89077896f42f170365f982b2c528e3a07_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S1 | Ftot=1668 | ΔF:-27.94 ΔR: 7.07 ΔT: 6.43 | MIA:0.5694 PredDiff:16.96%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [0][99/183]	Loss -0.8015 (-0.7666)	Accuracy 91.797 (90.844)	Time 22.10
train_accuracy 90.588
one epoch duration:39.30717897415161
Epoch #1, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [1][99/183]	Loss -0.8106 (-0.8592)	Accuracy 89.062 (90.621)	Time 22.51
train_accuracy 90.685
one epoch duration:39.7024085521698
Epoch #2, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [2][99/183]	Loss -0.9548 (-0.9335)	Accuracy 88.672 (90.023)	Time 22.50
train_accuracy 90.183
one epoch duration:39.721370220184326
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [3][99/183]	Loss -1.0744 (-1.0273)	Accuracy 91.016 (90.105)	Time 22.18
train_accuracy 90.340
one epoch duration:39.08441162109375
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
Epoch: [4][99/183]	Loss -1.0848 (-1.1130)	Accuracy 88.672 (90.184)	Time 22.06
train_accuracy 90.209
one epoch duration:38.9677619934082
[LOAD] Retrain from saved_models/retrain_8cbbe922167bbbd83c35bb584bb3817e9f05874b_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S2 | Ftot=3334 | ΔF:-35.78 ΔR: 6.90 ΔT: 7.73 | MIA:0.6804 PredDiff:17.36%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -1.2424 (-1.2046)	Accuracy 91.016 (90.758)	Time 22.97
train_accuracy 90.584
one epoch duration:38.59419512748718
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -1.2061 (-1.3050)	Accuracy 86.328 (90.594)	Time 23.22
train_accuracy 90.520
one epoch duration:39.07593321800232
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -1.4602 (-1.4145)	Accuracy 92.578 (90.629)	Time 23.33
train_accuracy 90.718
one epoch duration:39.19789910316467
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -1.3985 (-1.5169)	Accuracy 87.891 (90.566)	Time 23.20
train_accuracy 90.491
one epoch duration:39.015376567840576
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -1.5768 (-1.6305)	Accuracy 85.547 (90.422)	Time 23.34
train_accuracy 90.420
one epoch duration:39.055217266082764
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         NG | S3 | Ftot=5000 | ΔF:+2.54 ΔR: 5.16 ΔT: 3.34 | MIA:0.4506 PredDiff:16.12%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.4819 (0.4542)	Accuracy 83.984 (88.281)	Time 15.77
Epoch: [0][199/196]	Loss 0.3708 (0.4318)	Accuracy 89.062 (87.998)	Time 16.95
one epoch duration:33.2411675453186
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.4143 (0.3938)	Accuracy 86.719 (88.273)	Time 15.77
Epoch: [1][199/196]	Loss 0.3914 (0.3898)	Accuracy 87.891 (88.115)	Time 16.94
one epoch duration:33.243858337402344
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.3886 (0.3841)	Accuracy 89.453 (88.260)	Time 15.78
Epoch: [2][199/196]	Loss 0.3428 (0.3824)	Accuracy 90.234 (88.312)	Time 16.94
one epoch duration:33.252732276916504
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.3621 (0.3699)	Accuracy 87.500 (88.428)	Time 15.73
Epoch: [3][199/196]	Loss 0.3307 (0.3726)	Accuracy 88.281 (88.451)	Time 16.91
one epoch duration:33.1591591835022
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.4179 (0.3729)	Accuracy 87.500 (88.399)	Time 15.77
Epoch: [4][199/196]	Loss 0.3371 (0.3736)	Accuracy 88.672 (88.340)	Time 16.93
one epoch duration:33.23492121696472
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.3324 (0.3724)	Accuracy 89.453 (88.374)	Time 15.79
Epoch: [5][199/196]	Loss 0.3481 (0.3730)	Accuracy 90.234 (88.536)	Time 17.22
one epoch duration:33.546677589416504
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.3085 (0.3719)	Accuracy 91.016 (88.458)	Time 15.95
Epoch: [6][199/196]	Loss 0.4152 (0.3754)	Accuracy 84.766 (88.389)	Time 17.22
one epoch duration:33.71137070655823
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.3504 (0.3712)	Accuracy 89.453 (88.298)	Time 16.01
Epoch: [7][199/196]	Loss 0.3042 (0.3730)	Accuracy 89.453 (88.295)	Time 17.20
one epoch duration:33.75075626373291
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.3383 (0.3655)	Accuracy 89.453 (88.445)	Time 15.99
Epoch: [8][199/196]	Loss 0.3601 (0.3688)	Accuracy 88.672 (88.407)	Time 17.28
one epoch duration:33.80248522758484
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.3534 (0.3692)	Accuracy 89.844 (88.424)	Time 15.95
Epoch: [9][199/196]	Loss 0.4030 (0.3717)	Accuracy 87.109 (88.312)	Time 17.17
one epoch duration:33.65893363952637
[LOAD] Retrain from saved_models/retrain_3b7783e89077896f42f170365f982b2c528e3a07_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S1 | Ftot=1668 | ΔF:+6.71 ΔR: 1.56 ΔT: 0.40 | MIA:0.8689 PredDiff:9.62%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.001
Epoch: [0][99/197]	Loss 0.4876 (0.4626)	Accuracy 85.156 (85.751)	Time 14.86
Epoch: [0][199/197]	Loss 0.4978 (0.4594)	Accuracy 84.375 (85.763)	Time 17.22
one epoch duration:45.275625467300415
Epoch #1, Learning rate: 0.001
Epoch: [1][99/197]	Loss 0.3873 (0.4534)	Accuracy 86.328 (85.719)	Time 14.55
Epoch: [1][199/197]	Loss 0.4342 (0.4534)	Accuracy 85.547 (85.660)	Time 16.89
one epoch duration:41.659467935562134
Epoch #2, Learning rate: 0.001
Epoch: [2][99/197]	Loss 0.4236 (0.4438)	Accuracy 84.375 (85.715)	Time 14.55
Epoch: [2][199/197]	Loss 0.4086 (0.4474)	Accuracy 88.281 (85.667)	Time 17.11
one epoch duration:43.6843478679657
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/197]	Loss 0.4015 (0.4410)	Accuracy 86.719 (86.069)	Time 14.75
Epoch: [3][199/197]	Loss 0.4195 (0.4393)	Accuracy 86.328 (86.118)	Time 17.23
one epoch duration:33.724292039871216
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/197]	Loss 0.3866 (0.4377)	Accuracy 85.156 (86.024)	Time 14.71
Epoch: [4][199/197]	Loss 0.3893 (0.4396)	Accuracy 87.109 (85.942)	Time 16.94
one epoch duration:33.370468854904175
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/197]	Loss 0.4099 (0.4437)	Accuracy 87.109 (85.828)	Time 14.53
Epoch: [5][199/197]	Loss 0.4284 (0.4406)	Accuracy 87.500 (85.891)	Time 17.01
one epoch duration:33.28653359413147
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/197]	Loss 0.4375 (0.4457)	Accuracy 85.156 (85.856)	Time 14.83
Epoch: [6][199/197]	Loss 0.4634 (0.4399)	Accuracy 84.766 (85.988)	Time 17.20
one epoch duration:33.77020978927612
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/197]	Loss 0.5137 (0.4478)	Accuracy 82.031 (85.638)	Time 14.85
Epoch: [7][199/197]	Loss 0.4092 (0.4417)	Accuracy 86.719 (85.916)	Time 17.24
one epoch duration:33.84500074386597
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/197]	Loss 0.5047 (0.4467)	Accuracy 82.031 (85.815)	Time 14.60
Epoch: [8][199/197]	Loss 0.4517 (0.4441)	Accuracy 87.109 (85.904)	Time 17.10
one epoch duration:33.40797734260559
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/197]	Loss 0.4397 (0.4336)	Accuracy 85.938 (86.301)	Time 14.53
Epoch: [9][199/197]	Loss 0.5775 (0.4364)	Accuracy 83.203 (86.227)	Time 17.13
one epoch duration:33.40932774543762
[LOAD] Retrain from saved_models/retrain_8cbbe922167bbbd83c35bb584bb3817e9f05874b_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S2 | Ftot=3334 | ΔF:+16.29 ΔR: 0.88 ΔT: 0.12 | MIA:0.8091 PredDiff:10.40%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.4369 (0.4884)	Accuracy 85.938 (83.506)	Time 13.81
Epoch: [0][199/196]	Loss 0.4060 (0.4879)	Accuracy 85.938 (83.255)	Time 17.48
one epoch duration:44.628955602645874
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.4026 (0.4764)	Accuracy 86.719 (83.389)	Time 13.88
Epoch: [1][199/196]	Loss 0.4626 (0.4784)	Accuracy 83.203 (83.325)	Time 17.18
one epoch duration:40.73308229446411
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.4701 (0.4761)	Accuracy 82.812 (83.481)	Time 13.82
Epoch: [2][199/196]	Loss 0.4703 (0.4697)	Accuracy 84.375 (83.570)	Time 17.25
one epoch duration:39.93284368515015
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.5059 (0.4737)	Accuracy 81.641 (83.286)	Time 13.76
Epoch: [3][199/196]	Loss 0.4899 (0.4697)	Accuracy 83.203 (83.457)	Time 17.21
one epoch duration:33.749614000320435
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.4960 (0.4674)	Accuracy 82.422 (83.569)	Time 13.77
Epoch: [4][199/196]	Loss 0.4599 (0.4640)	Accuracy 83.984 (83.650)	Time 17.25
one epoch duration:33.79779624938965
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.4207 (0.4611)	Accuracy 87.109 (83.721)	Time 13.80
Epoch: [5][199/196]	Loss 0.4411 (0.4639)	Accuracy 83.984 (83.655)	Time 17.21
one epoch duration:33.787487745285034
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.3990 (0.4620)	Accuracy 85.938 (83.687)	Time 13.84
Epoch: [6][199/196]	Loss 0.4750 (0.4654)	Accuracy 81.641 (83.594)	Time 17.25
one epoch duration:33.86780905723572
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.4650 (0.4690)	Accuracy 84.766 (83.633)	Time 13.80
Epoch: [7][199/196]	Loss 0.5187 (0.4659)	Accuracy 83.984 (83.602)	Time 17.33
one epoch duration:33.91411519050598
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.5484 (0.4698)	Accuracy 80.859 (83.486)	Time 13.82
Epoch: [8][199/196]	Loss 0.4866 (0.4668)	Accuracy 81.641 (83.592)	Time 17.30
one epoch duration:34.11547017097473
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.4163 (0.4594)	Accuracy 84.375 (83.989)	Time 13.88
Epoch: [9][199/196]	Loss 0.4715 (0.4642)	Accuracy 83.984 (83.754)	Time 17.05
one epoch duration:33.72878289222717
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)         RL | S3 | Ftot=5000 | ΔF:+3.72 ΔR: 0.80 ΔT: 0.02 | MIA:0.4573 PredDiff:12.78%

===== Running Method: Wfisher =====
  > Applied specific params for Wfisher: {'alpha': 10.0}

[UNLEARN] Stage 1: |Forget Total|=1668
[LOAD] Retrain from saved_models/retrain_3b7783e89077896f42f170365f982b2c528e3a07_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S1 | Ftot=1668 | ΔF:-62.11 ΔR:81.44 ΔT:72.86 | MIA:0.5000 PredDiff:89.56%

[UNLEARN] Stage 2: |Forget Total|=3334
[LOAD] Retrain from saved_models/retrain_8cbbe922167bbbd83c35bb584bb3817e9f05874b_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S2 | Ftot=3334 | ΔF:+46.10 ΔR:89.17 ΔT:72.78 | MIA:0.0000 PredDiff:93.03%

[UNLEARN] Stage 3: |Forget Total|=5000
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)    Wfisher | S3 | Ftot=5000 | ΔF:+100.00 ΔR:92.92 ΔT:66.20 | MIA:0.0000 PredDiff:100.00%

===== Running Method: SCRUB =====
  > Applied specific params for SCRUB: {'unlearn_epochs': 10, 'kd_T': 4.0, 'gamma': 1.0, 'beta': 1.0, 'msteps': 5, 'decreasing_lr': '5,8'}

[UNLEARN] Stage 1: |Forget Total|=1668
Epoch #0, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [0][6/7]	Time 0.099 (0.163)	Data 0.067 (0.118)	Loss -10.8827 (-9.2398)	Forget_Acc@1 11.364 (16.487)
*** Minimize step ***
Epoch: [0][188/189]	Time 0.149 (0.172)	Data 0.105 (0.126)	Loss 0.2455 (0.3789)	Retain_Acc@1 93.137 (90.571)
Epoch: [0]	 train-acc:	90.5714640315475	 train-loss: 0.3789073444244967
one epoch duration:33.74628257751465
Epoch #1, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [1][6/7]	Time 0.099 (0.160)	Data 0.066 (0.118)	Loss -11.1337 (-10.9434)	Forget_Acc@1 9.848 (13.309)
*** Minimize step ***
Epoch: [1][188/189]	Time 0.149 (0.173)	Data 0.106 (0.126)	Loss 0.2292 (0.4130)	Retain_Acc@1 92.647 (90.503)
Epoch: [1]	 train-acc:	90.50318628532311	 train-loss: 0.41296219609435464
one epoch duration:33.73388195037842
Epoch #2, Learning rate: 0.001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [2][6/7]	Time 0.100 (0.162)	Data 0.068 (0.118)	Loss -12.4738 (-11.7489)	Forget_Acc@1 7.576 (12.590)
*** Minimize step ***
Epoch: [2][188/189]	Time 0.149 (0.172)	Data 0.105 (0.126)	Loss 0.3737 (0.4258)	Retain_Acc@1 89.216 (90.520)
Epoch: [2]	 train-acc:	90.519738461653	 train-loss: 0.4257874858677313
one epoch duration:33.72799801826477
Epoch #3, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [3][6/7]	Time 0.103 (0.162)	Data 0.067 (0.117)	Loss -12.2361 (-11.8167)	Forget_Acc@1 14.394 (12.470)
*** Minimize step ***
Epoch: [3][188/189]	Time 0.148 (0.172)	Data 0.106 (0.126)	Loss 0.2917 (0.3276)	Retain_Acc@1 93.137 (90.983)
Epoch: [3]	 train-acc:	90.98319952769913	 train-loss: 0.3275880500036666
one epoch duration:33.7041757106781
Epoch #4, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [4][6/7]	Time 0.103 (0.162)	Data 0.067 (0.118)	Loss -13.1303 (-12.2200)	Forget_Acc@1 15.152 (12.890)
*** Minimize step ***
Epoch: [4][188/189]	Time 0.149 (0.172)	Data 0.105 (0.126)	Loss 0.3516 (0.3377)	Retain_Acc@1 89.216 (90.925)
Epoch: [4]	 train-acc:	90.92526689002345	 train-loss: 0.33765370804555345
one epoch duration:33.719297885894775
Epoch #5, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Maximize step ***
Epoch: [5][6/7]	Time 0.102 (0.165)	Data 0.066 (0.118)	Loss -13.7213 (-12.5552)	Forget_Acc@1 11.364 (12.890)
*** Minimize step ***
Epoch: [5][188/189]	Time 0.152 (0.173)	Data 0.107 (0.126)	Loss 0.2607 (0.3421)	Retain_Acc@1 91.667 (90.975)
Epoch: [5]	 train-acc:	90.97492343542997	 train-loss: 0.3421477559686625
one epoch duration:33.76999568939209
Epoch #6, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [6][188/189]	Time 0.152 (0.173)	Data 0.106 (0.126)	Loss 0.4488 (0.3293)	Retain_Acc@1 89.706 (91.035)
Epoch: [6]	 train-acc:	91.03492508812238	 train-loss: 0.3293488076992464
one epoch duration:32.70053315162659
Epoch #7, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [7][188/189]	Time 0.151 (0.173)	Data 0.109 (0.126)	Loss 0.2540 (0.3191)	Retain_Acc@1 94.608 (91.037)
Epoch: [7]	 train-acc:	91.03699411703026	 train-loss: 0.3191436808486949
one epoch duration:32.65350675582886
Epoch #8, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [8][188/189]	Time 0.149 (0.172)	Data 0.105 (0.126)	Loss 0.2714 (0.3147)	Retain_Acc@1 91.667 (91.056)
Epoch: [8]	 train-acc:	91.0556153165853	 train-loss: 0.3147456374259557
one epoch duration:32.55018591880798
Epoch #9, Learning rate: 0.0001
len(r_loader): 189, len(f_loader): 7
*** Minimize step ***
Epoch: [9][188/189]	Time 0.154 (0.173)	Data 0.109 (0.126)	Loss 0.4239 (0.3236)	Retain_Acc@1 88.725 (90.857)
Epoch: [9]	 train-acc:	90.85698914379905	 train-loss: 0.3235833309970036
one epoch duration:32.63352632522583
[LOAD] Retrain from saved_models/retrain_3b7783e89077896f42f170365f982b2c528e3a07_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S1 | Ftot=1668 | ΔF:+21.82 ΔR: 1.30 ΔT: 0.83 | MIA:0.6117 PredDiff:9.56%

[UNLEARN] Stage 2: |Forget Total|=3334
Epoch #0, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [0][13/14]	Time 0.049 (0.162)	Data 0.016 (0.118)	Loss -24.4694 (-14.6334)	Forget_Acc@1 0.000 (8.548)
*** Minimize step ***
Epoch: [0][182/183]	Time 0.077 (0.172)	Data 0.043 (0.125)	Loss 0.5448 (0.8015)	Retain_Acc@1 89.189 (89.241)
Epoch: [0]	 train-acc:	89.24056058663038	 train-loss: 0.8014506065793888
one epoch duration:33.78795647621155
Epoch #1, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [1][13/14]	Time 0.045 (0.163)	Data 0.012 (0.118)	Loss -31.0128 (-19.4838)	Forget_Acc@1 0.000 (6.839)
*** Minimize step ***
Epoch: [1][182/183]	Time 0.073 (0.171)	Data 0.043 (0.125)	Loss 0.5794 (1.1001)	Retain_Acc@1 89.189 (89.431)
Epoch: [1]	 train-acc:	89.43127759687339	 train-loss: 1.1000897849269775
one epoch duration:33.603302240371704
Epoch #2, Learning rate: 0.001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [2][13/14]	Time 0.042 (0.162)	Data 0.011 (0.118)	Loss -28.0624 (-21.9760)	Forget_Acc@1 0.000 (6.239)
*** Minimize step ***
Epoch: [2][182/183]	Time 0.077 (0.170)	Data 0.047 (0.125)	Loss 0.4921 (1.1257)	Retain_Acc@1 90.541 (89.001)
Epoch: [2]	 train-acc:	89.00055715408622	 train-loss: 1.1257457962167128
one epoch duration:33.36333727836609
Epoch #3, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [3][13/14]	Time 0.043 (0.162)	Data 0.013 (0.116)	Loss -15.9076 (-21.0088)	Forget_Acc@1 0.000 (6.629)
*** Minimize step ***
Epoch: [3][182/183]	Time 0.074 (0.169)	Data 0.042 (0.123)	Loss 0.5496 (0.5795)	Retain_Acc@1 89.189 (90.141)
Epoch: [3]	 train-acc:	90.14057344395691	 train-loss: 0.5794924661042641
one epoch duration:33.19450235366821
Epoch #4, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [4][13/14]	Time 0.046 (0.161)	Data 0.014 (0.116)	Loss -21.3021 (-24.2719)	Forget_Acc@1 0.000 (6.119)
*** Minimize step ***
Epoch: [4][182/183]	Time 0.078 (0.169)	Data 0.043 (0.123)	Loss 0.4627 (0.5990)	Retain_Acc@1 90.541 (90.359)
Epoch: [4]	 train-acc:	90.35914799109818	 train-loss: 0.5990305536482923
one epoch duration:33.26000905036926
Epoch #5, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Maximize step ***
Epoch: [5][13/14]	Time 0.047 (0.165)	Data 0.016 (0.118)	Loss -25.6413 (-25.8493)	Forget_Acc@1 0.000 (5.939)
*** Minimize step ***
Epoch: [5][182/183]	Time 0.078 (0.171)	Data 0.043 (0.125)	Loss 0.5485 (0.6211)	Retain_Acc@1 90.541 (90.473)
Epoch: [5]	 train-acc:	90.47272104214177	 train-loss: 0.6211313277052549
one epoch duration:33.60372304916382
Epoch #6, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [6][182/183]	Time 0.074 (0.172)	Data 0.043 (0.125)	Loss 0.7067 (0.5009)	Retain_Acc@1 90.541 (90.546)
Epoch: [6]	 train-acc:	90.54557922583011	 train-loss: 0.5009121124213994
one epoch duration:31.50016498565674
Epoch #7, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [7][182/183]	Time 0.079 (0.171)	Data 0.043 (0.125)	Loss 0.4881 (0.4573)	Retain_Acc@1 94.595 (90.758)
Epoch: [7]	 train-acc:	90.75772511395549	 train-loss: 0.4573172894038855
one epoch duration:31.35571002960205
Epoch #8, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [8][182/183]	Time 0.075 (0.171)	Data 0.042 (0.125)	Loss 0.6166 (0.4383)	Retain_Acc@1 89.189 (90.908)
Epoch: [8]	 train-acc:	90.90772726044001	 train-loss: 0.43831333213401596
one epoch duration:31.29558753967285
Epoch #9, Learning rate: 0.0001
len(r_loader): 183, len(f_loader): 14
*** Minimize step ***
Epoch: [9][182/183]	Time 0.074 (0.169)	Data 0.042 (0.123)	Loss 0.5338 (0.4179)	Retain_Acc@1 89.189 (90.846)
Epoch: [9]	 train-acc:	90.84558351552937	 train-loss: 0.41787237941041144
one epoch duration:30.98354959487915
[LOAD] Retrain from saved_models/retrain_8cbbe922167bbbd83c35bb584bb3817e9f05874b_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S2 | Ftot=3334 | ΔF:+27.71 ΔR: 0.06 ΔT: 0.81 | MIA:0.6502 PredDiff:10.62%

[UNLEARN] Stage 3: |Forget Total|=5000
Epoch #0, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [0][19/20]	Time 0.112 (0.169)	Data 0.075 (0.122)	Loss -59.5254 (-37.1930)	Forget_Acc@1 0.000 (3.460)
*** Minimize step ***
Epoch: [0][175/176]	Time 0.146 (0.170)	Data 0.102 (0.124)	Loss 0.6346 (2.1276)	Retain_Acc@1 89.000 (86.620)
Epoch: [0]	 train-acc:	86.62	 train-loss: 2.1275802703645494
one epoch duration:33.37566518783569
Epoch #1, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [1][19/20]	Time 0.102 (0.167)	Data 0.068 (0.121)	Loss -111.4357 (-66.6531)	Forget_Acc@1 0.000 (0.700)
*** Minimize step ***
Epoch: [1][175/176]	Time 0.149 (0.173)	Data 0.103 (0.126)	Loss 1.0542 (5.5215)	Retain_Acc@1 90.000 (80.489)
Epoch: [1]	 train-acc:	80.4888888888889	 train-loss: 5.52145750196245
one epoch duration:33.767218828201294
Epoch #2, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [2][19/20]	Time 0.104 (0.169)	Data 0.068 (0.123)	Loss -164.0000 (-98.2891)	Forget_Acc@1 0.000 (0.560)
*** Minimize step ***
Epoch: [2][175/176]	Time 0.146 (0.173)	Data 0.105 (0.126)	Loss 2.7842 (7.6943)	Retain_Acc@1 85.000 (74.633)
Epoch: [2]	 train-acc:	74.63333333333334	 train-loss: 7.694343333731757
one epoch duration:33.807682037353516
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [3][19/20]	Time 0.105 (0.168)	Data 0.070 (0.123)	Loss -132.8070 (-96.8007)	Forget_Acc@1 0.735 (0.260)
*** Minimize step ***
Epoch: [3][175/176]	Time 0.145 (0.172)	Data 0.103 (0.126)	Loss 3.9121 (6.2443)	Retain_Acc@1 81.500 (80.149)
Epoch: [3]	 train-acc:	80.14888888888889	 train-loss: 6.244310356606378
one epoch duration:33.63731646537781
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [4][19/20]	Time 0.102 (0.169)	Data 0.070 (0.123)	Loss -133.2530 (-120.7948)	Forget_Acc@1 0.735 (0.780)
*** Minimize step ***
Epoch: [4][175/176]	Time 0.148 (0.173)	Data 0.103 (0.126)	Loss 3.2275 (4.5050)	Retain_Acc@1 86.500 (82.156)
Epoch: [4]	 train-acc:	82.15555555555555	 train-loss: 4.505026060867309
one epoch duration:33.82083225250244
Epoch #5, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [5][19/20]	Time 0.104 (0.168)	Data 0.068 (0.123)	Loss -153.9283 (-140.7095)	Forget_Acc@1 0.000 (0.360)
*** Minimize step ***
Epoch: [5][175/176]	Time 0.146 (0.172)	Data 0.103 (0.126)	Loss 3.1166 (4.2025)	Retain_Acc@1 86.000 (82.860)
Epoch: [5]	 train-acc:	82.86	 train-loss: 4.202484407128228
one epoch duration:33.63162660598755
Epoch #6, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [6][175/176]	Time 0.148 (0.172)	Data 0.104 (0.126)	Loss 2.5312 (2.8638)	Retain_Acc@1 87.000 (84.267)
Epoch: [6]	 train-acc:	84.26666666666667	 train-loss: 2.8637692579481335
one epoch duration:30.360377550125122
Epoch #7, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [7][175/176]	Time 0.147 (0.173)	Data 0.105 (0.126)	Loss 2.1219 (2.5303)	Retain_Acc@1 87.000 (84.938)
Epoch: [7]	 train-acc:	84.93777777777778	 train-loss: 2.5303184591505263
one epoch duration:30.44275689125061
Epoch #8, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [8][175/176]	Time 0.150 (0.173)	Data 0.103 (0.126)	Loss 1.9593 (2.2888)	Retain_Acc@1 86.500 (85.264)
Epoch: [8]	 train-acc:	85.26444444444445	 train-loss: 2.288786379199558
one epoch duration:30.50377368927002
Epoch #9, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [9][175/176]	Time 0.147 (0.173)	Data 0.103 (0.126)	Loss 2.1566 (2.1039)	Retain_Acc@1 84.000 (85.793)
Epoch: [9]	 train-acc:	85.79333333333334	 train-loss: 2.1038825940873886
one epoch duration:30.380233764648438
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Eval)      SCRUB | S3 | Ftot=5000 | ΔF:+13.48 ΔR: 4.90 ΔT: 0.73 | MIA:0.4585 PredDiff:15.66%

===== Full Results =====
 method  stage  forget_total  Retrain_F  Retrain_R  Retrain_T  Unlearn_F  Unlearn_R  Unlearn_T         ΔF        ΔR    ΔT      MIA  PredDiff(%)
     FT      1          1668  62.110312  91.788049      82.86  78.776978  93.310850      83.78  16.666667  1.522801  0.92 0.605314        9.520
     FT      2          3334  53.899220  92.735611      82.78  78.224355  93.603480      83.26  24.325135  0.867870  0.48 0.624103       10.164
     FT      3          5000   0.000000  92.920000      76.20  58.100000  93.811111      80.98  58.100000  0.891111  4.78 0.459711       14.190
  FT_l1      1          1668  62.110312  91.788049      82.86  80.755396  93.130845      83.65  18.645084  1.342796  0.79 0.641227        9.518
  FT_l1      2          3334  53.899220  92.735611      82.78  78.584283  93.339905      83.11  24.685063  0.604294  0.33 0.662064       10.216
  FT_l1      3          5000   0.000000  92.920000      76.20  67.340000  93.353333      81.68  67.340000  0.433333  5.48 0.449689       14.822
     GA      1          1668  62.110312  91.788049      82.86  21.642686  78.587685      71.23 -40.467626 13.200364 11.63 0.522895       23.078
     GA      2          3334  53.899220  92.735611      82.78  17.726455  81.168302      71.56 -36.172765 11.567308 11.22 0.489224       21.986
     GA      3          5000   0.000000  92.920000      76.20  16.620000  82.875556      71.04  16.620000 10.044444  5.16 0.413867       22.354
     NG      1          1668  62.110312  91.788049      82.86  34.172662  84.714061      76.43 -27.937650  7.073988  6.43 0.569408       16.958
     NG      2          3334  53.899220  92.735611      82.78  18.116377  85.831226      75.05 -35.782843  6.904384  7.73 0.680446       17.358
     NG      3          5000   0.000000  92.920000      76.20   2.540000  87.762222      72.86   2.540000  5.157778  3.34 0.450622       16.118
     RL      1          1668  62.110312  91.788049      82.86  68.824940  93.343954      83.26   6.714628  1.555905  0.40 0.868863        9.624
     RL      2          3334  53.899220  92.735611      82.78  70.185963  93.614194      82.90  16.286743  0.878584  0.12 0.809121       10.396
     RL      3          5000   0.000000  92.920000      76.20   3.720000  93.724444      76.22   3.720000  0.804444  0.02 0.457289       12.784
Wfisher      1          1668  62.110312  91.788049      82.86   0.000000  10.345113      10.00 -62.110312 81.442936 72.86 0.500000       89.564
Wfisher      2          3334  53.899220  92.735611      82.78 100.000000   3.570051      10.00  46.100780 89.165560 72.78 0.000000       93.034
Wfisher      3          5000   0.000000  92.920000      76.20 100.000000   0.000000      10.00 100.000000 92.920000 66.20 0.000000      100.000
  SCRUB      1          1668  62.110312  91.788049      82.86  83.932854  93.087396      83.69  21.822542  1.299346  0.83 0.611669        9.558
  SCRUB      2          3334  53.899220  92.735611      82.78  81.613677  92.799897      83.59  27.714457  0.064287  0.81 0.650228       10.620
  SCRUB      3          5000   0.000000  92.920000      76.20  13.480000  88.024444      75.47  13.480000  4.895556  0.73 0.458489       15.656

Results saved to saved_models/results_class_es_stage_hard_first.csv

--- [2/8] Experiment FINISHED ---
============================================================

--- [3/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: es
  - unlearning_granularity: batch
  - forget_partition_ordering: easy_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[LOAD] Original model from saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'es' method...
Partition sizes for forget: [1666, 1666, 1668]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for FT
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2698 (0.2817)	Accuracy 91.016 (90.184)	Time 17.17
train_accuracy 90.364
one epoch duration:30.125056505203247
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.2619 (0.2558)	Accuracy 89.844 (91.133)	Time 17.09
train_accuracy 90.918
one epoch duration:30.031760692596436
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2393 (0.2550)	Accuracy 90.234 (90.824)	Time 16.97
train_accuracy 90.784
one epoch duration:29.783956289291382
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2585 (0.2503)	Accuracy 92.578 (91.270)	Time 16.92
train_accuracy 91.164
one epoch duration:29.749807119369507
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.2473 (0.2493)	Accuracy 92.188 (91.168)	Time 16.91
train_accuracy 91.249
one epoch duration:29.973809003829956
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.2547 (0.2522)	Accuracy 92.578 (91.199)	Time 17.41
train_accuracy 91.331
one epoch duration:30.57996106147766
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.2669 (0.2474)	Accuracy 91.406 (91.496)	Time 17.37
train_accuracy 91.382
one epoch duration:30.398681163787842
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.1872 (0.2477)	Accuracy 93.750 (91.383)	Time 17.21
train_accuracy 91.531
one epoch duration:30.048612356185913
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2831 (0.2474)	Accuracy 89.844 (91.316)	Time 16.94
train_accuracy 91.373
one epoch duration:29.850004196166992
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.3051 (0.2453)	Accuracy 88.672 (91.371)	Time 17.01
train_accuracy 91.160
one epoch duration:29.979820489883423
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         FT | Ftot=5000 | ΔF:+60.40 ΔR: 0.38 ΔT: 4.77 | MIA:0.4581 PredDiff:14.41%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for FT_l1
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.9340 (0.9505)	Accuracy 89.453 (90.125)	Time 17.34
train_accuracy 90.204
one epoch duration:30.47887873649597
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.7687 (0.7944)	Accuracy 91.797 (90.824)	Time 17.32
train_accuracy 90.644
one epoch duration:30.313069343566895
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.6955 (0.6520)	Accuracy 91.016 (91.223)	Time 17.02
train_accuracy 91.040
one epoch duration:30.073323011398315
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.5748 (0.5188)	Accuracy 89.062 (91.078)	Time 17.34
train_accuracy 91.211
one epoch duration:30.451773166656494
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.3371 (0.3862)	Accuracy 94.141 (91.250)	Time 17.43
train_accuracy 91.116
one epoch duration:30.653433561325073
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2327 (0.2522)	Accuracy 92.969 (91.234)	Time 17.43
train_accuracy 91.213
one epoch duration:30.62856364250183
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2227 (0.2503)	Accuracy 92.188 (91.336)	Time 17.40
train_accuracy 91.293
one epoch duration:30.55929708480835
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.2996 (0.2567)	Accuracy 89.844 (91.094)	Time 17.42
train_accuracy 91.127
one epoch duration:30.49172568321228
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2254 (0.2500)	Accuracy 91.016 (91.297)	Time 17.03
train_accuracy 91.151
one epoch duration:29.929874181747437
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2553 (0.2570)	Accuracy 91.797 (90.848)	Time 16.98
train_accuracy 90.916
one epoch duration:29.85514521598816
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)      FT_l1 | Ftot=5000 | ΔF:+70.28 ΔR: 0.11 ΔT: 5.65 | MIA:0.4525 PredDiff:15.10%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for GA
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.980
one epoch duration:3.3149094581604004
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 16.360
one epoch duration:3.3081252574920654
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.420
one epoch duration:3.312330722808838
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.640
one epoch duration:3.3128933906555176
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.600
one epoch duration:3.263096570968628
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.680
one epoch duration:3.316149950027466
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.740
one epoch duration:3.31376051902771
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.280
one epoch duration:3.322382688522339
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.220
one epoch duration:3.3096752166748047
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.280
one epoch duration:3.2598376274108887
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         GA | Ftot=5000 | ΔF:+17.80 ΔR: 9.82 ΔT: 4.92 | MIA:0.4133 PredDiff:22.22%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}
[UNLEARN MODE] Batch-wise curriculum for NG
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -0.3087 (-0.3244)	Accuracy 87.891 (89.340)	Time 22.95
train_accuracy 89.756
one epoch duration:38.695075273513794
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -0.5064 (-0.4377)	Accuracy 91.797 (90.367)	Time 23.25
train_accuracy 90.324
one epoch duration:39.07146620750427
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -0.6032 (-0.5225)	Accuracy 91.797 (90.578)	Time 23.26
train_accuracy 90.622
one epoch duration:39.25809049606323
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -0.5575 (-0.6069)	Accuracy 89.453 (90.781)	Time 23.31
train_accuracy 90.947
one epoch duration:39.104055404663086
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -0.7478 (-0.6904)	Accuracy 91.797 (90.703)	Time 23.44
train_accuracy 90.687
one epoch duration:39.349385261535645
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         NG | Ftot=5000 | ΔF:+23.80 ΔR: 3.82 ΔT: 0.34 | MIA:0.4534 PredDiff:15.84%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Batch-wise curriculum for RL
Epoch #0, Learning rate: 0.001

--- [3/8] Experiment FAILED ---
============================================================

--- [4/8] Running Experiment ---
  - forget_set_definition: class
  - forget_partitioning_method: es
  - unlearning_granularity: sample
  - forget_partition_ordering: easy_first
  - use_retain_ordering: False
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[LOAD] Original model from saved_models/original_resnet18_E30_lr0.1_s42.pth
Defining forget set: all samples from class 0.
Partitioning 5000 forget samples using 'es' method...
Partition sizes for forget: [5000]

===== Running Method: FT =====
  > Applied specific params for FT: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for FT
Epoch #0, Learning rate: 0.001
Epoch: [0][99/176]	Loss 0.2698 (0.2817)	Accuracy 91.016 (90.184)	Time 17.22
train_accuracy 90.364
one epoch duration:30.312315225601196
Epoch #1, Learning rate: 0.001
Epoch: [1][99/176]	Loss 0.2619 (0.2558)	Accuracy 89.844 (91.133)	Time 17.32
train_accuracy 90.918
one epoch duration:30.33959722518921
Epoch #2, Learning rate: 0.001
Epoch: [2][99/176]	Loss 0.2393 (0.2550)	Accuracy 90.234 (90.824)	Time 17.14
train_accuracy 90.784
one epoch duration:30.16987633705139
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/176]	Loss 0.2585 (0.2503)	Accuracy 92.578 (91.270)	Time 17.28
train_accuracy 91.164
one epoch duration:30.180254459381104
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/176]	Loss 0.2473 (0.2493)	Accuracy 92.188 (91.168)	Time 16.93
train_accuracy 91.249
one epoch duration:29.78079390525818
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/176]	Loss 0.2547 (0.2522)	Accuracy 92.578 (91.199)	Time 17.23
train_accuracy 91.331
one epoch duration:30.200345039367676
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/176]	Loss 0.2669 (0.2474)	Accuracy 91.406 (91.496)	Time 17.11
train_accuracy 91.382
one epoch duration:30.11426830291748
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/176]	Loss 0.1872 (0.2477)	Accuracy 93.750 (91.383)	Time 17.12
train_accuracy 91.531
one epoch duration:30.080362558364868
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/176]	Loss 0.2831 (0.2474)	Accuracy 89.844 (91.316)	Time 17.23
train_accuracy 91.373
one epoch duration:30.22829508781433
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/176]	Loss 0.3051 (0.2453)	Accuracy 88.672 (91.371)	Time 17.23
train_accuracy 91.160
one epoch duration:30.3145432472229
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         FT | Ftot=5000 | ΔF:+60.40 ΔR: 0.38 ΔT: 4.77 | MIA:0.4581 PredDiff:14.41%

===== Running Method: FT_l1 =====
  > Applied specific params for FT_l1: {'unlearn_epochs': 10, 'unlearn_lr': 0.005, 'alpha': 1e-05, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for FT_l1
Epoch #0, Learning rate: 0.0005
Epoch: [0][99/176]	Loss 0.9340 (0.9505)	Accuracy 89.453 (90.125)	Time 17.51
train_accuracy 90.204
one epoch duration:30.78242039680481
Epoch #1, Learning rate: 0.0005
Epoch: [1][99/176]	Loss 0.7687 (0.7944)	Accuracy 91.797 (90.824)	Time 17.36
train_accuracy 90.644
one epoch duration:30.522398710250854
Epoch #2, Learning rate: 0.0005
Epoch: [2][99/176]	Loss 0.6955 (0.6520)	Accuracy 91.016 (91.223)	Time 17.31
train_accuracy 91.040
one epoch duration:30.445990085601807
Epoch #3, Learning rate: 5e-05
Epoch: [3][99/176]	Loss 0.5748 (0.5188)	Accuracy 89.062 (91.078)	Time 17.29
train_accuracy 91.211
one epoch duration:30.39875602722168
Epoch #4, Learning rate: 5e-05
Epoch: [4][99/176]	Loss 0.3371 (0.3862)	Accuracy 94.141 (91.250)	Time 17.30
train_accuracy 91.116
one epoch duration:30.43306875228882
Epoch #5, Learning rate: 5e-05
Epoch: [5][99/176]	Loss 0.2327 (0.2522)	Accuracy 92.969 (91.234)	Time 17.26
train_accuracy 91.213
one epoch duration:30.377870798110962
Epoch #6, Learning rate: 5e-05
Epoch: [6][99/176]	Loss 0.2227 (0.2503)	Accuracy 92.188 (91.336)	Time 17.30
train_accuracy 91.293
one epoch duration:30.403949975967407
Epoch #7, Learning rate: 5e-05
Epoch: [7][99/176]	Loss 0.2996 (0.2567)	Accuracy 89.844 (91.094)	Time 17.30
train_accuracy 91.127
one epoch duration:30.434823989868164
Epoch #8, Learning rate: 5e-05
Epoch: [8][99/176]	Loss 0.2254 (0.2500)	Accuracy 91.016 (91.297)	Time 17.31
train_accuracy 91.151
one epoch duration:30.420005083084106
Epoch #9, Learning rate: 5e-05
Epoch: [9][99/176]	Loss 0.2553 (0.2570)	Accuracy 91.797 (90.848)	Time 17.33
train_accuracy 90.916
one epoch duration:30.456708192825317
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)      FT_l1 | Ftot=5000 | ΔF:+70.28 ΔR: 0.11 ΔT: 5.65 | MIA:0.4525 PredDiff:15.10%

===== Running Method: GA =====
  > Applied specific params for GA: {'unlearn_epochs': 10, 'unlearn_lr': 0.0001, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for GA
Epoch #0, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.000
one epoch duration:3.348834753036499
Epoch #1, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 15.260
one epoch duration:3.346775770187378
Epoch #2, Learning rate: 1e-05
len(train_loader):  20
train_accuracy 14.860
one epoch duration:3.349932909011841
Epoch #3, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.840
one epoch duration:3.3491296768188477
Epoch #4, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 15.140
one epoch duration:3.3490633964538574
Epoch #5, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.500
one epoch duration:3.346296548843384
Epoch #6, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.960
one epoch duration:3.358870506286621
Epoch #7, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.540
one epoch duration:3.3524560928344727
Epoch #8, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.180
one epoch duration:3.3503880500793457
Epoch #9, Learning rate: 1.0000000000000002e-06
len(train_loader):  20
train_accuracy 14.400
one epoch duration:3.349626302719116
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         GA | Ftot=5000 | ΔF:+15.52 ΔR:11.13 ΔT: 6.12 | MIA:0.4029 PredDiff:23.38%

===== Running Method: NG =====
  > Applied specific params for NG: {'unlearn_epochs': 5, 'unlearn_lr': 0.01, 'alpha': 0.9, 'decreasing_lr': '2,4'}
[UNLEARN MODE] Sample-wise curriculum for NG
Epoch #0, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [0][99/176]	Loss -0.3520 (-0.3307)	Accuracy 90.234 (89.254)	Time 23.26
train_accuracy 89.469
one epoch duration:39.06732249259949
Epoch #1, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [1][99/176]	Loss -0.4315 (-0.4410)	Accuracy 88.672 (90.633)	Time 23.29
train_accuracy 90.504
one epoch duration:39.11079168319702
Epoch #2, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [2][99/176]	Loss -0.6135 (-0.5273)	Accuracy 93.359 (90.559)	Time 23.18
train_accuracy 90.569
one epoch duration:38.87481713294983
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [3][99/176]	Loss -0.6962 (-0.6121)	Accuracy 92.188 (90.961)	Time 22.91
train_accuracy 90.827
one epoch duration:38.54285502433777
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
Epoch: [4][99/176]	Loss -0.7346 (-0.7027)	Accuracy 89.453 (90.887)	Time 22.89
train_accuracy 91.007
one epoch duration:38.487290143966675
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         NG | Ftot=5000 | ΔF:+24.54 ΔR: 3.82 ΔT: 0.08 | MIA:0.4450 PredDiff:15.82%

===== Running Method: RL =====
  > Applied specific params for RL: {'unlearn_epochs': 10, 'unlearn_lr': 0.01, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for RL
Epoch #0, Learning rate: 0.001
Epoch: [0][99/196]	Loss 0.6084 (0.7537)	Accuracy 81.641 (82.578)	Time 13.57
Epoch: [0][199/196]	Loss 0.5112 (0.6349)	Accuracy 81.641 (82.841)	Time 16.94
one epoch duration:33.240190505981445
Epoch #1, Learning rate: 0.001
Epoch: [1][99/196]	Loss 0.5455 (0.5129)	Accuracy 80.469 (82.754)	Time 13.61
Epoch: [1][199/196]	Loss 0.5249 (0.5076)	Accuracy 81.641 (82.897)	Time 17.20
one epoch duration:33.59721517562866
Epoch #2, Learning rate: 0.001
Epoch: [2][99/196]	Loss 0.3670 (0.4931)	Accuracy 86.719 (82.993)	Time 13.77
Epoch: [2][199/196]	Loss 0.4439 (0.4935)	Accuracy 85.156 (82.936)	Time 17.24
one epoch duration:33.80383276939392
Epoch #3, Learning rate: 0.0001
Epoch: [3][99/196]	Loss 0.5086 (0.4774)	Accuracy 81.641 (83.638)	Time 13.76
Epoch: [3][199/196]	Loss 0.4503 (0.4897)	Accuracy 83.594 (83.108)	Time 17.21
one epoch duration:33.746464014053345
Epoch #4, Learning rate: 0.0001
Epoch: [4][99/196]	Loss 0.6000 (0.4925)	Accuracy 79.688 (82.910)	Time 13.81
Epoch: [4][199/196]	Loss 0.5038 (0.4850)	Accuracy 82.422 (83.188)	Time 17.25
one epoch duration:33.83490347862244
Epoch #5, Learning rate: 0.0001
Epoch: [5][99/196]	Loss 0.6560 (0.4839)	Accuracy 77.734 (83.315)	Time 13.74
Epoch: [5][199/196]	Loss 0.5118 (0.4856)	Accuracy 83.203 (83.177)	Time 17.17
one epoch duration:33.68989443778992
Epoch #6, Learning rate: 0.0001
Epoch: [6][99/196]	Loss 0.4982 (0.4919)	Accuracy 81.641 (82.661)	Time 13.78
Epoch: [6][199/196]	Loss 0.4513 (0.4867)	Accuracy 84.766 (82.941)	Time 17.25
one epoch duration:33.809375286102295
Epoch #7, Learning rate: 0.0001
Epoch: [7][99/196]	Loss 0.4419 (0.4805)	Accuracy 83.984 (83.438)	Time 13.76
Epoch: [7][199/196]	Loss 0.4536 (0.4850)	Accuracy 85.938 (83.147)	Time 17.24
one epoch duration:33.79407072067261
Epoch #8, Learning rate: 0.0001
Epoch: [8][99/196]	Loss 0.4766 (0.4880)	Accuracy 83.594 (83.066)	Time 13.86
Epoch: [8][199/196]	Loss 0.5016 (0.4835)	Accuracy 80.859 (83.136)	Time 17.30
one epoch duration:33.943602323532104
Epoch #9, Learning rate: 0.0001
Epoch: [9][99/196]	Loss 0.5685 (0.4757)	Accuracy 79.297 (83.350)	Time 13.55
Epoch: [9][199/196]	Loss 0.4762 (0.4865)	Accuracy 84.375 (83.006)	Time 16.92
one epoch duration:33.211167097091675
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)         RL | Ftot=5000 | ΔF:+8.62 ΔR: 0.33 ΔT: 0.38 | MIA:0.4501 PredDiff:12.80%

===== Running Method: Wfisher =====
  > Applied specific params for Wfisher: {'alpha': 10.0}
[UNLEARN MODE] Sample-wise curriculum for Wfisher
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)    Wfisher | Ftot=5000 | ΔF:+0.00 ΔR:81.81 ΔT:66.20 | MIA:0.5000 PredDiff:86.74%

===== Running Method: SCRUB =====
  > Applied specific params for SCRUB: {'unlearn_epochs': 10, 'kd_T': 4.0, 'gamma': 1.0, 'beta': 1.0, 'msteps': 5, 'decreasing_lr': '5,8'}
[UNLEARN MODE] Sample-wise curriculum for SCRUB
Epoch #0, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [0][19/20]	Time 0.105 (0.170)	Data 0.069 (0.123)	Loss -29.6496 (-15.9176)	Forget_Acc@1 0.000 (7.060)
*** Minimize step ***
Epoch: [0][175/176]	Time 0.148 (0.172)	Data 0.104 (0.126)	Loss 0.4189 (1.3561)	Retain_Acc@1 90.500 (88.760)
Epoch: [0]	 train-acc:	88.76	 train-loss: 1.3561198712295957
one epoch duration:33.66381859779358
Epoch #1, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [1][19/20]	Time 0.100 (0.168)	Data 0.068 (0.124)	Loss -39.9273 (-24.5253)	Forget_Acc@1 0.000 (1.500)
*** Minimize step ***
Epoch: [1][175/176]	Time 0.144 (0.170)	Data 0.104 (0.126)	Loss 0.6720 (1.9504)	Retain_Acc@1 92.500 (88.929)
Epoch: [1]	 train-acc:	88.92888888888889	 train-loss: 1.95042622373369
one epoch duration:33.30945038795471
Epoch #2, Learning rate: 0.001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [2][19/20]	Time 0.100 (0.167)	Data 0.068 (0.122)	Loss -50.1381 (-32.5167)	Forget_Acc@1 0.000 (1.200)
*** Minimize step ***
Epoch: [2][175/176]	Time 0.146 (0.172)	Data 0.102 (0.125)	Loss 0.8633 (2.6655)	Retain_Acc@1 92.000 (87.727)
Epoch: [2]	 train-acc:	87.72666666666667	 train-loss: 2.6654966183874342
one epoch duration:33.52066087722778
Epoch #3, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [3][19/20]	Time 0.105 (0.167)	Data 0.068 (0.122)	Loss -34.0160 (-28.9131)	Forget_Acc@1 0.000 (1.320)
*** Minimize step ***
Epoch: [3][175/176]	Time 0.146 (0.172)	Data 0.103 (0.125)	Loss 1.0076 (1.1199)	Retain_Acc@1 90.000 (89.611)
Epoch: [3]	 train-acc:	89.61111111111111	 train-loss: 1.119866541586982
one epoch duration:33.562628507614136
Epoch #4, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [4][19/20]	Time 0.103 (0.169)	Data 0.068 (0.122)	Loss -37.1534 (-33.7709)	Forget_Acc@1 0.735 (1.320)
*** Minimize step ***
Epoch: [4][175/176]	Time 0.145 (0.169)	Data 0.101 (0.124)	Loss 0.9540 (1.2137)	Retain_Acc@1 92.500 (89.596)
Epoch: [4]	 train-acc:	89.59555555555555	 train-loss: 1.2136959418614706
one epoch duration:33.15061163902283
Epoch #5, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Maximize step ***
Epoch: [5][19/20]	Time 0.096 (0.164)	Data 0.065 (0.120)	Loss -41.6225 (-37.1537)	Forget_Acc@1 0.735 (1.060)
*** Minimize step ***
Epoch: [5][175/176]	Time 0.143 (0.168)	Data 0.103 (0.123)	Loss 1.2610 (1.2713)	Retain_Acc@1 90.000 (89.722)
Epoch: [5]	 train-acc:	89.72222222222223	 train-loss: 1.2713217645009358
one epoch duration:32.92054080963135
Epoch #6, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [6][175/176]	Time 0.146 (0.169)	Data 0.101 (0.123)	Loss 1.0404 (0.9782)	Retain_Acc@1 89.500 (89.976)
Epoch: [6]	 train-acc:	89.97555555555556	 train-loss: 0.9781588575575086
one epoch duration:29.766184329986572
Epoch #7, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [7][175/176]	Time 0.157 (0.169)	Data 0.113 (0.124)	Loss 0.8002 (0.8590)	Retain_Acc@1 90.000 (90.149)
Epoch: [7]	 train-acc:	90.14888888888889	 train-loss: 0.8589565735075209
one epoch duration:29.763042211532593
Epoch #8, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [8][175/176]	Time 0.148 (0.170)	Data 0.103 (0.124)	Loss 0.7350 (0.7749)	Retain_Acc@1 90.000 (90.231)
Epoch: [8]	 train-acc:	90.2311111111111	 train-loss: 0.7749394984351264
one epoch duration:29.948720455169678
Epoch #9, Learning rate: 0.0001
len(r_loader): 176, len(f_loader): 20
*** Minimize step ***
Epoch: [9][175/176]	Time 0.145 (0.169)	Data 0.103 (0.123)	Loss 0.6604 (0.7063)	Retain_Acc@1 89.500 (90.324)
Epoch: [9]	 train-acc:	90.32444444444444	 train-loss: 0.7063036176681519
one epoch duration:29.745545148849487
[LOAD] Retrain from saved_models/retrain_a62f7f023205747846b1034cd18248029bb5c0bd_resnet18_E30_lr0.1_s42.pth
  (Final Eval)      SCRUB | Ftot=5000 | ΔF:+65.10 ΔR: 0.70 ΔT: 5.14 | MIA:0.4572 PredDiff:15.73%

===== Full Results =====
 method stage  forget_total  Retrain_F  Retrain_R  Retrain_T  Unlearn_F  Unlearn_R  Unlearn_T    ΔF        ΔR    ΔT      MIA  PredDiff(%)
     FT final          5000        0.0      92.92       76.2      60.40  93.304444      80.97 60.40  0.384444  4.77 0.458067       14.410
  FT_l1 final          5000        0.0      92.92       76.2      70.28  93.028889      81.85 70.28  0.108889  5.65 0.452544       15.104
     GA final          5000        0.0      92.92       76.2      15.52  81.793333      70.08 15.52 11.126667  6.12 0.402889       23.380
     NG final          5000        0.0      92.92       76.2      24.54  89.102222      76.12 24.54  3.817778  0.08 0.445011       15.820
     RL final          5000        0.0      92.92       76.2       8.62  93.251111      76.58  8.62  0.331111  0.38 0.450056       12.802
Wfisher final          5000        0.0      92.92       76.2       0.00  11.111111      10.00  0.00 81.808889 66.20 0.500000       86.742
  SCRUB final          5000        0.0      92.92       76.2      65.10  92.220000      81.34 65.10  0.700000  5.14 0.457233       15.730

Results saved to saved_models/results_class_es_sample_easy_first.csv

--- [4/8] Experiment FINISHED ---
============================================================
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_all_conditions.py", line 91, in main
    run_experiment(run_config)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_experiment.py", line 166, in run_experiment
    model_u = unlearn_fn(model_u, loaders, method_config)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/methods.py", line 95, in unlearn_rl
    return _run_iterative_unlearn(unlearn_method, m, loaders, cfg)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/methods.py", line 38, in _run_iterative_unlearn
    unlearn_fn(
  File "/workspace/machine-unlearning/curriculum_unlearning/official/unlearn_methods/impl.py", line 284, in _wrapped
    train_acc = unlearn_iter_func(
  File "/workspace/machine-unlearning/curriculum_unlearning/official/unlearn_methods/RL_original.py", line 24, in RL_og
    original_dataset = forget_dataset.dataset
AttributeError: 'InterleavedDataset' object has no attribute 'dataset'
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
Traceback (most recent call last):
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_all_conditions.py", line 106, in <module>
    main(args)
  File "/workspace/machine-unlearning/curriculum_unlearning/official/run_all_conditions.py", line 81, in main
    fname_parts.append(f"paired_{exp_params['retain_ordering']}")
KeyError: 'retain_ordering'

ERROR conda.cli.main_run:execute(127): `conda run python run_all_conditions.py --config_module config_es` failed. (See above for error)

