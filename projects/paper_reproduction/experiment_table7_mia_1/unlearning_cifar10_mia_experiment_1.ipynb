{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff91a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 21 14:14:13 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              37W / 250W |  13436MiB / 16384MiB |      8%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              30W / 250W |   6302MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   45C    P0              34W / 250W |   8370MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   54C    P0              80W / 250W |   9384MiB / 16384MiB |     71%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              36W / 250W |  15940MiB / 16384MiB |     24%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   44C    P0              65W / 250W |   9076MiB / 16384MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              31W / 250W |   5290MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              48W / 250W |  14760MiB / 16384MiB |     15%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4b6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device (GPU if available)\n",
    "DEVICE_NUM = 6\n",
    "ADDITIONAL_GPU = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}:{DEVICE_NUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d828e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global random seed set to 42\n",
      "Using device: cuda\n",
      "\n",
      "==================== Starting Run 1/3 ====================\n",
      "\n",
      "[TRAINING] original model\n",
      "    Epoch 1/30 completed in 32.62s\n",
      "    Epoch 2/30 completed in 31.31s\n",
      "    Epoch 3/30 completed in 31.98s\n",
      "    Epoch 4/30 completed in 31.47s\n",
      "    Epoch 5/30 completed in 31.81s\n",
      "    Epoch 6/30 completed in 31.55s\n",
      "    Epoch 7/30 completed in 31.53s\n",
      "    Epoch 8/30 completed in 32.11s\n",
      "    Epoch 9/30 completed in 31.29s\n",
      "    Epoch 10/30 completed in 31.61s\n",
      "    Epoch 11/30 completed in 31.38s\n",
      "    Epoch 12/30 completed in 31.79s\n",
      "    Epoch 13/30 completed in 31.55s\n",
      "    Epoch 14/30 completed in 31.29s\n",
      "    Epoch 15/30 completed in 32.22s\n",
      "    Epoch 16/30 completed in 31.52s\n",
      "    Epoch 17/30 completed in 31.18s\n",
      "    Epoch 18/30 completed in 31.62s\n",
      "    Epoch 19/30 completed in 31.87s\n",
      "    Epoch 20/30 completed in 31.46s\n",
      "    Epoch 21/30 completed in 31.25s\n",
      "    Epoch 22/30 completed in 30.78s\n",
      "    Epoch 23/30 completed in 31.21s\n",
      "    Epoch 24/30 completed in 31.48s\n",
      "    Epoch 25/30 completed in 31.39s\n",
      "    Epoch 26/30 completed in 31.32s\n",
      "    Epoch 27/30 completed in 30.86s\n",
      "    Epoch 28/30 completed in 31.31s\n",
      "    Epoch 29/30 completed in 31.38s\n",
      "    Epoch 30/30 completed in 31.70s\n",
      "\n",
      "Creating ES partitions...\n",
      "      Batch 40/196\n",
      "      Batch 80/196\n",
      "      Batch 120/196\n",
      "      Batch 160/196\n",
      "ES partitions created in 27.18s.\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "[TRAINING] retrained model for Low ES\n",
      "    Epoch 1/30 completed in 29.81s\n",
      "    Epoch 2/30 completed in 29.44s\n",
      "    Epoch 3/30 completed in 29.88s\n",
      "    Epoch 4/30 completed in 30.89s\n",
      "    Epoch 5/30 completed in 29.96s\n",
      "    Epoch 6/30 completed in 29.65s\n",
      "    Epoch 7/30 completed in 29.80s\n",
      "    Epoch 8/30 completed in 29.78s\n",
      "    Epoch 9/30 completed in 29.88s\n",
      "    Epoch 10/30 completed in 29.59s\n",
      "    Epoch 11/30 completed in 30.98s\n",
      "    Epoch 12/30 completed in 29.88s\n",
      "    Epoch 13/30 completed in 29.59s\n",
      "    Epoch 14/30 completed in 31.33s\n",
      "    Epoch 15/30 completed in 29.64s\n",
      "    Epoch 16/30 completed in 29.51s\n",
      "    Epoch 17/30 completed in 30.24s\n",
      "    Epoch 18/30 completed in 30.44s\n",
      "    Epoch 19/30 completed in 29.86s\n",
      "    Epoch 20/30 completed in 30.27s\n",
      "    Epoch 21/30 completed in 29.97s\n",
      "    Epoch 22/30 completed in 29.82s\n",
      "    Epoch 23/30 completed in 29.73s\n",
      "    Epoch 24/30 completed in 29.71s\n",
      "    Epoch 25/30 completed in 29.39s\n",
      "    Epoch 26/30 completed in 31.43s\n",
      "    Epoch 27/30 completed in 30.41s\n",
      "    Epoch 28/30 completed in 29.78s\n",
      "    Epoch 29/30 completed in 30.24s\n",
      "    Epoch 30/30 completed in 32.02s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:98.83% R:90.74% T:84.09%  MIA:0.441\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.37% R:90.44% T:83.94%  MIA:0.449\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 29.95s\n",
      "    Epoch 2/10 completed in 29.91s\n",
      "    Epoch 3/10 completed in 29.66s\n",
      "    Epoch 4/10 completed in 29.52s\n",
      "    Epoch 5/10 completed in 29.65s\n",
      "    Epoch 6/10 completed in 29.90s\n",
      "    Epoch 7/10 completed in 30.67s\n",
      "    Epoch 8/10 completed in 29.68s\n",
      "    Epoch 9/10 completed in 29.85s\n",
      "    Epoch 10/10 completed in 29.63s\n",
      "      - Fine-tune  F:99.17% R:89.49% T:82.62%  MIA:0.449\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:98.93% R:90.11% T:82.83%  MIA:0.447\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:98.30% R:83.56% T:79.19%  MIA:0.429\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:98.47% R:87.50% T:82.05%  MIA:0.437\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:84.23% R:80.07% T:74.27%  MIA:0.471\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:2.53% R:10.00% T:8.78%  MIA:0.614\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.93s\n",
      "    Epoch 2/10 completed in 1.89s\n",
      "    Epoch 3/10 completed in 1.90s\n",
      "    Epoch 4/10 completed in 1.90s\n",
      "    Epoch 5/10 completed in 1.91s\n",
      "    Epoch 6/10 completed in 1.91s\n",
      "    Epoch 7/10 completed in 1.88s\n",
      "    Epoch 8/10 completed in 1.89s\n",
      "    Epoch 9/10 completed in 1.89s\n",
      "    Epoch 10/10 completed in 1.91s\n",
      "      - Random-label  F:3.13% R:9.95% T:9.16%  MIA:0.582\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "[TRAINING] retrained model for Medium ES\n",
      "    Epoch 1/30 completed in 29.99s\n",
      "    Epoch 2/30 completed in 29.72s\n",
      "    Epoch 3/30 completed in 30.20s\n",
      "    Epoch 4/30 completed in 29.79s\n",
      "    Epoch 5/30 completed in 32.93s\n",
      "    Epoch 6/30 completed in 29.70s\n",
      "    Epoch 7/30 completed in 31.11s\n",
      "    Epoch 8/30 completed in 33.53s\n",
      "    Epoch 9/30 completed in 31.75s\n",
      "    Epoch 10/30 completed in 29.90s\n",
      "    Epoch 11/30 completed in 29.61s\n",
      "    Epoch 12/30 completed in 29.30s\n",
      "    Epoch 13/30 completed in 29.46s\n",
      "    Epoch 14/30 completed in 30.51s\n",
      "    Epoch 15/30 completed in 29.91s\n",
      "    Epoch 16/30 completed in 29.40s\n",
      "    Epoch 17/30 completed in 30.18s\n",
      "    Epoch 18/30 completed in 30.02s\n",
      "    Epoch 19/30 completed in 29.73s\n",
      "    Epoch 20/30 completed in 32.89s\n",
      "    Epoch 21/30 completed in 29.75s\n",
      "    Epoch 22/30 completed in 29.70s\n",
      "    Epoch 23/30 completed in 30.38s\n",
      "    Epoch 24/30 completed in 31.81s\n",
      "    Epoch 25/30 completed in 30.20s\n",
      "    Epoch 26/30 completed in 29.60s\n",
      "    Epoch 27/30 completed in 29.53s\n",
      "    Epoch 28/30 completed in 29.79s\n",
      "    Epoch 29/30 completed in 30.35s\n",
      "    Epoch 30/30 completed in 33.14s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:98.67% R:90.81% T:83.64%  MIA:0.446\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.57% R:90.44% T:83.94%  MIA:0.454\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 30.04s\n",
      "    Epoch 2/10 completed in 29.94s\n",
      "    Epoch 3/10 completed in 29.95s\n",
      "    Epoch 4/10 completed in 29.81s\n",
      "    Epoch 5/10 completed in 29.98s\n",
      "    Epoch 6/10 completed in 30.13s\n",
      "    Epoch 7/10 completed in 31.61s\n",
      "    Epoch 8/10 completed in 29.89s\n",
      "    Epoch 9/10 completed in 30.03s\n",
      "    Epoch 10/10 completed in 30.08s\n",
      "      - Fine-tune  F:98.87% R:89.92% T:83.29%  MIA:0.453\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:98.67% R:89.28% T:82.09%  MIA:0.455\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:98.43% R:85.52% T:80.70%  MIA:0.433\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:98.43% R:88.63% T:82.68%  MIA:0.434\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:84.13% R:80.31% T:74.63%  MIA:0.453\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:3.80% R:10.44% T:9.05%  MIA:0.554\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.94s\n",
      "    Epoch 2/10 completed in 1.96s\n",
      "    Epoch 3/10 completed in 1.94s\n",
      "    Epoch 4/10 completed in 1.88s\n",
      "    Epoch 5/10 completed in 1.91s\n",
      "    Epoch 6/10 completed in 1.92s\n",
      "    Epoch 7/10 completed in 1.95s\n",
      "    Epoch 8/10 completed in 1.91s\n",
      "    Epoch 9/10 completed in 1.93s\n",
      "    Epoch 10/10 completed in 1.93s\n",
      "      - Random-label  F:3.57% R:8.87% T:8.23%  MIA:0.587\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "[TRAINING] retrained model for High ES\n",
      "    Epoch 1/30 completed in 29.65s\n",
      "    Epoch 2/30 completed in 29.73s\n",
      "    Epoch 3/30 completed in 30.31s\n",
      "    Epoch 4/30 completed in 29.81s\n",
      "    Epoch 5/30 completed in 29.87s\n",
      "    Epoch 6/30 completed in 32.43s\n",
      "    Epoch 7/30 completed in 29.90s\n",
      "    Epoch 8/30 completed in 30.47s\n",
      "    Epoch 9/30 completed in 30.05s\n",
      "    Epoch 10/30 completed in 30.02s\n",
      "    Epoch 11/30 completed in 30.03s\n",
      "    Epoch 12/30 completed in 30.03s\n",
      "    Epoch 13/30 completed in 30.02s\n",
      "    Epoch 14/30 completed in 30.09s\n",
      "    Epoch 15/30 completed in 29.30s\n",
      "    Epoch 16/30 completed in 29.60s\n",
      "    Epoch 17/30 completed in 31.62s\n",
      "    Epoch 18/30 completed in 30.47s\n",
      "    Epoch 19/30 completed in 30.50s\n",
      "    Epoch 20/30 completed in 29.57s\n",
      "    Epoch 21/30 completed in 29.74s\n",
      "    Epoch 22/30 completed in 29.71s\n",
      "    Epoch 23/30 completed in 29.99s\n",
      "    Epoch 24/30 completed in 29.90s\n",
      "    Epoch 25/30 completed in 29.80s\n",
      "    Epoch 26/30 completed in 29.95s\n",
      "    Epoch 27/30 completed in 29.52s\n",
      "    Epoch 28/30 completed in 29.88s\n",
      "    Epoch 29/30 completed in 29.89s\n",
      "    Epoch 30/30 completed in 31.69s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:98.07% R:90.63% T:84.40%  MIA:0.460\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.17% R:90.57% T:83.94%  MIA:0.449\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 29.86s\n",
      "    Epoch 2/10 completed in 29.35s\n",
      "    Epoch 3/10 completed in 30.22s\n",
      "    Epoch 4/10 completed in 30.08s\n",
      "    Epoch 5/10 completed in 29.37s\n",
      "    Epoch 6/10 completed in 30.11s\n",
      "    Epoch 7/10 completed in 29.69s\n",
      "    Epoch 8/10 completed in 29.26s\n",
      "    Epoch 9/10 completed in 29.59s\n",
      "    Epoch 10/10 completed in 29.93s\n",
      "      - Fine-tune  F:97.83% R:89.94% T:83.24%  MIA:0.442\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:98.50% R:89.93% T:82.93%  MIA:0.453\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:97.77% R:86.12% T:80.97%  MIA:0.433\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:97.87% R:88.42% T:82.71%  MIA:0.438\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:84.33% R:80.10% T:73.85%  MIA:0.475\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:4.67% R:7.57% T:7.01%  MIA:0.536\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.90s\n",
      "    Epoch 2/10 completed in 1.88s\n",
      "    Epoch 3/10 completed in 1.89s\n",
      "    Epoch 4/10 completed in 1.88s\n",
      "    Epoch 5/10 completed in 1.86s\n",
      "    Epoch 6/10 completed in 1.88s\n",
      "    Epoch 7/10 completed in 1.87s\n",
      "    Epoch 8/10 completed in 1.91s\n",
      "    Epoch 9/10 completed in 1.90s\n",
      "    Epoch 10/10 completed in 1.95s\n",
      "      - Random-label  F:6.67% R:10.37% T:9.74%  MIA:0.537\n",
      "\n",
      "==================== Starting Run 2/3 ====================\n",
      "\n",
      "[TRAINING] original model\n",
      "    Epoch 1/30 completed in 34.28s\n",
      "    Epoch 2/30 completed in 31.73s\n",
      "    Epoch 3/30 completed in 31.84s\n",
      "    Epoch 4/30 completed in 31.66s\n",
      "    Epoch 5/30 completed in 31.81s\n",
      "    Epoch 6/30 completed in 31.63s\n",
      "    Epoch 7/30 completed in 31.52s\n",
      "    Epoch 8/30 completed in 31.90s\n",
      "    Epoch 9/30 completed in 31.36s\n",
      "    Epoch 10/30 completed in 32.17s\n",
      "    Epoch 11/30 completed in 32.74s\n",
      "    Epoch 12/30 completed in 32.38s\n",
      "    Epoch 13/30 completed in 31.83s\n",
      "    Epoch 14/30 completed in 31.71s\n",
      "    Epoch 15/30 completed in 31.90s\n",
      "    Epoch 16/30 completed in 31.86s\n",
      "    Epoch 17/30 completed in 31.73s\n",
      "    Epoch 18/30 completed in 31.68s\n",
      "    Epoch 19/30 completed in 33.14s\n",
      "    Epoch 20/30 completed in 32.91s\n",
      "    Epoch 21/30 completed in 31.16s\n",
      "    Epoch 22/30 completed in 31.98s\n",
      "    Epoch 23/30 completed in 33.94s\n",
      "    Epoch 24/30 completed in 31.60s\n",
      "    Epoch 25/30 completed in 32.54s\n",
      "    Epoch 26/30 completed in 31.45s\n",
      "    Epoch 27/30 completed in 31.68s\n",
      "    Epoch 28/30 completed in 31.29s\n",
      "    Epoch 29/30 completed in 31.60s\n",
      "    Epoch 30/30 completed in 30.16s\n",
      "\n",
      "Creating ES partitions...\n",
      "      Batch 40/196\n",
      "      Batch 80/196\n",
      "      Batch 120/196\n",
      "      Batch 160/196\n",
      "ES partitions created in 30.00s.\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "[TRAINING] retrained model for Low ES\n",
      "    Epoch 1/30 completed in 30.20s\n",
      "    Epoch 2/30 completed in 29.74s\n",
      "    Epoch 3/30 completed in 29.72s\n",
      "    Epoch 4/30 completed in 29.58s\n",
      "    Epoch 5/30 completed in 30.23s\n",
      "    Epoch 6/30 completed in 30.24s\n",
      "    Epoch 7/30 completed in 30.60s\n",
      "    Epoch 8/30 completed in 29.86s\n",
      "    Epoch 9/30 completed in 29.53s\n",
      "    Epoch 10/30 completed in 30.07s\n",
      "    Epoch 11/30 completed in 29.67s\n",
      "    Epoch 12/30 completed in 30.64s\n",
      "    Epoch 13/30 completed in 32.32s\n",
      "    Epoch 14/30 completed in 31.90s\n",
      "    Epoch 15/30 completed in 30.90s\n",
      "    Epoch 16/30 completed in 29.93s\n",
      "    Epoch 17/30 completed in 29.90s\n",
      "    Epoch 18/30 completed in 29.60s\n",
      "    Epoch 19/30 completed in 30.08s\n",
      "    Epoch 20/30 completed in 29.93s\n",
      "    Epoch 21/30 completed in 29.86s\n",
      "    Epoch 22/30 completed in 30.35s\n",
      "    Epoch 23/30 completed in 29.02s\n",
      "    Epoch 24/30 completed in 30.19s\n",
      "    Epoch 25/30 completed in 29.54s\n",
      "    Epoch 26/30 completed in 29.52s\n",
      "    Epoch 27/30 completed in 30.01s\n",
      "    Epoch 28/30 completed in 29.78s\n",
      "    Epoch 29/30 completed in 30.87s\n",
      "    Epoch 30/30 completed in 30.20s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:98.80% R:89.64% T:83.30%  MIA:0.456\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.63% R:90.76% T:84.16%  MIA:0.446\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 30.04s\n",
      "    Epoch 2/10 completed in 29.73s\n",
      "    Epoch 3/10 completed in 29.97s\n",
      "    Epoch 4/10 completed in 29.94s\n",
      "    Epoch 5/10 completed in 31.20s\n",
      "    Epoch 6/10 completed in 30.33s\n",
      "    Epoch 7/10 completed in 29.97s\n",
      "    Epoch 8/10 completed in 30.59s\n",
      "    Epoch 9/10 completed in 30.48s\n",
      "    Epoch 10/10 completed in 29.60s\n",
      "      - Fine-tune  F:99.27% R:89.87% T:82.91%  MIA:0.454\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:99.23% R:90.25% T:83.30%  MIA:0.447\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:98.87% R:85.50% T:81.04%  MIA:0.442\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:99.17% R:89.09% T:83.60%  MIA:0.445\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:87.50% R:80.36% T:76.13%  MIA:0.466\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:1.20% R:7.88% T:7.17%  MIA:0.642\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.87s\n",
      "    Epoch 2/10 completed in 2.01s\n",
      "    Epoch 3/10 completed in 1.96s\n",
      "    Epoch 4/10 completed in 1.92s\n",
      "    Epoch 5/10 completed in 1.91s\n",
      "    Epoch 6/10 completed in 1.95s\n",
      "    Epoch 7/10 completed in 2.21s\n",
      "    Epoch 8/10 completed in 1.99s\n",
      "    Epoch 9/10 completed in 1.95s\n",
      "    Epoch 10/10 completed in 1.92s\n",
      "      - Random-label  F:0.73% R:8.66% T:7.73%  MIA:0.670\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "[TRAINING] retrained model for Medium ES\n",
      "    Epoch 1/30 completed in 29.85s\n",
      "    Epoch 2/30 completed in 29.87s\n",
      "    Epoch 3/30 completed in 30.14s\n",
      "    Epoch 4/30 completed in 30.01s\n",
      "    Epoch 5/30 completed in 29.71s\n",
      "    Epoch 6/30 completed in 29.90s\n",
      "    Epoch 7/30 completed in 30.00s\n",
      "    Epoch 8/30 completed in 29.78s\n",
      "    Epoch 9/30 completed in 30.28s\n",
      "    Epoch 10/30 completed in 30.11s\n",
      "    Epoch 11/30 completed in 30.35s\n",
      "    Epoch 12/30 completed in 31.05s\n",
      "    Epoch 13/30 completed in 29.82s\n",
      "    Epoch 14/30 completed in 30.89s\n",
      "    Epoch 15/30 completed in 31.38s\n",
      "    Epoch 16/30 completed in 32.06s\n",
      "    Epoch 17/30 completed in 32.36s\n",
      "    Epoch 18/30 completed in 30.83s\n",
      "    Epoch 19/30 completed in 29.91s\n",
      "    Epoch 20/30 completed in 30.09s\n",
      "    Epoch 21/30 completed in 29.76s\n",
      "    Epoch 22/30 completed in 29.59s\n",
      "    Epoch 23/30 completed in 30.05s\n",
      "    Epoch 24/30 completed in 30.09s\n",
      "    Epoch 25/30 completed in 30.28s\n",
      "    Epoch 26/30 completed in 29.88s\n",
      "    Epoch 27/30 completed in 30.02s\n",
      "    Epoch 28/30 completed in 30.36s\n",
      "    Epoch 29/30 completed in 33.16s\n",
      "    Epoch 30/30 completed in 29.49s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:98.67% R:90.83% T:84.26%  MIA:0.450\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.50% R:90.76% T:84.16%  MIA:0.447\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 29.59s\n",
      "    Epoch 2/10 completed in 30.62s\n",
      "    Epoch 3/10 completed in 29.88s\n",
      "    Epoch 4/10 completed in 30.20s\n",
      "    Epoch 5/10 completed in 30.08s\n",
      "    Epoch 6/10 completed in 29.67s\n",
      "    Epoch 7/10 completed in 29.95s\n",
      "    Epoch 8/10 completed in 30.99s\n",
      "    Epoch 9/10 completed in 30.11s\n",
      "    Epoch 10/10 completed in 30.05s\n",
      "      - Fine-tune  F:98.83% R:90.03% T:82.86%  MIA:0.462\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:98.97% R:90.91% T:83.65%  MIA:0.454\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:98.83% R:85.44% T:81.14%  MIA:0.443\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:99.43% R:89.19% T:83.40%  MIA:0.444\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:98.53% R:87.08% T:81.86%  MIA:0.443\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:5.43% R:10.54% T:9.78%  MIA:0.557\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.99s\n",
      "    Epoch 2/10 completed in 2.13s\n",
      "    Epoch 3/10 completed in 2.04s\n",
      "    Epoch 4/10 completed in 1.93s\n",
      "    Epoch 5/10 completed in 1.92s\n",
      "    Epoch 6/10 completed in 1.92s\n",
      "    Epoch 7/10 completed in 1.90s\n",
      "    Epoch 8/10 completed in 1.92s\n",
      "    Epoch 9/10 completed in 1.90s\n",
      "    Epoch 10/10 completed in 1.92s\n",
      "      - Random-label  F:2.70% R:8.17% T:7.37%  MIA:0.585\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "[TRAINING] retrained model for High ES\n",
      "    Epoch 1/30 completed in 32.77s\n",
      "    Epoch 2/30 completed in 32.29s\n",
      "    Epoch 3/30 completed in 29.97s\n",
      "    Epoch 4/30 completed in 30.15s\n",
      "    Epoch 5/30 completed in 30.33s\n",
      "    Epoch 6/30 completed in 30.20s\n",
      "    Epoch 7/30 completed in 31.67s\n",
      "    Epoch 8/30 completed in 30.07s\n",
      "    Epoch 9/30 completed in 29.85s\n",
      "    Epoch 10/30 completed in 30.37s\n",
      "    Epoch 11/30 completed in 29.84s\n",
      "    Epoch 12/30 completed in 29.61s\n",
      "    Epoch 13/30 completed in 29.60s\n",
      "    Epoch 14/30 completed in 29.95s\n",
      "    Epoch 15/30 completed in 30.19s\n",
      "    Epoch 16/30 completed in 30.04s\n",
      "    Epoch 17/30 completed in 29.64s\n",
      "    Epoch 18/30 completed in 30.07s\n",
      "    Epoch 19/30 completed in 29.57s\n",
      "    Epoch 20/30 completed in 30.19s\n",
      "    Epoch 21/30 completed in 30.82s\n",
      "    Epoch 22/30 completed in 30.22s\n",
      "    Epoch 23/30 completed in 31.25s\n",
      "    Epoch 24/30 completed in 29.94s\n",
      "    Epoch 25/30 completed in 29.37s\n",
      "    Epoch 26/30 completed in 29.62s\n",
      "    Epoch 27/30 completed in 30.00s\n",
      "    Epoch 28/30 completed in 30.38s\n",
      "    Epoch 29/30 completed in 29.57s\n",
      "    Epoch 30/30 completed in 28.12s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:97.70% R:90.21% T:84.33%  MIA:0.464\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:98.83% R:90.69% T:84.16%  MIA:0.458\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 30.32s\n",
      "    Epoch 2/10 completed in 29.90s\n",
      "    Epoch 3/10 completed in 29.71s\n",
      "    Epoch 4/10 completed in 29.80s\n",
      "    Epoch 5/10 completed in 30.07s\n",
      "    Epoch 6/10 completed in 30.07s\n",
      "    Epoch 7/10 completed in 30.18s\n",
      "    Epoch 8/10 completed in 30.91s\n",
      "    Epoch 9/10 completed in 30.44s\n",
      "    Epoch 10/10 completed in 29.91s\n",
      "      - Fine-tune  F:98.47% R:89.80% T:82.86%  MIA:0.458\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:97.87% R:90.36% T:83.06%  MIA:0.457\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:98.37% R:87.04% T:82.31%  MIA:0.453\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:98.63% R:89.56% T:83.70%  MIA:0.442\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:94.23% R:85.54% T:80.89%  MIA:0.455\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:2.13% R:7.70% T:6.79%  MIA:0.547\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 2.09s\n",
      "    Epoch 2/10 completed in 1.92s\n",
      "    Epoch 3/10 completed in 1.91s\n",
      "    Epoch 4/10 completed in 1.97s\n",
      "    Epoch 5/10 completed in 2.13s\n",
      "    Epoch 6/10 completed in 2.16s\n",
      "    Epoch 7/10 completed in 2.17s\n",
      "    Epoch 8/10 completed in 1.95s\n",
      "    Epoch 9/10 completed in 1.92s\n",
      "    Epoch 10/10 completed in 1.88s\n",
      "      - Random-label  F:1.07% R:5.52% T:5.13%  MIA:0.579\n",
      "\n",
      "==================== Starting Run 3/3 ====================\n",
      "\n",
      "[TRAINING] original model\n",
      "    Epoch 1/30 completed in 32.00s\n",
      "    Epoch 2/30 completed in 32.80s\n",
      "    Epoch 3/30 completed in 33.50s\n",
      "    Epoch 4/30 completed in 32.76s\n",
      "    Epoch 5/30 completed in 35.74s\n",
      "    Epoch 6/30 completed in 35.74s\n",
      "    Epoch 7/30 completed in 35.22s\n",
      "    Epoch 8/30 completed in 31.87s\n",
      "    Epoch 9/30 completed in 32.30s\n",
      "    Epoch 10/30 completed in 31.89s\n",
      "    Epoch 11/30 completed in 32.60s\n",
      "    Epoch 12/30 completed in 35.80s\n",
      "    Epoch 13/30 completed in 33.79s\n",
      "    Epoch 14/30 completed in 31.96s\n",
      "    Epoch 15/30 completed in 32.67s\n",
      "    Epoch 16/30 completed in 31.84s\n",
      "    Epoch 17/30 completed in 31.85s\n",
      "    Epoch 18/30 completed in 32.64s\n",
      "    Epoch 19/30 completed in 31.84s\n",
      "    Epoch 20/30 completed in 31.99s\n",
      "    Epoch 21/30 completed in 31.70s\n",
      "    Epoch 22/30 completed in 31.86s\n",
      "    Epoch 23/30 completed in 31.84s\n",
      "    Epoch 24/30 completed in 31.17s\n",
      "    Epoch 25/30 completed in 31.63s\n",
      "    Epoch 26/30 completed in 31.99s\n",
      "    Epoch 27/30 completed in 31.74s\n",
      "    Epoch 28/30 completed in 31.46s\n",
      "    Epoch 29/30 completed in 31.55s\n",
      "    Epoch 30/30 completed in 31.84s\n",
      "\n",
      "Creating ES partitions...\n",
      "      Batch 40/196\n",
      "      Batch 80/196\n",
      "      Batch 120/196\n",
      "      Batch 160/196\n",
      "ES partitions created in 29.96s.\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "[TRAINING] retrained model for Low ES\n",
      "    Epoch 1/30 completed in 29.96s\n",
      "    Epoch 2/30 completed in 29.99s\n",
      "    Epoch 3/30 completed in 29.96s\n",
      "    Epoch 4/30 completed in 30.00s\n",
      "    Epoch 5/30 completed in 30.21s\n",
      "    Epoch 6/30 completed in 29.95s\n",
      "    Epoch 7/30 completed in 30.09s\n",
      "    Epoch 8/30 completed in 30.56s\n",
      "    Epoch 9/30 completed in 30.27s\n",
      "    Epoch 10/30 completed in 33.72s\n",
      "    Epoch 11/30 completed in 33.77s\n",
      "    Epoch 12/30 completed in 30.80s\n",
      "    Epoch 13/30 completed in 31.71s\n",
      "    Epoch 14/30 completed in 29.45s\n",
      "    Epoch 15/30 completed in 29.98s\n",
      "    Epoch 16/30 completed in 30.04s\n",
      "    Epoch 17/30 completed in 30.15s\n",
      "    Epoch 18/30 completed in 29.74s\n",
      "    Epoch 19/30 completed in 29.80s\n",
      "    Epoch 20/30 completed in 30.24s\n",
      "    Epoch 21/30 completed in 29.92s\n",
      "    Epoch 22/30 completed in 30.58s\n",
      "    Epoch 23/30 completed in 29.70s\n",
      "    Epoch 24/30 completed in 30.10s\n",
      "    Epoch 25/30 completed in 33.60s\n",
      "    Epoch 26/30 completed in 32.79s\n",
      "    Epoch 27/30 completed in 29.27s\n",
      "    Epoch 28/30 completed in 30.10s\n",
      "    Epoch 29/30 completed in 29.55s\n",
      "    Epoch 30/30 completed in 29.48s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:99.40% R:90.78% T:84.50%  MIA:0.455\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.67% R:90.61% T:84.53%  MIA:0.445\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 30.26s\n",
      "    Epoch 2/10 completed in 30.18s\n",
      "    Epoch 3/10 completed in 29.87s\n",
      "    Epoch 4/10 completed in 30.01s\n",
      "    Epoch 5/10 completed in 30.16s\n",
      "    Epoch 6/10 completed in 31.40s\n",
      "    Epoch 7/10 completed in 29.56s\n",
      "    Epoch 8/10 completed in 29.68s\n",
      "    Epoch 9/10 completed in 29.96s\n",
      "    Epoch 10/10 completed in 30.31s\n",
      "      - Fine-tune  F:99.50% R:89.75% T:83.23%  MIA:0.449\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:99.30% R:90.08% T:83.43%  MIA:0.452\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:99.10% R:84.92% T:81.28%  MIA:0.450\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:99.53% R:88.78% T:83.26%  MIA:0.449\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:93.00% R:82.81% T:78.63%  MIA:0.446\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:2.70% R:10.12% T:8.35%  MIA:0.644\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.86s\n",
      "    Epoch 2/10 completed in 1.88s\n",
      "    Epoch 3/10 completed in 1.91s\n",
      "    Epoch 4/10 completed in 1.92s\n",
      "    Epoch 5/10 completed in 2.02s\n",
      "    Epoch 6/10 completed in 1.90s\n",
      "    Epoch 7/10 completed in 1.90s\n",
      "    Epoch 8/10 completed in 1.90s\n",
      "    Epoch 9/10 completed in 1.94s\n",
      "    Epoch 10/10 completed in 1.96s\n",
      "      - Random-label  F:2.47% R:11.33% T:9.67%  MIA:0.691\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "[TRAINING] retrained model for Medium ES\n",
      "    Epoch 1/30 completed in 30.12s\n",
      "    Epoch 2/30 completed in 30.18s\n",
      "    Epoch 3/30 completed in 30.01s\n",
      "    Epoch 4/30 completed in 31.54s\n",
      "    Epoch 5/30 completed in 30.50s\n",
      "    Epoch 6/30 completed in 30.48s\n",
      "    Epoch 7/30 completed in 30.55s\n",
      "    Epoch 8/30 completed in 30.00s\n",
      "    Epoch 9/30 completed in 31.16s\n",
      "    Epoch 10/30 completed in 31.10s\n",
      "    Epoch 11/30 completed in 29.56s\n",
      "    Epoch 12/30 completed in 29.72s\n",
      "    Epoch 13/30 completed in 30.06s\n",
      "    Epoch 14/30 completed in 29.67s\n",
      "    Epoch 15/30 completed in 30.15s\n",
      "    Epoch 16/30 completed in 29.57s\n",
      "    Epoch 17/30 completed in 29.61s\n",
      "    Epoch 18/30 completed in 30.35s\n",
      "    Epoch 19/30 completed in 29.96s\n",
      "    Epoch 20/30 completed in 29.74s\n",
      "    Epoch 21/30 completed in 30.28s\n",
      "    Epoch 22/30 completed in 29.93s\n",
      "    Epoch 23/30 completed in 30.32s\n",
      "    Epoch 24/30 completed in 30.56s\n",
      "    Epoch 25/30 completed in 31.57s\n",
      "    Epoch 26/30 completed in 31.16s\n",
      "    Epoch 27/30 completed in 29.85s\n",
      "    Epoch 28/30 completed in 30.30s\n",
      "    Epoch 29/30 completed in 33.77s\n",
      "    Epoch 30/30 completed in 29.99s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:98.97% R:90.78% T:84.09%  MIA:0.455\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.40% R:90.52% T:84.53%  MIA:0.442\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 29.82s\n",
      "    Epoch 2/10 completed in 30.15s\n",
      "    Epoch 3/10 completed in 30.05s\n",
      "    Epoch 4/10 completed in 30.10s\n",
      "    Epoch 5/10 completed in 31.27s\n",
      "    Epoch 6/10 completed in 31.81s\n",
      "    Epoch 7/10 completed in 29.92s\n",
      "    Epoch 8/10 completed in 30.85s\n",
      "    Epoch 9/10 completed in 29.79s\n",
      "    Epoch 10/10 completed in 29.99s\n",
      "      - Fine-tune  F:98.93% R:90.43% T:83.76%  MIA:0.456\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:99.30% R:91.04% T:83.67%  MIA:0.449\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:99.03% R:84.58% T:80.65%  MIA:0.452\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:99.33% R:88.71% T:83.40%  MIA:0.453\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:89.07% R:82.49% T:77.34%  MIA:0.475\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:5.07% R:13.82% T:12.90%  MIA:0.572\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.96s\n",
      "    Epoch 2/10 completed in 1.93s\n",
      "    Epoch 3/10 completed in 1.95s\n",
      "    Epoch 4/10 completed in 1.93s\n",
      "    Epoch 5/10 completed in 1.97s\n",
      "    Epoch 6/10 completed in 2.08s\n",
      "    Epoch 7/10 completed in 2.17s\n",
      "    Epoch 8/10 completed in 2.15s\n",
      "    Epoch 9/10 completed in 2.18s\n",
      "    Epoch 10/10 completed in 2.17s\n",
      "      - Random-label  F:3.97% R:9.93% T:9.60%  MIA:0.578\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "[TRAINING] retrained model for High ES\n",
      "    Epoch 1/30 completed in 29.61s\n",
      "    Epoch 2/30 completed in 30.17s\n",
      "    Epoch 3/30 completed in 30.11s\n",
      "    Epoch 4/30 completed in 30.17s\n",
      "    Epoch 5/30 completed in 30.07s\n",
      "    Epoch 6/30 completed in 30.15s\n",
      "    Epoch 7/30 completed in 30.10s\n",
      "    Epoch 8/30 completed in 30.24s\n",
      "    Epoch 9/30 completed in 29.81s\n",
      "    Epoch 10/30 completed in 30.11s\n",
      "    Epoch 11/30 completed in 29.23s\n",
      "    Epoch 12/30 completed in 29.40s\n",
      "    Epoch 13/30 completed in 30.18s\n",
      "    Epoch 14/30 completed in 32.53s\n",
      "    Epoch 15/30 completed in 31.82s\n",
      "    Epoch 16/30 completed in 29.56s\n",
      "    Epoch 17/30 completed in 29.92s\n",
      "    Epoch 18/30 completed in 30.14s\n",
      "    Epoch 19/30 completed in 30.05s\n",
      "    Epoch 20/30 completed in 29.97s\n",
      "    Epoch 21/30 completed in 29.47s\n",
      "    Epoch 22/30 completed in 29.77s\n",
      "    Epoch 23/30 completed in 29.97s\n",
      "    Epoch 24/30 completed in 30.07s\n",
      "    Epoch 25/30 completed in 30.06s\n",
      "    Epoch 26/30 completed in 30.06s\n",
      "    Epoch 27/30 completed in 29.58s\n",
      "    Epoch 28/30 completed in 29.85s\n",
      "    Epoch 29/30 completed in 29.68s\n",
      "    Epoch 30/30 completed in 29.97s\n",
      "\n",
      "Evaluating retrained model...\n",
      "  Retrain Accs -> F:98.03% R:88.67% T:82.80%  MIA:0.458\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] Original\n",
      "      - Original  F:99.53% R:90.47% T:84.53%  MIA:0.442\n",
      "    > [TRAINING] Fine-tune\n",
      "    Epoch 1/10 completed in 30.18s\n",
      "    Epoch 2/10 completed in 30.14s\n",
      "    Epoch 3/10 completed in 30.15s\n",
      "    Epoch 4/10 completed in 30.71s\n",
      "    Epoch 5/10 completed in 29.89s\n",
      "    Epoch 6/10 completed in 30.10s\n",
      "    Epoch 7/10 completed in 30.70s\n",
      "    Epoch 8/10 completed in 29.68s\n",
      "    Epoch 9/10 completed in 30.85s\n",
      "    Epoch 10/10 completed in 30.34s\n",
      "      - Fine-tune  F:98.43% R:90.26% T:83.74%  MIA:0.447\n",
      "    > [TRAINING] L1-sparse\n",
      "      - L1-sparse  F:98.87% R:90.40% T:83.65%  MIA:0.458\n",
      "    > [TRAINING] NegGrad\n",
      "      - NegGrad  F:98.40% R:86.25% T:81.58%  MIA:0.452\n",
      "    > [TRAINING] NegGrad+\n",
      "      - NegGrad+  F:98.60% R:89.17% T:83.04%  MIA:0.452\n",
      "    > [TRAINING] SCRUB\n",
      "      - SCRUB  F:94.17% R:84.83% T:79.56%  MIA:0.459\n",
      "    > [TRAINING] SalUn\n",
      "      - SalUn  F:2.50% R:9.94% T:9.15%  MIA:0.503\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 1.96s\n",
      "    Epoch 2/10 completed in 1.93s\n",
      "    Epoch 3/10 completed in 1.92s\n",
      "    Epoch 4/10 completed in 1.99s\n",
      "    Epoch 5/10 completed in 1.94s\n",
      "    Epoch 6/10 completed in 2.21s\n",
      "    Epoch 7/10 completed in 2.16s\n",
      "    Epoch 8/10 completed in 2.16s\n",
      "    Epoch 9/10 completed in 2.17s\n",
      "    Epoch 10/10 completed in 2.15s\n",
      "      - Random-label  F:5.93% R:13.24% T:12.63%  MIA:0.516\n",
      "\n",
      "==================== Final Results ====================\n",
      "\n",
      "--- Results for Low ES ---\n",
      "      Method      Forget Acc     Retain Acc       Test Acc           MIA\n",
      "     Retrain  99.011 ± 0.838 90.389 ± 1.603 83.963 ± 1.515 0.451 ± 0.021\n",
      "    Original  99.556 ± 0.408 90.605 ± 0.399 84.210 ± 0.741 0.446 ± 0.005\n",
      "   Fine-tune  99.311 ± 0.425 89.701 ± 0.482 82.920 ± 0.758 0.450 ± 0.007\n",
      "   L1-sparse  99.156 ± 0.485 90.147 ± 0.223 83.187 ± 0.784 0.449 ± 0.008\n",
      "     NegGrad  98.756 ± 1.022 84.661 ± 2.468 80.503 ± 2.841 0.440 ± 0.026\n",
      "    NegGrad+  99.056 ± 1.346 88.458 ± 2.102 82.970 ± 2.024 0.444 ± 0.015\n",
      "       SCRUB 88.244 ± 11.006 81.079 ± 3.747 76.343 ± 5.435 0.461 ± 0.033\n",
      "       SalUn   2.144 ± 2.042  9.334 ± 3.139  8.100 ± 2.071 0.633 ± 0.042\n",
      "Random-label   2.111 ± 3.078  9.982 ± 3.312  8.853 ± 2.498 0.648 ± 0.144\n",
      "\n",
      "--- Results for Medium ES ---\n",
      "      Method      Forget Acc     Retain Acc       Test Acc           MIA\n",
      "     Retrain  98.767 ± 0.430 90.807 ± 0.068 83.997 ± 0.796 0.450 ± 0.011\n",
      "    Original  99.489 ± 0.208 90.572 ± 0.405 84.210 ± 0.741 0.448 ± 0.016\n",
      "   Fine-tune  98.878 ± 0.126 90.126 ± 0.665 83.303 ± 1.118 0.457 ± 0.011\n",
      "   L1-sparse  98.978 ± 0.787 90.411 ± 2.441 83.137 ± 2.252 0.452 ± 0.009\n",
      "     NegGrad  98.767 ± 0.759 85.179 ± 1.286 80.830 ± 0.670 0.442 ± 0.024\n",
      "    NegGrad+  99.067 ± 1.368 88.840 ± 0.750 83.160 ± 1.033 0.444 ± 0.024\n",
      "       SCRUB 90.578 ± 18.179 83.295 ± 8.589 77.943 ± 9.073 0.457 ± 0.041\n",
      "       SalUn   4.767 ± 2.129 11.600 ± 4.780 10.577 ± 5.080 0.561 ± 0.024\n",
      "Random-label   3.411 ± 1.608  8.992 ± 2.195  8.400 ± 2.794 0.583 ± 0.012\n",
      "\n",
      "--- Results for High ES ---\n",
      "      Method      Forget Acc     Retain Acc       Test Acc           MIA\n",
      "     Retrain  97.933 ± 0.504 89.835 ± 2.558 83.843 ± 2.246 0.461 ± 0.007\n",
      "    Original  99.178 ± 0.870 90.578 ± 0.278 84.210 ± 0.741 0.450 ± 0.020\n",
      "   Fine-tune  98.244 ± 0.885 89.997 ± 0.581 83.280 ± 1.096 0.449 ± 0.020\n",
      "   L1-sparse  98.411 ± 1.257 90.229 ± 0.642 83.213 ± 0.953 0.456 ± 0.006\n",
      "     NegGrad  98.178 ± 0.885 86.469 ± 1.232 81.620 ± 1.667 0.446 ± 0.027\n",
      "    NegGrad+  98.367 ± 1.076 89.050 ± 1.444 83.150 ± 1.252 0.444 ± 0.018\n",
      "       SCRUB 90.911 ± 14.151 83.491 ± 7.343 78.100 ± 9.291 0.463 ± 0.027\n",
      "       SalUn   3.100 ± 3.401  8.404 ± 3.310  7.650 ± 3.239 0.528 ± 0.056\n",
      "Random-label   4.556 ± 7.561  9.711 ± 9.697  9.167 ± 9.397 0.544 ± 0.080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import time, copy, itertools, os, random\n",
    "\n",
    "# ===================================================================\n",
    "# 0. 재현성(Reproducibility)을 위한 시드 설정\n",
    "# ===================================================================\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Global random seed set to {SEED}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. 실험 환경 설정\n",
    "# ===================================================================\n",
    "CONFIG = {\n",
    "    \"run_training\": True,\n",
    "    \"model_save_dir\": \"saved_models\",\n",
    "    \"num_runs\": 3,\n",
    "    \"epochs\": 30,\n",
    "    \"unlearn_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 0.1,\n",
    "    \"unlearn_lr\": 0.01,\n",
    "    \"unlearn_lr_neggrad\": 1e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"forget_set_size\": 3000,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"l1_lambda\": 1e-5,\n",
    "    \"neggrad_plus_alpha\": 0.2,\n",
    "    \"salun_sparsity\": 0.5,\n",
    "    \"scrub_alpha\": 0.5,\n",
    "}\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. 모델 및 데이터 헬퍼\n",
    "# ===================================================================\n",
    "def get_model():\n",
    "    return models.resnet18(weights=None, num_classes=10).to(CONFIG[\"device\"])\n",
    "\n",
    "def train_model(model, train_loader, epochs, lr, is_unlearning=False):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(model.parameters(), lr=lr,\n",
    "                    momentum=CONFIG[\"momentum\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "    sched = None if is_unlearning else optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        t0 = time.time()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if sched: sched.step()\n",
    "        print(f\"    Epoch {ep+1}/{epochs} completed in {time.time()-t0:.2f}s\")\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    tot = corr = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n",
    "            pred = model(x).argmax(1)\n",
    "            tot += y.size(0)\n",
    "            corr += (pred == y).sum().item()\n",
    "    return 100 * corr / tot\n",
    "\n",
    "# ===================================================================\n",
    "# 3. ES(Entanglement Score) 분할\n",
    "# ===================================================================\n",
    "def create_es_partitions(original_model, train_dataset, current_seed):\n",
    "    print(\"\\nCreating ES partitions...\")\n",
    "    t0 = time.time()\n",
    "    extractor = nn.Sequential(*list(original_model.children())[:-1]).eval()\n",
    "    emb = []\n",
    "    g = torch.Generator(); g.manual_seed(current_seed)\n",
    "    loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                        worker_init_fn=lambda i: set_seed(current_seed+i), generator=g)\n",
    "    for i, (x, _) in enumerate(loader):\n",
    "        if (i+1) % 40 == 0: print(f\"      Batch {i+1}/{len(loader)}\")\n",
    "        with torch.no_grad():\n",
    "            emb.append(extractor(x.to(CONFIG[\"device\"])).squeeze().cpu())\n",
    "    emb = torch.cat(emb, 0)\n",
    "    dists = ((emb - emb.mean(0))**2).sum(1)\n",
    "    idx = dists.argsort(descending=True).numpy()\n",
    "    fs = CONFIG[\"forget_set_size\"]\n",
    "    parts = {\"Low ES\": idx[:fs], \"Medium ES\": idx[fs:2*fs], \"High ES\": idx[2*fs:3*fs]}\n",
    "    print(f\"ES partitions created in {time.time()-t0:.2f}s.\")\n",
    "    return parts\n",
    "\n",
    "# ===================================================================\n",
    "# 4. 언러닝 알고리즘\n",
    "# ===================================================================\n",
    "class RelabelDataset(Dataset):\n",
    "    def __init__(self, ds, num_classes=10):\n",
    "        self.ds = ds\n",
    "        self.n = num_classes\n",
    "        self.new_labels = [torch.randint(0, self.n, (1,)).item() for _ in range(len(ds))]\n",
    "    def __len__(self): return len(self.ds)\n",
    "    def __getitem__(self, i):\n",
    "        img, lbl = self.ds[i]\n",
    "        nl = self.new_labels[i]\n",
    "        while nl == lbl:\n",
    "            nl = torch.randint(0, self.n, (1,)).item()\n",
    "        return img, nl\n",
    "\n",
    "def unlearn_finetune(orig, retain_loader, cfg):\n",
    "    m = copy.deepcopy(orig)\n",
    "    train_model(m, retain_loader, cfg[\"unlearn_epochs\"], cfg[\"unlearn_lr\"], True)\n",
    "    return m\n",
    "\n",
    "def unlearn_neggrad(orig, forget_loader, cfg):\n",
    "    m = copy.deepcopy(orig); crit = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(m.parameters(), lr=cfg[\"unlearn_lr_neggrad\"]); m.train()\n",
    "    for _ in range(cfg[\"unlearn_epochs\"]):\n",
    "        for x, y in forget_loader:\n",
    "            x, y = x.to(cfg[\"device\"]), y.to(cfg[\"device\"])\n",
    "            opt.zero_grad(); loss = -crit(m(x), y); loss.backward(); opt.step()\n",
    "    return m\n",
    "\n",
    "def unlearn_l1_sparse(orig, retain_loader, cfg):\n",
    "    m = copy.deepcopy(orig); crit = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(m.parameters(), lr=cfg[\"unlearn_lr\"], momentum=CONFIG[\"momentum\"]); m.train()\n",
    "    for _ in range(cfg[\"unlearn_epochs\"]):\n",
    "        for x, y in retain_loader:\n",
    "            x, y = x.to(cfg[\"device\"]), y.to(cfg[\"device\"])\n",
    "            opt.zero_grad()\n",
    "            l1 = sum(p.abs().sum() for p in m.parameters())\n",
    "            loss = crit(m(x), y) + cfg[\"l1_lambda\"] * l1\n",
    "            loss.backward(); opt.step()\n",
    "    return m\n",
    "\n",
    "def unlearn_neggrad_plus(orig, retain_loader, forget_loader, cfg):\n",
    "    m = copy.deepcopy(orig); crit = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(m.parameters(), lr=cfg[\"unlearn_lr\"]); m.train()\n",
    "    for _ in range(cfg[\"unlearn_epochs\"]):\n",
    "        r_iter = iter(itertools.cycle(retain_loader))\n",
    "        for fx, fy in forget_loader:\n",
    "            rx, ry = next(r_iter)\n",
    "            rx, ry = rx.to(cfg[\"device\"]), ry.to(cfg[\"device\"])\n",
    "            fx, fy = fx.to(cfg[\"device\"]), fy.to(cfg[\"device\"])\n",
    "            opt.zero_grad()\n",
    "            loss = crit(m(rx), ry) - cfg[\"neggrad_plus_alpha\"] * crit(m(fx), fy)\n",
    "            loss.backward(); opt.step()\n",
    "    return m\n",
    "\n",
    "def unlearn_scrub(orig, retain_loader, forget_loader, cfg):\n",
    "    m = copy.deepcopy(orig); t_model = copy.deepcopy(orig).eval()\n",
    "    crit = nn.CrossEntropyLoss(); kld = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    opt = optim.SGD(m.parameters(), lr=cfg[\"unlearn_lr\"]); m.train()\n",
    "    for _ in range(cfg[\"unlearn_epochs\"]):\n",
    "        r_iter = iter(itertools.cycle(retain_loader))\n",
    "        for fx, _ in forget_loader:\n",
    "            rx, ry = next(r_iter)\n",
    "            rx, ry, fx = rx.to(cfg[\"device\"]), ry.to(cfg[\"device\"]), fx.to(cfg[\"device\"])\n",
    "            opt.zero_grad()\n",
    "            loss = (1-cfg[\"scrub_alpha\"])*crit(m(rx), ry) \\\n",
    "                   - cfg[\"scrub_alpha\"]*kld(F.log_softmax(m(fx),1),\n",
    "                                             F.softmax(t_model(fx),1))\n",
    "            loss.backward(); opt.step()\n",
    "    return m\n",
    "\n",
    "def unlearn_random_label(orig, forget_set, cfg):\n",
    "    m = copy.deepcopy(orig)\n",
    "    loader = DataLoader(RelabelDataset(forget_set), batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "    train_model(m, loader, cfg[\"unlearn_epochs\"], cfg[\"unlearn_lr\"], True)\n",
    "    return m\n",
    "\n",
    "def unlearn_salun(orig, forget_set, cfg):\n",
    "    m = copy.deepcopy(orig); sal = [torch.zeros_like(p) for p in m.parameters()]\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    f_loader = DataLoader(forget_set, batch_size=cfg[\"batch_size\"])\n",
    "    for x, y in f_loader:\n",
    "        x, y = x.to(cfg[\"device\"]), y.to(cfg[\"device\"])\n",
    "        m.zero_grad(); loss = crit(m(x), y); loss.backward()\n",
    "        for i, p in enumerate(m.parameters()):\n",
    "            if p.grad is not None: sal[i] += p.grad.abs()\n",
    "    flat = torch.cat([s.flatten() for s in sal]); k = int(len(flat)*cfg[\"salun_sparsity\"])\n",
    "    th, _ = torch.kthvalue(flat, k); masks = [(s>th).float() for s in sal]\n",
    "\n",
    "    loader = DataLoader(RelabelDataset(forget_set), batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "    opt = optim.SGD(m.parameters(), lr=cfg[\"unlearn_lr\"], momentum=CONFIG[\"momentum\"]); m.train()\n",
    "    for _ in range(cfg[\"unlearn_epochs\"]):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(cfg[\"device\"]), y.to(cfg[\"device\"])\n",
    "            opt.zero_grad(); loss = crit(m(x), y); loss.backward()\n",
    "            for i, p in enumerate(m.parameters()):\n",
    "                if p.grad is not None: p.grad *= masks[i]\n",
    "            opt.step()\n",
    "    return m\n",
    "\n",
    "# ===================================================================\n",
    "# 5. MIA: black-box benchmarks\n",
    "# ===================================================================\n",
    "class black_box_benchmarks:\n",
    "    def __init__(self, s_tr, s_te, t_tr, t_te, num_classes):\n",
    "        self.k = num_classes\n",
    "        self.s_tr_out, self.s_tr_lab = s_tr\n",
    "        self.s_te_out, self.s_te_lab = s_te\n",
    "        self.t_tr_out, self.t_tr_lab = t_tr\n",
    "        self.t_te_out, self.t_te_lab = t_te\n",
    "\n",
    "        self.s_tr_corr = (self.s_tr_out.argmax(1)==self.s_tr_lab).astype(int)\n",
    "        self.s_te_corr = (self.s_te_out.argmax(1)==self.s_te_lab).astype(int)\n",
    "        self.t_tr_corr = (self.t_tr_out.argmax(1)==self.t_tr_lab).astype(int)\n",
    "        self.t_te_corr = (self.t_te_out.argmax(1)==self.t_te_lab).astype(int)\n",
    "\n",
    "        self.s_tr_conf = self.s_tr_out[np.arange(len(self.s_tr_lab)), self.s_tr_lab]\n",
    "        self.s_te_conf = self.s_te_out[np.arange(len(self.s_te_lab)), self.s_te_lab]\n",
    "        self.t_tr_conf = self.t_tr_out[np.arange(len(self.t_tr_lab)), self.t_tr_lab]\n",
    "        self.t_te_conf = self.t_te_out[np.arange(len(self.t_te_lab)), self.t_te_lab]\n",
    "\n",
    "        self.s_tr_entr = self._entr(self.s_tr_out)\n",
    "        self.s_te_entr = self._entr(self.s_te_out)\n",
    "        self.t_tr_entr = self._entr(self.t_tr_out)\n",
    "        self.t_te_entr = self._entr(self.t_te_out)\n",
    "\n",
    "        self.s_tr_m_entr = self._m_entr(self.s_tr_out, self.s_tr_lab)\n",
    "        self.s_te_m_entr = self._m_entr(self.s_te_out, self.s_te_lab)\n",
    "        self.t_tr_m_entr = self._m_entr(self.t_tr_out, self.t_tr_lab)\n",
    "        self.t_te_m_entr = self._m_entr(self.t_te_out, self.t_te_lab)\n",
    "\n",
    "    def _log(self,p,eps=1e-30): return -np.log(np.maximum(p,eps))\n",
    "    def _entr(self,p): return (p*self._log(p)).sum(1)\n",
    "    def _m_entr(self,p,l):\n",
    "        lp = self._log(p); rp = 1-p; lrp = self._log(rp)\n",
    "        mp = p.copy(); mp[np.arange(l.size),l]=rp[np.arange(l.size),l]\n",
    "        mlp=lrp.copy(); mlp[np.arange(l.size),l]=lp[np.arange(l.size),l]\n",
    "        return (mp*mlp).sum(1)\n",
    "\n",
    "    def _thre(self, tr, te):\n",
    "        vals = np.concatenate((tr,te)); best_acc=0; best_t=0\n",
    "        for v in vals:\n",
    "            acc = 0.5*( (tr>=v).mean() + (te<v).mean() )\n",
    "            if acc>best_acc: best_acc, best_t = acc, v\n",
    "        return best_t\n",
    "\n",
    "    def _via_corr(self):\n",
    "        acc = 0.5*(self.t_tr_corr.mean() + (1-self.t_te_corr).mean())\n",
    "        return acc\n",
    "\n",
    "    def _via_feat(self, tr, te, Ttr, Tte):\n",
    "        t_mem = t_non = 0\n",
    "        for c in range(self.k):\n",
    "            thr = self._thre(tr[self.s_tr_lab==c], te[self.s_te_lab==c])\n",
    "            t_mem  += (Ttr[self.t_tr_lab==c] >= thr).sum()\n",
    "            t_non  += (Tte[self.t_te_lab==c] <  thr).sum()\n",
    "        return 0.5*(t_mem/len(Ttr) + t_non/len(Tte))\n",
    "\n",
    "    def run(self):\n",
    "        return {\n",
    "            \"correctness\": self._via_corr(),\n",
    "            \"confidence\" : self._via_feat(self.s_tr_conf, self.s_te_conf,\n",
    "                                          self.t_tr_conf, self.t_te_conf),\n",
    "            \"entropy\"    : self._via_feat(-self.s_tr_entr, -self.s_te_entr,\n",
    "                                          -self.t_tr_entr, -self.t_te_entr),\n",
    "            \"m_entropy\"  : self._via_feat(-self.s_tr_m_entr, -self.s_te_m_entr,\n",
    "                                          -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "        }\n",
    "\n",
    "def collect_performance(loader, model, device):\n",
    "    outs, labs = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            outs.append(F.softmax(model(x),1).cpu())\n",
    "            labs.append(y.cpu())\n",
    "    return torch.cat(outs).numpy(), torch.cat(labs).numpy()\n",
    "\n",
    "def calculate_mia_score(model, retain_loader_train, retain_loader_test,\n",
    "                        forget_loader, test_loader):\n",
    "    s_tr = collect_performance(retain_loader_train, model, CONFIG[\"device\"])\n",
    "    s_te = collect_performance(test_loader,         model, CONFIG[\"device\"])\n",
    "    t_tr = collect_performance(retain_loader_test,  model, CONFIG[\"device\"])\n",
    "    t_te = collect_performance(forget_loader,       model, CONFIG[\"device\"])\n",
    "\n",
    "    mia = black_box_benchmarks(s_tr, s_te, t_tr, t_te, 10).run()\n",
    "    return mia[\"confidence\"]  # scalar 사용\n",
    "\n",
    "# ===================================================================\n",
    "# 6. 메인 실험 루프\n",
    "# ===================================================================\n",
    "def main():\n",
    "    sd = CONFIG[\"model_save_dir\"]; os.makedirs(sd, exist_ok=True)\n",
    "    tf_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32,4), transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),\n",
    "    ])\n",
    "    tf_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),\n",
    "    ])\n",
    "    DATASET = \"../../data\"\n",
    "    tr_ds = datasets.CIFAR10(root=DATASET,\n",
    "                         train=True,\n",
    "                         download=False,\n",
    "                         transform=tf_train)\n",
    "\n",
    "    te_ds = datasets.CIFAR10(root=DATASET,\n",
    "                            train=False,\n",
    "                            download=False,\n",
    "                            transform=tf_test)\n",
    "\n",
    "    g = torch.Generator(); g.manual_seed(SEED+9999)\n",
    "    te_loader = DataLoader(te_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                           worker_init_fn=lambda i:set_seed(SEED+9999+i), generator=g)\n",
    "\n",
    "    methods = [\"Retrain\",\"Original\",\"Fine-tune\",\"L1-sparse\",\"NegGrad\",\"NegGrad+\",\n",
    "               \"SCRUB\",\"SalUn\",\"Random-label\"]\n",
    "    res = {m:{es:{\"F\":[],\"R\":[],\"T\":[],\"M\":[]} for es in [\"Low ES\",\"Medium ES\",\"High ES\"]} for m in methods}\n",
    "\n",
    "    for run in range(CONFIG[\"num_runs\"]):\n",
    "        print(f\"\\n{'='*20} Starting Run {run+1}/{CONFIG['num_runs']} {'='*20}\")\n",
    "        orig = get_model()\n",
    "        orig_pth = f\"{sd}/run_{run}_original_model.pth\"\n",
    "        part_pth = f\"{sd}/run_{run}_es_partitions.pth\"\n",
    "\n",
    "        if os.path.exists(orig_pth):\n",
    "            print(\"\\n[LOADING] original model\"); orig.load_state_dict(torch.load(orig_pth,map_location=CONFIG[\"device\"]))\n",
    "            parts = torch.load(part_pth)\n",
    "        else:\n",
    "            print(\"\\n[TRAINING] original model\")\n",
    "            gtr = torch.Generator(); gtr.manual_seed(SEED+run)\n",
    "            tr_loader = DataLoader(tr_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True,\n",
    "                                   worker_init_fn=lambda i:set_seed(SEED+run+i), generator=gtr)\n",
    "            train_model(orig, tr_loader, CONFIG[\"epochs\"], CONFIG[\"lr\"])\n",
    "            torch.save(orig.state_dict(), orig_pth)\n",
    "            parts = create_es_partitions(orig, tr_ds, SEED+run); torch.save(parts, part_pth)\n",
    "\n",
    "        for es, forget_idx in parts.items():\n",
    "            print(f\"\\n--- Processing ES Level: {es} ---\")\n",
    "            all_idx = np.arange(len(tr_ds))\n",
    "            retain_idx = np.setdiff1d(all_idx, forget_idx, assume_unique=True)\n",
    "            r_set, f_set = Subset(tr_ds, retain_idx), Subset(tr_ds, forget_idx)\n",
    "\n",
    "            # loaders\n",
    "            g_r = torch.Generator(); g_r.manual_seed(SEED+run+ord(es[0]))\n",
    "            retain_loader = DataLoader(r_set, batch_size=CONFIG[\"batch_size\"], shuffle=True,\n",
    "                                       worker_init_fn=lambda i:set_seed(SEED+run+ord(es[0])+i), generator=g_r)\n",
    "            g_re = torch.Generator(); g_re.manual_seed(SEED+run+ord(es[0])+1000)\n",
    "            retain_eval = DataLoader(r_set, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                     worker_init_fn=lambda i:set_seed(SEED+run+ord(es[0])+1000+i), generator=g_re)\n",
    "            g_f = torch.Generator(); g_f.manual_seed(SEED+run+ord(es[0])+2000)\n",
    "            forget_loader = DataLoader(f_set, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                       worker_init_fn=lambda i:set_seed(SEED+run+ord(es[0])+2000+i), generator=g_f)\n",
    "\n",
    "            # retrain\n",
    "            rt_pth = f\"{sd}/run_{run}_{es.replace(' ','')}_retrained.pth\"\n",
    "            retr = get_model()\n",
    "            if os.path.exists(rt_pth):\n",
    "                print(f\"\\n[LOADING] retrained model for {es}\"); retr.load_state_dict(torch.load(rt_pth,map_location=CONFIG[\"device\"]))\n",
    "            else:\n",
    "                print(f\"\\n[TRAINING] retrained model for {es}\")\n",
    "                train_model(retr, retain_loader, CONFIG[\"epochs\"], CONFIG[\"lr\"])\n",
    "                torch.save(retr.state_dict(), rt_pth)\n",
    "\n",
    "            print(\"\\nEvaluating retrained model...\")\n",
    "            r_acc = evaluate_model(retr, forget_loader)\n",
    "            r_ret = evaluate_model(retr, retain_eval)\n",
    "            r_test = evaluate_model(retr, te_loader)\n",
    "            r_mia = calculate_mia_score(retr, retain_loader, retain_eval, forget_loader, te_loader)\n",
    "            print(f\"  Retrain Accs -> F:{r_acc:.2f}% R:{r_ret:.2f}% T:{r_test:.2f}%  MIA:{r_mia:.3f}\")\n",
    "            res[\"Retrain\"][es][\"F\"].append(r_acc); res[\"Retrain\"][es][\"R\"].append(r_ret)\n",
    "            res[\"Retrain\"][es][\"T\"].append(r_test); res[\"Retrain\"][es][\"M\"].append(r_mia)\n",
    "\n",
    "            unlearn = {\n",
    "                \"Original\"    : lambda: copy.deepcopy(orig),\n",
    "                \"Fine-tune\"   : lambda: unlearn_finetune(orig, retain_loader, CONFIG),\n",
    "                \"L1-sparse\"   : lambda: unlearn_l1_sparse(orig, retain_loader, CONFIG),\n",
    "                \"NegGrad\"     : lambda: unlearn_neggrad(orig, forget_loader, CONFIG),\n",
    "                \"NegGrad+\"    : lambda: unlearn_neggrad_plus(orig, retain_loader, forget_loader, CONFIG),\n",
    "                \"SCRUB\"       : lambda: unlearn_scrub(orig, retain_loader, forget_loader, CONFIG),\n",
    "                \"SalUn\"       : lambda: unlearn_salun(orig, f_set, CONFIG),\n",
    "                \"Random-label\": lambda: unlearn_random_label(orig, f_set, CONFIG),\n",
    "            }\n",
    "\n",
    "            print(\"\\nApplying and evaluating unlearning methods...\")\n",
    "            for m_name, fn in unlearn.items():\n",
    "                upth = f\"{sd}/run_{run}_{es.replace(' ','')}_{m_name}_unlearned.pth\"\n",
    "                if os.path.exists(upth):\n",
    "                    print(f\"    > [LOADING] {m_name}\"); u_model = get_model(); u_model.load_state_dict(torch.load(upth,map_location=CONFIG[\"device\"]))\n",
    "                else:\n",
    "                    print(f\"    > [TRAINING] {m_name}\"); u_model = fn(); torch.save(u_model.state_dict(), upth)\n",
    "\n",
    "                u_f = evaluate_model(u_model, forget_loader)\n",
    "                u_r = evaluate_model(u_model, retain_eval)\n",
    "                u_t = evaluate_model(u_model, te_loader)\n",
    "                u_m = calculate_mia_score(u_model, retain_loader, retain_eval, forget_loader, te_loader)\n",
    "                print(f\"      - {m_name}  F:{u_f:.2f}% R:{u_r:.2f}% T:{u_t:.2f}%  MIA:{u_m:.3f}\")\n",
    "\n",
    "                res[m_name][es][\"F\"].append(u_f); res[m_name][es][\"R\"].append(u_r)\n",
    "                res[m_name][es][\"T\"].append(u_t); res[m_name][es][\"M\"].append(u_m)\n",
    "\n",
    "    # ===================================================================\n",
    "    # 7. 결과 정리\n",
    "    # ===================================================================\n",
    "    print(f\"\\n{'='*20} Final Results {'='*20}\")\n",
    "    def fmt(xs):\n",
    "        xs=np.array(xs); mu=xs.mean()\n",
    "        return f\"{mu:.3f}\" if len(xs)<2 else f\"{mu:.3f} ± {(stats.sem(xs)*stats.t.ppf(0.975,len(xs)-1)):.3f}\"\n",
    "    for es in [\"Low ES\",\"Medium ES\",\"High ES\"]:\n",
    "        print(f\"\\n--- Results for {es} ---\")\n",
    "        rows=[]\n",
    "        for m in methods:\n",
    "            row={\"Method\":m,\n",
    "                 \"Forget Acc\":fmt(res[m][es][\"F\"]),\n",
    "                 \"Retain Acc\":fmt(res[m][es][\"R\"]),\n",
    "                 \"Test Acc\"  :fmt(res[m][es][\"T\"]),\n",
    "                 \"MIA\"       :fmt(res[m][es][\"M\"])}\n",
    "            rows.append(row)\n",
    "        print(pd.DataFrame(rows).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
