{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a599f8b",
   "metadata": {},
   "source": [
    "\"What makes unlearning hard and what to do about it\" (Zhao et al., NeurIPS 2024) 논문의 Table 5a와 유사한 결과를 재현하는 코드임\n",
    "\n",
    "이 코드는 논문에 기술된 실험 절차, 특히 ES(Entanglement Score)에 따른 데이터 분할, 각종 언러닝(Unlearning) 방법 적용, 그리고 ToW 및 예측 차이율 계산 과정을 모두 포함하고 있음\n",
    "\n",
    "L1-sparse, SalUn 등 일부 복잡한 언러닝 알고리즘은 논문의 핵심 아이디어를 바탕으로 단순화하여 구현함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "743d3ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 20 15:55:37 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              34W / 250W |  12422MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              30W / 250W |   6302MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   44C    P0              34W / 250W |   8370MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   54C    P0              84W / 250W |   9384MiB / 16384MiB |     72%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              32W / 250W |  13916MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   44C    P0              41W / 250W |   9074MiB / 16384MiB |     17%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              31W / 250W |   5006MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              34W / 250W |  14760MiB / 16384MiB |     27%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b4493da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device (GPU if available)\n",
    "DEVICE_NUM = 6\n",
    "ADDITIONAL_GPU = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}:{DEVICE_NUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea189425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "==================== Starting Run 1/3 ====================\n",
      "\n",
      "[TRAINING MODE] Step 1/5: Training original model...\n",
      "    Epoch 1/30 completed in 31.16s\n",
      "    Epoch 2/30 completed in 29.40s\n",
      "    Epoch 3/30 completed in 28.80s\n",
      "    Epoch 4/30 completed in 30.71s\n",
      "    Epoch 5/30 completed in 29.65s\n",
      "    Epoch 6/30 completed in 29.20s\n",
      "    Epoch 7/30 completed in 32.04s\n",
      "    Epoch 8/30 completed in 31.04s\n",
      "    Epoch 9/30 completed in 29.13s\n",
      "    Epoch 10/30 completed in 28.91s\n",
      "    Epoch 11/30 completed in 28.80s\n",
      "    Epoch 12/30 completed in 28.95s\n",
      "    Epoch 13/30 completed in 28.69s\n",
      "    Epoch 14/30 completed in 28.41s\n",
      "    Epoch 15/30 completed in 28.46s\n",
      "    Epoch 16/30 completed in 28.63s\n",
      "    Epoch 17/30 completed in 29.01s\n",
      "    Epoch 18/30 completed in 29.20s\n",
      "    Epoch 19/30 completed in 29.31s\n",
      "    Epoch 20/30 completed in 29.30s\n",
      "    Epoch 21/30 completed in 29.14s\n",
      "    Epoch 22/30 completed in 29.19s\n",
      "    Epoch 23/30 completed in 28.81s\n",
      "    Epoch 24/30 completed in 29.07s\n",
      "    Epoch 25/30 completed in 29.16s\n",
      "    Epoch 26/30 completed in 29.16s\n",
      "    Epoch 27/30 completed in 29.06s\n",
      "    Epoch 28/30 completed in 28.86s\n",
      "    Epoch 29/30 completed in 28.82s\n",
      "    Epoch 30/30 completed in 28.67s\n",
      "Original model trained and saved in 878.88s\n",
      "Creating ES partitions...\n",
      "  Extracting embeddings from original model...\n",
      "    Batch 40/196\n",
      "    Batch 80/196\n",
      "    Batch 120/196\n",
      "    Batch 160/196\n",
      "ES partitions created in 26.89s.\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for Low ES...\n",
      "    Epoch 1/30 completed in 27.58s\n",
      "    Epoch 2/30 completed in 26.95s\n",
      "    Epoch 3/30 completed in 27.07s\n",
      "    Epoch 4/30 completed in 26.90s\n",
      "    Epoch 5/30 completed in 27.18s\n",
      "    Epoch 6/30 completed in 27.55s\n",
      "    Epoch 7/30 completed in 27.64s\n",
      "    Epoch 8/30 completed in 27.51s\n",
      "    Epoch 9/30 completed in 27.45s\n",
      "    Epoch 10/30 completed in 27.26s\n",
      "    Epoch 11/30 completed in 27.54s\n",
      "    Epoch 12/30 completed in 27.38s\n",
      "    Epoch 13/30 completed in 27.46s\n",
      "    Epoch 14/30 completed in 27.14s\n",
      "    Epoch 15/30 completed in 27.15s\n",
      "    Epoch 16/30 completed in 27.49s\n",
      "    Epoch 17/30 completed in 27.72s\n",
      "    Epoch 18/30 completed in 31.07s\n",
      "    Epoch 19/30 completed in 29.19s\n",
      "    Epoch 20/30 completed in 27.43s\n",
      "    Epoch 21/30 completed in 27.47s\n",
      "    Epoch 22/30 completed in 27.37s\n",
      "    Epoch 23/30 completed in 27.02s\n",
      "    Epoch 24/30 completed in 27.19s\n",
      "    Epoch 25/30 completed in 27.42s\n",
      "    Epoch 26/30 completed in 27.07s\n",
      "    Epoch 27/30 completed in 27.26s\n",
      "    Epoch 28/30 completed in 29.69s\n",
      "    Epoch 29/30 completed in 27.59s\n",
      "    Epoch 30/30 completed in 27.59s\n",
      "Retrained model trained and saved in 828.44s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.26%, Forget: 99.23%, Test: 83.88%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 91.22%, Forget: 99.63%, Test: 84.75%\n",
      "    - Scores -> ToW: 0.978, Pred Diff: 8.186%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.52s\n",
      "    Epoch 2/10 completed in 27.48s\n",
      "    Epoch 3/10 completed in 27.39s\n",
      "    Epoch 4/10 completed in 27.46s\n",
      "    Epoch 5/10 completed in 27.51s\n",
      "    Epoch 6/10 completed in 27.44s\n",
      "    Epoch 7/10 completed in 29.85s\n",
      "    Epoch 8/10 completed in 31.07s\n",
      "    Epoch 9/10 completed in 30.85s\n",
      "    Epoch 10/10 completed in 29.31s\n",
      "    Method Fine-tune applied and saved in 286.01s\n",
      "    - Unlearned Accs -> Retain: 90.62%, Forget: 99.37%, Test: 83.71%\n",
      "    - Scores -> ToW: 0.993, Pred Diff: 9.426%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 291.08s\n",
      "    - Unlearned Accs -> Retain: 90.57%, Forget: 99.33%, Test: 83.47%\n",
      "    - Scores -> ToW: 0.992, Pred Diff: 9.302%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.46s\n",
      "    - Unlearned Accs -> Retain: 85.34%, Forget: 98.83%, Test: 80.29%\n",
      "    - Scores -> ToW: 0.913, Pred Diff: 12.330%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 34.43s\n",
      "    - Unlearned Accs -> Retain: 88.56%, Forget: 98.67%, Test: 82.79%\n",
      "    - Scores -> ToW: 0.967, Pred Diff: 9.930%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 19.67s\n",
      "    - Unlearned Accs -> Retain: 7.85%, Forget: 2.00%, Test: 6.68%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 92.944%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.77s\n",
      "    Epoch 2/10 completed in 1.76s\n",
      "    Epoch 3/10 completed in 1.76s\n",
      "    Epoch 4/10 completed in 1.75s\n",
      "    Epoch 5/10 completed in 1.76s\n",
      "    Epoch 6/10 completed in 1.73s\n",
      "    Epoch 7/10 completed in 1.75s\n",
      "    Epoch 8/10 completed in 1.73s\n",
      "    Epoch 9/10 completed in 1.74s\n",
      "    Epoch 10/10 completed in 1.78s\n",
      "    Method Random-label applied and saved in 17.70s\n",
      "    - Unlearned Accs -> Retain: 11.37%, Forget: 4.83%, Test: 11.09%\n",
      "    - Scores -> ToW: 0.003, Pred Diff: 88.766%\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for Medium ES...\n",
      "    Epoch 1/30 completed in 27.16s\n",
      "    Epoch 2/30 completed in 27.22s\n",
      "    Epoch 3/30 completed in 27.02s\n",
      "    Epoch 4/30 completed in 27.48s\n",
      "    Epoch 5/30 completed in 27.58s\n",
      "    Epoch 6/30 completed in 27.65s\n",
      "    Epoch 7/30 completed in 27.51s\n",
      "    Epoch 8/30 completed in 27.69s\n",
      "    Epoch 9/30 completed in 27.49s\n",
      "    Epoch 10/30 completed in 27.30s\n",
      "    Epoch 11/30 completed in 27.39s\n",
      "    Epoch 12/30 completed in 27.42s\n",
      "    Epoch 13/30 completed in 27.42s\n",
      "    Epoch 14/30 completed in 27.29s\n",
      "    Epoch 15/30 completed in 27.37s\n",
      "    Epoch 16/30 completed in 27.39s\n",
      "    Epoch 17/30 completed in 27.37s\n",
      "    Epoch 18/30 completed in 27.13s\n",
      "    Epoch 19/30 completed in 27.54s\n",
      "    Epoch 20/30 completed in 27.43s\n",
      "    Epoch 21/30 completed in 27.78s\n",
      "    Epoch 22/30 completed in 27.61s\n",
      "    Epoch 23/30 completed in 27.56s\n",
      "    Epoch 24/30 completed in 27.22s\n",
      "    Epoch 25/30 completed in 27.21s\n",
      "    Epoch 26/30 completed in 27.21s\n",
      "    Epoch 27/30 completed in 27.51s\n",
      "    Epoch 28/30 completed in 27.65s\n",
      "    Epoch 29/30 completed in 27.56s\n",
      "    Epoch 30/30 completed in 27.73s\n",
      "Retrained model trained and saved in 823.00s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.21%, Forget: 98.73%, Test: 84.49%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 91.30%, Forget: 99.47%, Test: 84.75%\n",
      "    - Scores -> ToW: 0.979, Pred Diff: 7.982%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.89s\n",
      "    Epoch 2/10 completed in 27.83s\n",
      "    Epoch 3/10 completed in 27.64s\n",
      "    Epoch 4/10 completed in 27.40s\n",
      "    Epoch 5/10 completed in 27.51s\n",
      "    Epoch 6/10 completed in 27.12s\n",
      "    Epoch 7/10 completed in 27.13s\n",
      "    Epoch 8/10 completed in 27.05s\n",
      "    Epoch 9/10 completed in 27.58s\n",
      "    Epoch 10/10 completed in 27.18s\n",
      "    Method Fine-tune applied and saved in 274.47s\n",
      "    - Unlearned Accs -> Retain: 90.29%, Forget: 98.87%, Test: 83.08%\n",
      "    - Scores -> ToW: 0.984, Pred Diff: 9.366%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 286.04s\n",
      "    - Unlearned Accs -> Retain: 91.27%, Forget: 99.13%, Test: 83.90%\n",
      "    - Scores -> ToW: 0.980, Pred Diff: 8.984%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 19.61s\n",
      "    - Unlearned Accs -> Retain: 86.50%, Forget: 98.97%, Test: 81.27%\n",
      "    - Scores -> ToW: 0.930, Pred Diff: 11.328%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 37.20s\n",
      "    - Unlearned Accs -> Retain: 89.47%, Forget: 98.83%, Test: 82.87%\n",
      "    - Scores -> ToW: 0.976, Pred Diff: 9.256%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 19.63s\n",
      "    - Unlearned Accs -> Retain: 10.89%, Forget: 2.40%, Test: 10.29%\n",
      "    - Scores -> ToW: 0.002, Pred Diff: 89.684%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.79s\n",
      "    Epoch 2/10 completed in 1.77s\n",
      "    Epoch 3/10 completed in 1.75s\n",
      "    Epoch 4/10 completed in 1.76s\n",
      "    Epoch 5/10 completed in 1.80s\n",
      "    Epoch 6/10 completed in 1.76s\n",
      "    Epoch 7/10 completed in 1.76s\n",
      "    Epoch 8/10 completed in 1.76s\n",
      "    Epoch 9/10 completed in 1.81s\n",
      "    Epoch 10/10 completed in 1.77s\n",
      "    Method Random-label applied and saved in 17.90s\n",
      "    - Unlearned Accs -> Retain: 8.10%, Forget: 3.07%, Test: 7.56%\n",
      "    - Scores -> ToW: 0.002, Pred Diff: 91.674%\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for High ES...\n",
      "    Epoch 1/30 completed in 27.74s\n",
      "    Epoch 2/30 completed in 27.52s\n",
      "    Epoch 3/30 completed in 27.41s\n",
      "    Epoch 4/30 completed in 27.59s\n",
      "    Epoch 5/30 completed in 29.44s\n",
      "    Epoch 6/30 completed in 28.44s\n",
      "    Epoch 7/30 completed in 27.50s\n",
      "    Epoch 8/30 completed in 27.68s\n",
      "    Epoch 9/30 completed in 27.30s\n",
      "    Epoch 10/30 completed in 27.15s\n",
      "    Epoch 11/30 completed in 26.84s\n",
      "    Epoch 12/30 completed in 28.74s\n",
      "    Epoch 13/30 completed in 27.84s\n",
      "    Epoch 14/30 completed in 27.08s\n",
      "    Epoch 15/30 completed in 26.91s\n",
      "    Epoch 16/30 completed in 27.01s\n",
      "    Epoch 17/30 completed in 27.27s\n",
      "    Epoch 18/30 completed in 26.68s\n",
      "    Epoch 19/30 completed in 28.17s\n",
      "    Epoch 20/30 completed in 30.92s\n",
      "    Epoch 21/30 completed in 30.98s\n",
      "    Epoch 22/30 completed in 30.96s\n",
      "    Epoch 23/30 completed in 28.34s\n",
      "    Epoch 24/30 completed in 26.85s\n",
      "    Epoch 25/30 completed in 27.01s\n",
      "    Epoch 26/30 completed in 27.16s\n",
      "    Epoch 27/30 completed in 27.34s\n",
      "    Epoch 28/30 completed in 26.90s\n",
      "    Epoch 29/30 completed in 27.07s\n",
      "    Epoch 30/30 completed in 27.04s\n",
      "Retrained model trained and saved in 835.02s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.51%, Forget: 98.13%, Test: 84.18%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 91.36%, Forget: 99.30%, Test: 84.75%\n",
      "    - Scores -> ToW: 0.974, Pred Diff: 7.964%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.70s\n",
      "    Epoch 2/10 completed in 28.68s\n",
      "    Epoch 3/10 completed in 27.08s\n",
      "    Epoch 4/10 completed in 27.07s\n",
      "    Epoch 5/10 completed in 27.22s\n",
      "    Epoch 6/10 completed in 27.52s\n",
      "    Epoch 7/10 completed in 30.74s\n",
      "    Epoch 8/10 completed in 28.03s\n",
      "    Epoch 9/10 completed in 27.39s\n",
      "    Epoch 10/10 completed in 27.39s\n",
      "    Method Fine-tune applied and saved in 278.94s\n",
      "    - Unlearned Accs -> Retain: 90.55%, Forget: 98.53%, Test: 83.51%\n",
      "    - Scores -> ToW: 0.989, Pred Diff: 8.948%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 292.28s\n",
      "    - Unlearned Accs -> Retain: 91.06%, Forget: 98.53%, Test: 83.65%\n",
      "    - Scores -> ToW: 0.985, Pred Diff: 9.046%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.31s\n",
      "    - Unlearned Accs -> Retain: 87.24%, Forget: 99.03%, Test: 81.38%\n",
      "    - Scores -> ToW: 0.932, Pred Diff: 10.946%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 34.12s\n",
      "    - Unlearned Accs -> Retain: 90.06%, Forget: 99.00%, Test: 83.34%\n",
      "    - Scores -> ToW: 0.979, Pred Diff: 8.886%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 19.51s\n",
      "    - Unlearned Accs -> Retain: 9.40%, Forget: 5.00%, Test: 9.57%\n",
      "    - Scores -> ToW: 0.003, Pred Diff: 90.126%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.75s\n",
      "    Epoch 2/10 completed in 1.76s\n",
      "    Epoch 3/10 completed in 1.75s\n",
      "    Epoch 4/10 completed in 1.74s\n",
      "    Epoch 5/10 completed in 1.76s\n",
      "    Epoch 6/10 completed in 1.77s\n",
      "    Epoch 7/10 completed in 1.75s\n",
      "    Epoch 8/10 completed in 1.75s\n",
      "    Epoch 9/10 completed in 1.75s\n",
      "    Epoch 10/10 completed in 1.73s\n",
      "    Method Random-label applied and saved in 17.68s\n",
      "    - Unlearned Accs -> Retain: 10.83%, Forget: 3.83%, Test: 10.63%\n",
      "    - Scores -> ToW: 0.003, Pred Diff: 89.142%\n",
      "\n",
      "==================== Starting Run 2/3 ====================\n",
      "\n",
      "[TRAINING MODE] Step 1/5: Training original model...\n",
      "    Epoch 1/30 completed in 28.55s\n",
      "    Epoch 2/30 completed in 29.63s\n",
      "    Epoch 3/30 completed in 29.08s\n",
      "    Epoch 4/30 completed in 29.09s\n",
      "    Epoch 5/30 completed in 29.08s\n",
      "    Epoch 6/30 completed in 30.56s\n",
      "    Epoch 7/30 completed in 29.25s\n",
      "    Epoch 8/30 completed in 29.29s\n",
      "    Epoch 9/30 completed in 29.06s\n",
      "    Epoch 10/30 completed in 29.09s\n",
      "    Epoch 11/30 completed in 29.12s\n",
      "    Epoch 12/30 completed in 29.34s\n",
      "    Epoch 13/30 completed in 29.27s\n",
      "    Epoch 14/30 completed in 29.13s\n",
      "    Epoch 15/30 completed in 29.33s\n",
      "    Epoch 16/30 completed in 29.32s\n",
      "    Epoch 17/30 completed in 29.20s\n",
      "    Epoch 18/30 completed in 28.91s\n",
      "    Epoch 19/30 completed in 28.84s\n",
      "    Epoch 20/30 completed in 28.96s\n",
      "    Epoch 21/30 completed in 29.16s\n",
      "    Epoch 22/30 completed in 29.17s\n",
      "    Epoch 23/30 completed in 29.02s\n",
      "    Epoch 24/30 completed in 28.96s\n",
      "    Epoch 25/30 completed in 28.84s\n",
      "    Epoch 26/30 completed in 29.20s\n",
      "    Epoch 27/30 completed in 29.31s\n",
      "    Epoch 28/30 completed in 31.67s\n",
      "    Epoch 29/30 completed in 29.01s\n",
      "    Epoch 30/30 completed in 28.92s\n",
      "Original model trained and saved in 877.43s\n",
      "Creating ES partitions...\n",
      "  Extracting embeddings from original model...\n",
      "    Batch 40/196\n",
      "    Batch 80/196\n",
      "    Batch 120/196\n",
      "    Batch 160/196\n",
      "ES partitions created in 26.88s.\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for Low ES...\n",
      "    Epoch 1/30 completed in 27.33s\n",
      "    Epoch 2/30 completed in 27.66s\n",
      "    Epoch 3/30 completed in 28.24s\n",
      "    Epoch 4/30 completed in 28.09s\n",
      "    Epoch 5/30 completed in 27.17s\n",
      "    Epoch 6/30 completed in 26.99s\n",
      "    Epoch 7/30 completed in 27.12s\n",
      "    Epoch 8/30 completed in 27.42s\n",
      "    Epoch 9/30 completed in 27.66s\n",
      "    Epoch 10/30 completed in 27.66s\n",
      "    Epoch 11/30 completed in 27.60s\n",
      "    Epoch 12/30 completed in 27.58s\n",
      "    Epoch 13/30 completed in 27.67s\n",
      "    Epoch 14/30 completed in 27.55s\n",
      "    Epoch 15/30 completed in 27.14s\n",
      "    Epoch 16/30 completed in 27.14s\n",
      "    Epoch 17/30 completed in 30.57s\n",
      "    Epoch 18/30 completed in 27.74s\n",
      "    Epoch 19/30 completed in 27.75s\n",
      "    Epoch 20/30 completed in 29.66s\n",
      "    Epoch 21/30 completed in 30.92s\n",
      "    Epoch 22/30 completed in 27.55s\n",
      "    Epoch 23/30 completed in 27.45s\n",
      "    Epoch 24/30 completed in 27.48s\n",
      "    Epoch 25/30 completed in 27.69s\n",
      "    Epoch 26/30 completed in 27.65s\n",
      "    Epoch 27/30 completed in 27.63s\n",
      "    Epoch 28/30 completed in 27.26s\n",
      "    Epoch 29/30 completed in 27.00s\n",
      "    Epoch 30/30 completed in 27.49s\n",
      "Retrained model trained and saved in 834.01s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 89.93%, Forget: 98.93%, Test: 84.11%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 90.88%, Forget: 99.57%, Test: 84.33%\n",
      "    - Scores -> ToW: 0.982, Pred Diff: 8.498%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.20s\n",
      "    Epoch 2/10 completed in 27.17s\n",
      "    Epoch 3/10 completed in 27.41s\n",
      "    Epoch 4/10 completed in 27.60s\n",
      "    Epoch 5/10 completed in 27.27s\n",
      "    Epoch 6/10 completed in 27.44s\n",
      "    Epoch 7/10 completed in 27.60s\n",
      "    Epoch 8/10 completed in 27.88s\n",
      "    Epoch 9/10 completed in 27.61s\n",
      "    Epoch 10/10 completed in 27.48s\n",
      "    Method Fine-tune applied and saved in 274.81s\n",
      "    - Unlearned Accs -> Retain: 90.12%, Forget: 99.20%, Test: 83.04%\n",
      "    - Scores -> ToW: 0.985, Pred Diff: 9.768%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 287.92s\n",
      "    - Unlearned Accs -> Retain: 90.46%, Forget: 99.07%, Test: 83.22%\n",
      "    - Scores -> ToW: 0.984, Pred Diff: 9.764%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.13s\n",
      "    - Unlearned Accs -> Retain: 77.19%, Forget: 97.70%, Test: 74.24%\n",
      "    - Scores -> ToW: 0.777, Pred Diff: 19.700%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 33.75s\n",
      "    - Unlearned Accs -> Retain: 83.41%, Forget: 95.17%, Test: 78.94%\n",
      "    - Scores -> ToW: 0.853, Pred Diff: 14.836%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 19.30s\n",
      "    - Unlearned Accs -> Retain: 15.14%, Forget: 0.70%, Test: 12.78%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 86.604%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.77s\n",
      "    Epoch 2/10 completed in 1.75s\n",
      "    Epoch 3/10 completed in 1.73s\n",
      "    Epoch 4/10 completed in 1.75s\n",
      "    Epoch 5/10 completed in 1.79s\n",
      "    Epoch 6/10 completed in 1.74s\n",
      "    Epoch 7/10 completed in 1.74s\n",
      "    Epoch 8/10 completed in 1.72s\n",
      "    Epoch 9/10 completed in 1.77s\n",
      "    Epoch 10/10 completed in 1.73s\n",
      "    Method Random-label applied and saved in 17.66s\n",
      "    - Unlearned Accs -> Retain: 12.81%, Forget: 0.77%, Test: 10.02%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 89.350%\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for Medium ES...\n",
      "    Epoch 1/30 completed in 27.29s\n",
      "    Epoch 2/30 completed in 27.59s\n",
      "    Epoch 3/30 completed in 27.44s\n",
      "    Epoch 4/30 completed in 27.27s\n",
      "    Epoch 5/30 completed in 26.90s\n",
      "    Epoch 6/30 completed in 27.00s\n",
      "    Epoch 7/30 completed in 27.34s\n",
      "    Epoch 8/30 completed in 27.46s\n",
      "    Epoch 9/30 completed in 27.26s\n",
      "    Epoch 10/30 completed in 27.22s\n",
      "    Epoch 11/30 completed in 27.18s\n",
      "    Epoch 12/30 completed in 27.11s\n",
      "    Epoch 13/30 completed in 27.57s\n",
      "    Epoch 14/30 completed in 27.81s\n",
      "    Epoch 15/30 completed in 27.49s\n",
      "    Epoch 16/30 completed in 27.67s\n",
      "    Epoch 17/30 completed in 27.71s\n",
      "    Epoch 18/30 completed in 27.69s\n",
      "    Epoch 19/30 completed in 27.57s\n",
      "    Epoch 20/30 completed in 27.24s\n",
      "    Epoch 21/30 completed in 27.22s\n",
      "    Epoch 22/30 completed in 27.53s\n",
      "    Epoch 23/30 completed in 27.40s\n",
      "    Epoch 24/30 completed in 27.57s\n",
      "    Epoch 25/30 completed in 28.64s\n",
      "    Epoch 26/30 completed in 28.36s\n",
      "    Epoch 27/30 completed in 27.48s\n",
      "    Epoch 28/30 completed in 27.43s\n",
      "    Epoch 29/30 completed in 27.47s\n",
      "    Epoch 30/30 completed in 27.40s\n",
      "Retrained model trained and saved in 824.45s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.70%, Forget: 99.00%, Test: 83.96%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 90.68%, Forget: 99.60%, Test: 84.33%\n",
      "    - Scores -> ToW: 0.990, Pred Diff: 8.100%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 28.01s\n",
      "    Epoch 2/10 completed in 27.44s\n",
      "    Epoch 3/10 completed in 28.03s\n",
      "    Epoch 4/10 completed in 27.65s\n",
      "    Epoch 5/10 completed in 27.20s\n",
      "    Epoch 6/10 completed in 27.44s\n",
      "    Epoch 7/10 completed in 27.34s\n",
      "    Epoch 8/10 completed in 27.44s\n",
      "    Epoch 9/10 completed in 27.09s\n",
      "    Epoch 10/10 completed in 27.55s\n",
      "    Method Fine-tune applied and saved in 275.34s\n",
      "    - Unlearned Accs -> Retain: 90.36%, Forget: 99.20%, Test: 82.92%\n",
      "    - Scores -> ToW: 0.984, Pred Diff: 9.128%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 286.81s\n",
      "    - Unlearned Accs -> Retain: 90.43%, Forget: 99.27%, Test: 82.73%\n",
      "    - Scores -> ToW: 0.982, Pred Diff: 9.318%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.46s\n",
      "    - Unlearned Accs -> Retain: 83.15%, Forget: 98.73%, Test: 78.36%\n",
      "    - Scores -> ToW: 0.870, Pred Diff: 14.674%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 34.48s\n",
      "    - Unlearned Accs -> Retain: 87.84%, Forget: 98.93%, Test: 81.81%\n",
      "    - Scores -> ToW: 0.950, Pred Diff: 10.816%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 19.76s\n",
      "    - Unlearned Accs -> Retain: 10.59%, Forget: 1.37%, Test: 9.22%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 90.630%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.77s\n",
      "    Epoch 2/10 completed in 1.82s\n",
      "    Epoch 3/10 completed in 1.76s\n",
      "    Epoch 4/10 completed in 1.75s\n",
      "    Epoch 5/10 completed in 1.78s\n",
      "    Epoch 6/10 completed in 1.78s\n",
      "    Epoch 7/10 completed in 1.77s\n",
      "    Epoch 8/10 completed in 1.82s\n",
      "    Epoch 9/10 completed in 1.78s\n",
      "    Epoch 10/10 completed in 1.76s\n",
      "    Method Random-label applied and saved in 17.96s\n",
      "    - Unlearned Accs -> Retain: 7.86%, Forget: 0.43%, Test: 5.98%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 93.488%\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for High ES...\n",
      "    Epoch 1/30 completed in 27.40s\n",
      "    Epoch 2/30 completed in 27.30s\n",
      "    Epoch 3/30 completed in 27.32s\n",
      "    Epoch 4/30 completed in 27.20s\n",
      "    Epoch 5/30 completed in 26.96s\n",
      "    Epoch 6/30 completed in 27.18s\n",
      "    Epoch 7/30 completed in 28.44s\n",
      "    Epoch 8/30 completed in 27.60s\n",
      "    Epoch 9/30 completed in 29.78s\n",
      "    Epoch 10/30 completed in 30.52s\n",
      "    Epoch 11/30 completed in 27.26s\n",
      "    Epoch 12/30 completed in 27.49s\n",
      "    Epoch 13/30 completed in 27.63s\n",
      "    Epoch 14/30 completed in 27.59s\n",
      "    Epoch 15/30 completed in 27.84s\n",
      "    Epoch 16/30 completed in 27.62s\n",
      "    Epoch 17/30 completed in 27.50s\n",
      "    Epoch 18/30 completed in 28.03s\n",
      "    Epoch 19/30 completed in 27.59s\n",
      "    Epoch 20/30 completed in 27.48s\n",
      "    Epoch 21/30 completed in 27.72s\n",
      "    Epoch 22/30 completed in 27.62s\n",
      "    Epoch 23/30 completed in 30.39s\n",
      "    Epoch 24/30 completed in 27.74s\n",
      "    Epoch 25/30 completed in 27.76s\n",
      "    Epoch 26/30 completed in 27.70s\n",
      "    Epoch 27/30 completed in 27.59s\n",
      "    Epoch 28/30 completed in 27.77s\n",
      "    Epoch 29/30 completed in 27.17s\n",
      "    Epoch 30/30 completed in 27.30s\n",
      "Retrained model trained and saved in 834.65s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.29%, Forget: 97.93%, Test: 83.79%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 90.97%, Forget: 99.40%, Test: 84.33%\n",
      "    - Scores -> ToW: 0.973, Pred Diff: 8.026%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.73s\n",
      "    Epoch 2/10 completed in 27.65s\n",
      "    Epoch 3/10 completed in 27.58s\n",
      "    Epoch 4/10 completed in 27.58s\n",
      "    Epoch 5/10 completed in 28.88s\n",
      "    Epoch 6/10 completed in 27.84s\n",
      "    Epoch 7/10 completed in 27.54s\n",
      "    Epoch 8/10 completed in 27.56s\n",
      "    Epoch 9/10 completed in 27.44s\n",
      "    Epoch 10/10 completed in 27.81s\n",
      "    Method Fine-tune applied and saved in 277.75s\n",
      "    - Unlearned Accs -> Retain: 90.01%, Forget: 98.53%, Test: 82.82%\n",
      "    - Scores -> ToW: 0.982, Pred Diff: 9.412%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 290.44s\n",
      "    - Unlearned Accs -> Retain: 91.04%, Forget: 98.43%, Test: 83.56%\n",
      "    - Scores -> ToW: 0.985, Pred Diff: 9.068%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.41s\n",
      "    - Unlearned Accs -> Retain: 86.21%, Forget: 98.43%, Test: 80.74%\n",
      "    - Scores -> ToW: 0.925, Pred Diff: 11.822%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 34.03s\n",
      "    - Unlearned Accs -> Retain: 88.91%, Forget: 98.87%, Test: 82.86%\n",
      "    - Scores -> ToW: 0.968, Pred Diff: 9.466%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 19.21s\n",
      "    - Unlearned Accs -> Retain: 5.43%, Forget: 0.77%, Test: 4.05%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 95.578%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.75s\n",
      "    Epoch 2/10 completed in 1.78s\n",
      "    Epoch 3/10 completed in 1.76s\n",
      "    Epoch 4/10 completed in 1.78s\n",
      "    Epoch 5/10 completed in 1.78s\n",
      "    Epoch 6/10 completed in 1.77s\n",
      "    Epoch 7/10 completed in 1.76s\n",
      "    Epoch 8/10 completed in 1.77s\n",
      "    Epoch 9/10 completed in 1.78s\n",
      "    Epoch 10/10 completed in 1.83s\n",
      "    Method Random-label applied and saved in 17.93s\n",
      "    - Unlearned Accs -> Retain: 7.54%, Forget: 1.27%, Test: 6.47%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 93.168%\n",
      "\n",
      "==================== Starting Run 3/3 ====================\n",
      "\n",
      "[TRAINING MODE] Step 1/5: Training original model...\n",
      "    Epoch 1/30 completed in 28.58s\n",
      "    Epoch 2/30 completed in 28.79s\n",
      "    Epoch 3/30 completed in 28.86s\n",
      "    Epoch 4/30 completed in 28.97s\n",
      "    Epoch 5/30 completed in 29.13s\n",
      "    Epoch 6/30 completed in 29.04s\n",
      "    Epoch 7/30 completed in 29.19s\n",
      "    Epoch 8/30 completed in 32.95s\n",
      "    Epoch 9/30 completed in 29.27s\n",
      "    Epoch 10/30 completed in 28.91s\n",
      "    Epoch 11/30 completed in 28.68s\n",
      "    Epoch 12/30 completed in 28.61s\n",
      "    Epoch 13/30 completed in 28.83s\n",
      "    Epoch 14/30 completed in 28.94s\n",
      "    Epoch 15/30 completed in 28.55s\n",
      "    Epoch 16/30 completed in 28.67s\n",
      "    Epoch 17/30 completed in 28.73s\n",
      "    Epoch 18/30 completed in 28.91s\n",
      "    Epoch 19/30 completed in 29.15s\n",
      "    Epoch 20/30 completed in 29.25s\n",
      "    Epoch 21/30 completed in 29.59s\n",
      "    Epoch 22/30 completed in 29.55s\n",
      "    Epoch 23/30 completed in 29.18s\n",
      "    Epoch 24/30 completed in 29.16s\n",
      "    Epoch 25/30 completed in 29.31s\n",
      "    Epoch 26/30 completed in 29.20s\n",
      "    Epoch 27/30 completed in 29.13s\n",
      "    Epoch 28/30 completed in 32.90s\n",
      "    Epoch 29/30 completed in 29.19s\n",
      "    Epoch 30/30 completed in 28.76s\n",
      "Original model trained and saved in 878.09s\n",
      "Creating ES partitions...\n",
      "  Extracting embeddings from original model...\n",
      "    Batch 40/196\n",
      "    Batch 80/196\n",
      "    Batch 120/196\n",
      "    Batch 160/196\n",
      "ES partitions created in 26.81s.\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for Low ES...\n",
      "    Epoch 1/30 completed in 30.70s\n",
      "    Epoch 2/30 completed in 28.53s\n",
      "    Epoch 3/30 completed in 27.45s\n",
      "    Epoch 4/30 completed in 30.01s\n",
      "    Epoch 5/30 completed in 31.15s\n",
      "    Epoch 6/30 completed in 28.41s\n",
      "    Epoch 7/30 completed in 30.57s\n",
      "    Epoch 8/30 completed in 27.61s\n",
      "    Epoch 9/30 completed in 27.73s\n",
      "    Epoch 10/30 completed in 27.47s\n",
      "    Epoch 11/30 completed in 27.58s\n",
      "    Epoch 12/30 completed in 27.57s\n",
      "    Epoch 13/30 completed in 27.52s\n",
      "    Epoch 14/30 completed in 27.60s\n",
      "    Epoch 15/30 completed in 27.62s\n",
      "    Epoch 16/30 completed in 27.56s\n",
      "    Epoch 17/30 completed in 27.45s\n",
      "    Epoch 18/30 completed in 28.06s\n",
      "    Epoch 19/30 completed in 27.52s\n",
      "    Epoch 20/30 completed in 27.26s\n",
      "    Epoch 21/30 completed in 27.39s\n",
      "    Epoch 22/30 completed in 27.70s\n",
      "    Epoch 23/30 completed in 27.40s\n",
      "    Epoch 24/30 completed in 27.51s\n",
      "    Epoch 25/30 completed in 27.07s\n",
      "    Epoch 26/30 completed in 27.49s\n",
      "    Epoch 27/30 completed in 27.55s\n",
      "    Epoch 28/30 completed in 27.53s\n",
      "    Epoch 29/30 completed in 27.43s\n",
      "    Epoch 30/30 completed in 27.60s\n",
      "Retrained model trained and saved in 840.21s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.76%, Forget: 99.23%, Test: 84.00%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 90.78%, Forget: 99.67%, Test: 84.52%\n",
      "    - Scores -> ToW: 0.990, Pred Diff: 7.950%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.39s\n",
      "    Epoch 2/10 completed in 27.39s\n",
      "    Epoch 3/10 completed in 27.55s\n",
      "    Epoch 4/10 completed in 27.58s\n",
      "    Epoch 5/10 completed in 27.45s\n",
      "    Epoch 6/10 completed in 27.53s\n",
      "    Epoch 7/10 completed in 27.28s\n",
      "    Epoch 8/10 completed in 26.89s\n",
      "    Epoch 9/10 completed in 27.26s\n",
      "    Epoch 10/10 completed in 27.45s\n",
      "    Method Fine-tune applied and saved in 273.91s\n",
      "    - Unlearned Accs -> Retain: 90.33%, Forget: 99.47%, Test: 83.27%\n",
      "    - Scores -> ToW: 0.986, Pred Diff: 9.124%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 293.65s\n",
      "    - Unlearned Accs -> Retain: 90.79%, Forget: 99.40%, Test: 83.82%\n",
      "    - Scores -> ToW: 0.996, Pred Diff: 8.790%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.55s\n",
      "    - Unlearned Accs -> Retain: 84.60%, Forget: 99.03%, Test: 80.18%\n",
      "    - Scores -> ToW: 0.901, Pred Diff: 13.188%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 33.99s\n",
      "    - Unlearned Accs -> Retain: 88.40%, Forget: 99.07%, Test: 82.92%\n",
      "    - Scores -> ToW: 0.964, Pred Diff: 10.018%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 20.50s\n",
      "    - Unlearned Accs -> Retain: 8.81%, Forget: 1.60%, Test: 7.71%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 92.034%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.77s\n",
      "    Epoch 2/10 completed in 1.80s\n",
      "    Epoch 3/10 completed in 1.79s\n",
      "    Epoch 4/10 completed in 1.77s\n",
      "    Epoch 5/10 completed in 1.77s\n",
      "    Epoch 6/10 completed in 1.79s\n",
      "    Epoch 7/10 completed in 1.77s\n",
      "    Epoch 8/10 completed in 1.79s\n",
      "    Epoch 9/10 completed in 1.78s\n",
      "    Epoch 10/10 completed in 1.77s\n",
      "    Method Random-label applied and saved in 17.96s\n",
      "    - Unlearned Accs -> Retain: 11.74%, Forget: 3.47%, Test: 9.78%\n",
      "    - Scores -> ToW: 0.002, Pred Diff: 89.594%\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for Medium ES...\n",
      "    Epoch 1/30 completed in 31.05s\n",
      "    Epoch 2/30 completed in 28.31s\n",
      "    Epoch 3/30 completed in 27.13s\n",
      "    Epoch 4/30 completed in 27.66s\n",
      "    Epoch 5/30 completed in 27.83s\n",
      "    Epoch 6/30 completed in 27.43s\n",
      "    Epoch 7/30 completed in 27.69s\n",
      "    Epoch 8/30 completed in 27.79s\n",
      "    Epoch 9/30 completed in 27.55s\n",
      "    Epoch 10/30 completed in 27.78s\n",
      "    Epoch 11/30 completed in 27.44s\n",
      "    Epoch 12/30 completed in 27.51s\n",
      "    Epoch 13/30 completed in 27.36s\n",
      "    Epoch 14/30 completed in 27.60s\n",
      "    Epoch 15/30 completed in 27.47s\n",
      "    Epoch 16/30 completed in 27.53s\n",
      "    Epoch 17/30 completed in 27.65s\n",
      "    Epoch 18/30 completed in 29.27s\n",
      "    Epoch 19/30 completed in 30.88s\n",
      "    Epoch 20/30 completed in 27.53s\n",
      "    Epoch 21/30 completed in 30.49s\n",
      "    Epoch 22/30 completed in 29.85s\n",
      "    Epoch 23/30 completed in 27.62s\n",
      "    Epoch 24/30 completed in 28.02s\n",
      "    Epoch 25/30 completed in 27.49s\n",
      "    Epoch 26/30 completed in 26.96s\n",
      "    Epoch 27/30 completed in 27.85s\n",
      "    Epoch 28/30 completed in 27.64s\n",
      "    Epoch 29/30 completed in 27.46s\n",
      "    Epoch 30/30 completed in 27.28s\n",
      "Retrained model trained and saved in 841.25s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.27%, Forget: 98.77%, Test: 83.87%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.10s\n",
      "    - Unlearned Accs -> Retain: 90.87%, Forget: 99.57%, Test: 84.52%\n",
      "    - Scores -> ToW: 0.980, Pred Diff: 8.300%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.51s\n",
      "    Epoch 2/10 completed in 27.47s\n",
      "    Epoch 3/10 completed in 27.42s\n",
      "    Epoch 4/10 completed in 28.43s\n",
      "    Epoch 5/10 completed in 27.49s\n",
      "    Epoch 6/10 completed in 27.19s\n",
      "    Epoch 7/10 completed in 27.13s\n",
      "    Epoch 8/10 completed in 26.97s\n",
      "    Epoch 9/10 completed in 27.41s\n",
      "    Epoch 10/10 completed in 26.68s\n",
      "    Method Fine-tune applied and saved in 273.85s\n",
      "    - Unlearned Accs -> Retain: 90.14%, Forget: 99.13%, Test: 83.47%\n",
      "    - Scores -> ToW: 0.991, Pred Diff: 9.490%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 287.24s\n",
      "    - Unlearned Accs -> Retain: 90.59%, Forget: 99.17%, Test: 83.55%\n",
      "    - Scores -> ToW: 0.990, Pred Diff: 9.256%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.42s\n",
      "    - Unlearned Accs -> Retain: 85.06%, Forget: 99.30%, Test: 80.70%\n",
      "    - Scores -> ToW: 0.913, Pred Diff: 12.610%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 34.30s\n",
      "    - Unlearned Accs -> Retain: 89.13%, Forget: 99.37%, Test: 83.08%\n",
      "    - Scores -> ToW: 0.975, Pred Diff: 9.724%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 21.95s\n",
      "    - Unlearned Accs -> Retain: 9.01%, Forget: 3.57%, Test: 8.19%\n",
      "    - Scores -> ToW: 0.002, Pred Diff: 91.392%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 2.01s\n",
      "    Epoch 2/10 completed in 2.02s\n",
      "    Epoch 3/10 completed in 2.01s\n",
      "    Epoch 4/10 completed in 2.00s\n",
      "    Epoch 5/10 completed in 1.99s\n",
      "    Epoch 6/10 completed in 1.99s\n",
      "    Epoch 7/10 completed in 2.00s\n",
      "    Epoch 8/10 completed in 1.99s\n",
      "    Epoch 9/10 completed in 1.99s\n",
      "    Epoch 10/10 completed in 2.01s\n",
      "    Method Random-label applied and saved in 20.20s\n",
      "    - Unlearned Accs -> Retain: 8.63%, Forget: 3.63%, Test: 7.64%\n",
      "    - Scores -> ToW: 0.002, Pred Diff: 92.428%\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "[TRAINING MODE] Step 2/5: Training retrained model for High ES...\n",
      "    Epoch 1/30 completed in 29.31s\n",
      "    Epoch 2/30 completed in 27.70s\n",
      "    Epoch 3/30 completed in 27.69s\n",
      "    Epoch 4/30 completed in 27.58s\n",
      "    Epoch 5/30 completed in 27.21s\n",
      "    Epoch 6/30 completed in 27.59s\n",
      "    Epoch 7/30 completed in 27.57s\n",
      "    Epoch 8/30 completed in 27.71s\n",
      "    Epoch 9/30 completed in 27.06s\n",
      "    Epoch 10/30 completed in 27.48s\n",
      "    Epoch 11/30 completed in 27.57s\n",
      "    Epoch 12/30 completed in 27.76s\n",
      "    Epoch 13/30 completed in 28.13s\n",
      "    Epoch 14/30 completed in 27.64s\n",
      "    Epoch 15/30 completed in 27.32s\n",
      "    Epoch 16/30 completed in 27.53s\n",
      "    Epoch 17/30 completed in 27.70s\n",
      "    Epoch 18/30 completed in 27.72s\n",
      "    Epoch 19/30 completed in 27.53s\n",
      "    Epoch 20/30 completed in 27.94s\n",
      "    Epoch 21/30 completed in 27.44s\n",
      "    Epoch 22/30 completed in 27.53s\n",
      "    Epoch 23/30 completed in 29.06s\n",
      "    Epoch 24/30 completed in 27.54s\n",
      "    Epoch 25/30 completed in 27.56s\n",
      "    Epoch 26/30 completed in 28.05s\n",
      "    Epoch 27/30 completed in 30.46s\n",
      "    Epoch 28/30 completed in 27.75s\n",
      "    Epoch 29/30 completed in 27.40s\n",
      "    Epoch 30/30 completed in 27.52s\n",
      "Retrained model trained and saved in 834.21s\n",
      "\n",
      "[Step 3/5] Evaluating retrained model...\n",
      "  Retrained Accs -> Retain: 90.36%, Forget: 98.40%, Test: 84.35%\n",
      "\n",
      "[Step 4/5] Applying and evaluating unlearning methods...\n",
      "  > [TRAINING MODE] Applying method: Original...\n",
      "    Method Original applied and saved in 0.09s\n",
      "    - Unlearned Accs -> Retain: 90.87%, Forget: 99.27%, Test: 84.52%\n",
      "    - Scores -> ToW: 0.985, Pred Diff: 8.206%\n",
      "  > [TRAINING MODE] Applying method: Fine-tune...\n",
      "    Epoch 1/10 completed in 27.46s\n",
      "    Epoch 2/10 completed in 27.33s\n",
      "    Epoch 3/10 completed in 27.13s\n",
      "    Epoch 4/10 completed in 27.28s\n",
      "    Epoch 5/10 completed in 27.13s\n",
      "    Epoch 6/10 completed in 27.78s\n",
      "    Epoch 7/10 completed in 31.00s\n",
      "    Epoch 8/10 completed in 31.15s\n",
      "    Epoch 9/10 completed in 31.07s\n",
      "    Epoch 10/10 completed in 29.14s\n",
      "    Method Fine-tune applied and saved in 286.60s\n",
      "    - Unlearned Accs -> Retain: 90.55%, Forget: 98.70%, Test: 83.70%\n",
      "    - Scores -> ToW: 0.989, Pred Diff: 9.090%\n",
      "  > [TRAINING MODE] Applying method: L1-sparse...\n",
      "    Method L1-sparse applied and saved in 295.93s\n",
      "    - Unlearned Accs -> Retain: 90.40%, Forget: 98.73%, Test: 83.35%\n",
      "    - Scores -> ToW: 0.986, Pred Diff: 9.270%\n",
      "  > [TRAINING MODE] Applying method: NegGrad...\n",
      "    Method NegGrad applied and saved in 17.54s\n",
      "    - Unlearned Accs -> Retain: 86.62%, Forget: 98.47%, Test: 81.22%\n",
      "    - Scores -> ToW: 0.932, Pred Diff: 11.498%\n",
      "  > [TRAINING MODE] Applying method: NegGrad+...\n",
      "    Method NegGrad+ applied and saved in 34.47s\n",
      "    - Unlearned Accs -> Retain: 89.34%, Forget: 98.77%, Test: 83.30%\n",
      "    - Scores -> ToW: 0.976, Pred Diff: 9.332%\n",
      "  > [TRAINING MODE] Applying method: SalUn...\n",
      "    Method SalUn applied and saved in 18.72s\n",
      "    - Unlearned Accs -> Retain: 7.44%, Forget: 2.53%, Test: 6.89%\n",
      "    - Scores -> ToW: 0.002, Pred Diff: 92.666%\n",
      "  > [TRAINING MODE] Applying method: Random-label...\n",
      "    Epoch 1/10 completed in 1.71s\n",
      "    Epoch 2/10 completed in 1.70s\n",
      "    Epoch 3/10 completed in 1.70s\n",
      "    Epoch 4/10 completed in 1.71s\n",
      "    Epoch 5/10 completed in 1.72s\n",
      "    Epoch 6/10 completed in 1.70s\n",
      "    Epoch 7/10 completed in 1.70s\n",
      "    Epoch 8/10 completed in 1.70s\n",
      "    Epoch 9/10 completed in 1.72s\n",
      "    Epoch 10/10 completed in 1.70s\n",
      "    Method Random-label applied and saved in 17.23s\n",
      "    - Unlearned Accs -> Retain: 5.74%, Forget: 2.03%, Test: 5.23%\n",
      "    - Scores -> ToW: 0.001, Pred Diff: 94.446%\n",
      "\n",
      "==================== Final Results ====================\n",
      "                       ToW                             Percentage of different predictions (%)                              \n",
      "      Method        Low ES     Medium ES       High ES                                  Low ES      Medium ES        High ES\n",
      "    Original 0.983 ± 0.016 0.983 ± 0.015 0.977 ± 0.015                           8.211 ± 0.683  8.127 ± 0.399  8.065 ± 0.312\n",
      "   Fine-tune 0.988 ± 0.012 0.986 ± 0.010 0.986 ± 0.010                           9.439 ± 0.800  9.328 ± 0.457  9.150 ± 0.591\n",
      "   L1-sparse 0.991 ± 0.015 0.984 ± 0.013 0.986 ± 0.002                           9.285 ± 1.210  9.186 ± 0.441  9.128 ± 0.307\n",
      "     NegGrad 0.864 ± 0.187 0.904 ± 0.076 0.930 ± 0.009                         15.073 ± 10.012 12.871 ± 4.194 11.422 ± 1.100\n",
      "    NegGrad+ 0.928 ± 0.161 0.967 ± 0.036 0.974 ± 0.014                          11.595 ± 6.974  9.932 ± 1.989  9.228 ± 0.754\n",
      "       SalUn 0.001 ± 0.000 0.002 ± 0.001 0.002 ± 0.003                          90.527 ± 8.516 90.569 ± 2.126 92.790 ± 6.777\n",
      "Random-label 0.002 ± 0.003 0.001 ± 0.002 0.002 ± 0.003                          89.237 ± 1.057 92.530 ± 2.264 92.252 ± 6.876\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import time\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "# from torch_influence import WoodFisherInfluenceComputer\n",
    "\n",
    "# ===================================================================\n",
    "# 1. 실험 환경 설정\n",
    "# ===================================================================\n",
    "CONFIG = {\n",
    "    \"run_training\": True,  # True: 훈련 및 저장 실행, False: 저장된 파일 로드하여 평가만 실행\n",
    "    \"model_save_dir\": \"../saved_models\", \n",
    "    \"num_runs\": 3,\n",
    "    \"epochs\": 30,\n",
    "    \"unlearn_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 0.1,\n",
    "    \"unlearn_lr\": 0.01,\n",
    "    \"unlearn_lr_neggrad\": 1e-4, # NegGrad는 작은 LR 사용\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"forget_set_size\": 3000,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"l1_lambda\": 1e-5,\n",
    "    \"neggrad_plus_alpha\": 0.2,\n",
    "    \"salun_sparsity\": 0.5,\n",
    "}\n",
    "\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. 모델 및 데이터 관련 헬퍼 함수\n",
    "# ===================================================================\n",
    "def get_model():\n",
    "    \"\"\"ResNet-18 모델 생성\"\"\"\n",
    "    model = models.resnet18(weights=None, num_classes=10)\n",
    "    return model.to(CONFIG['device'])\n",
    "\n",
    "def train_model(model, train_loader, epochs, lr, is_unlearning=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n",
    "    if not is_unlearning:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time() # 에포크 시작 시간 기록\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if not is_unlearning:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # 에포크별 진행 상황 출력\n",
    "        epoch_end_time = time.time()\n",
    "        print(f\"    Epoch {epoch+1}/{epochs} completed in {epoch_end_time - epoch_start_time:.2f}s\")\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    \"\"\"모델 정확도 평가\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# ===================================================================\n",
    "# 3. ES(Entanglement Score) 기반 데이터 분할 \n",
    "# ===================================================================\n",
    "def create_es_partitions(original_model, train_dataset):\n",
    "    \"\"\"\n",
    "    논문 A.3 절차에 따라 ES 파티션을 생성\n",
    "    1. 원본 모델의 임베딩 추출\n",
    "    2. 전체 임베딩의 중심(centroid) 계산\n",
    "    3. 각 데이터와 중심 간의 거리 계산 후 정렬\n",
    "    4. 거리가 가장 먼 데이터부터 low, medium, high ES 그룹 생성\n",
    "    \"\"\"\n",
    "    print(\"Creating ES partitions...\")\n",
    "    start_time = time.time() \n",
    "    # 1. 임베딩 추출\n",
    "    embedding_extractor = nn.Sequential(*list(original_model.children())[:-1])\n",
    "    embedding_extractor.eval()\n",
    "    \n",
    "    all_embeddings = []\n",
    "    loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    # 임베딩 추출 진행 상황 출력\n",
    "    print(\"  Extracting embeddings from original model...\")\n",
    "    for i, (inputs, _) in enumerate(loader):\n",
    "        if (i + 1) % 40 == 0:\n",
    "            print(f\"    Batch {i+1}/{len(loader)}\")\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            embeddings = embedding_extractor(inputs).squeeze()\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "\n",
    "    centroid = all_embeddings.mean(dim=0)\n",
    "    distances = torch.sum((all_embeddings - centroid) ** 2, dim=1)\n",
    "    sorted_indices = torch.argsort(distances, descending=True).numpy()\n",
    "\n",
    "    fs_size = CONFIG['forget_set_size']\n",
    "    partitions = {\n",
    "        \"Low ES\": sorted_indices[:fs_size],\n",
    "        \"Medium ES\": sorted_indices[fs_size : 2 * fs_size],\n",
    "        \"High ES\": sorted_indices[2 * fs_size : 3 * fs_size],\n",
    "    }\n",
    "    end_time = time.time() \n",
    "    print(f\"ES partitions created in {end_time - start_time:.2f}s.\")\n",
    "    return partitions\n",
    "\n",
    "# ===================================================================\n",
    "# 4. 언러닝(Unlearning) 알고리즘 구현\n",
    "# ===================================================================\n",
    "class RelabelDataset(Dataset):\n",
    "    \"\"\"Random-label과 SalUn에서 레이블을 변경하기 위한 커스텀 데이터셋\"\"\"\n",
    "    def __init__(self, original_dataset, num_classes=10):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.num_classes = num_classes\n",
    "        # 원본 레이블과 다른 랜덤 레이블 생성\n",
    "        self.new_labels = [torch.randint(0, num_classes, (1,)).item() for _ in range(len(original_dataset))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, original_label = self.original_dataset[idx]\n",
    "        new_label = self.new_labels[idx]\n",
    "        # 원본 레이블과 같지 않도록 보장\n",
    "        while new_label == original_label:\n",
    "            new_label = torch.randint(0, self.num_classes, (1,)).item()\n",
    "        return image, new_label\n",
    "\n",
    "def unlearn_finetune(original_model, retain_loader, config):\n",
    "    \"\"\"Fine-tune: 원본 모델을 retain set으로 추가 학습\"\"\"\n",
    "    unlearned_model = copy.deepcopy(original_model)\n",
    "    train_model(unlearned_model, retain_loader, config['unlearn_epochs'], config['unlearn_lr'], is_unlearning=True)\n",
    "    return unlearned_model\n",
    "\n",
    "def unlearn_neggrad(original_model, forget_loader, config):\n",
    "    \"\"\"NegGrad: Forget set에 대해 loss를 최대화 (gradient ascent)\"\"\"\n",
    "    unlearned_model = copy.deepcopy(original_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(unlearned_model.parameters(), lr=config['unlearn_lr_neggrad'])\n",
    "    \n",
    "    unlearned_model.train()\n",
    "    for _ in range(config['unlearn_epochs']):\n",
    "        for inputs, targets in forget_loader:\n",
    "            inputs, targets = inputs.to(config['device']), targets.to(config['device'])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = unlearned_model(inputs)\n",
    "            loss = -criterion(outputs, targets) # Loss를 최대화하기 위해 음수 사용\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return unlearned_model\n",
    "\n",
    "def unlearn_l1_sparse(original_model, retain_loader, config):\n",
    "    \"\"\"L1-sparse: 리테인셋에 L1 패널티를 주며 파인튜닝\"\"\"\n",
    "    unlearned_model = copy.deepcopy(original_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(unlearned_model.parameters(), lr=config['unlearn_lr'], momentum=config['momentum'])\n",
    "    \n",
    "    unlearned_model.train()\n",
    "    for _ in range(config['unlearn_epochs']):\n",
    "        for inputs, targets in retain_loader:\n",
    "            inputs, targets = inputs.to(config['device']), targets.to(config['device'])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = unlearned_model(inputs)\n",
    "            \n",
    "            # L1 패널티 계산\n",
    "            l1_penalty = 0.\n",
    "            for param in unlearned_model.parameters():\n",
    "                l1_penalty += torch.abs(param).sum()\n",
    "            \n",
    "            loss = criterion(outputs, targets) + config['l1_lambda'] * l1_penalty\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return unlearned_model\n",
    "\n",
    "def unlearn_neggrad_plus(original_model, retain_loader, forget_loader, config):\n",
    "    \"\"\"NegGrad+: Retain loss 최소화와 Forget loss 최대화를 동시에 수행\"\"\"\n",
    "    unlearned_model = copy.deepcopy(original_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(unlearned_model.parameters(), lr=config['unlearn_lr'])\n",
    "\n",
    "    unlearned_model.train()\n",
    "    for _ in range(config['unlearn_epochs']):\n",
    "        # 데이터로더 길이가 다를 수 있으므로 itertools.cycle 사용\n",
    "        retain_iter = iter(itertools.cycle(retain_loader))\n",
    "        for forget_inputs, forget_targets in forget_loader:\n",
    "            retain_inputs, retain_targets = next(retain_iter)\n",
    "            \n",
    "            retain_inputs, retain_targets = retain_inputs.to(config['device']), retain_targets.to(config['device'])\n",
    "            forget_inputs, forget_targets = forget_inputs.to(config['device']), forget_targets.to(config['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Retain Loss (최소화)\n",
    "            retain_outputs = unlearned_model(retain_inputs)\n",
    "            loss_retain = criterion(retain_outputs, retain_targets)\n",
    "            \n",
    "            # Forget Loss (최대화)\n",
    "            forget_outputs = unlearned_model(forget_inputs)\n",
    "            loss_forget = -criterion(forget_outputs, forget_targets)\n",
    "            \n",
    "            # 두 Loss를 결합\n",
    "            loss = loss_retain + config['neggrad_plus_alpha'] * loss_forget\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return unlearned_model\n",
    "\n",
    "def unlearn_random_label(original_model, forget_set, config):\n",
    "    \"\"\"Random-label: Forget set의 레이블을 랜덤으로 바꾸어 학습\"\"\"\n",
    "    unlearned_model = copy.deepcopy(original_model)\n",
    "    relabel_dataset = RelabelDataset(forget_set)\n",
    "    relabel_loader = DataLoader(relabel_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    \n",
    "    # 일반적인 파인튜닝과 동일하게 학습\n",
    "    train_model(unlearned_model, relabel_loader, config['unlearn_epochs'], config['unlearn_lr'], is_unlearning=True)\n",
    "    return unlearned_model\n",
    "\n",
    "def unlearn_salun(original_model, forget_set, config):\n",
    "    \"\"\"SalUn: Forget set에 대해 중요한(salient) 파라미터만 랜덤 레이블로 학습\"\"\"\n",
    "    unlearned_model = copy.deepcopy(original_model)\n",
    "    \n",
    "    # 1. Saliency 계산 (Gradient 기반)\n",
    "    saliency = [torch.zeros_like(p) for p in unlearned_model.parameters()]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    forget_loader = DataLoader(forget_set, batch_size=config['batch_size'])\n",
    "\n",
    "    for inputs, targets in forget_loader:\n",
    "        inputs, targets = inputs.to(config['device']), targets.to(config['device'])\n",
    "        unlearned_model.zero_grad()\n",
    "        outputs = unlearned_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        for i, param in enumerate(unlearned_model.parameters()):\n",
    "            if param.grad is not None:\n",
    "                saliency[i] += param.grad.abs()\n",
    "\n",
    "    # 2. Saliency가 높은 파라미터를 찾기 위한 마스크 생성\n",
    "    flat_saliency = torch.cat([s.flatten() for s in saliency])\n",
    "    k = int(len(flat_saliency) * config['salun_sparsity'])\n",
    "    threshold, _ = torch.kthvalue(flat_saliency, k)\n",
    "    masks = [(s > threshold).float() for s in saliency]\n",
    "\n",
    "    # 3. 랜덤 레이블 데이터셋으로 Salient 파라미터만 학습\n",
    "    relabel_dataset = RelabelDataset(forget_set)\n",
    "    relabel_loader = DataLoader(relabel_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    optimizer = optim.SGD(unlearned_model.parameters(), lr=config['unlearn_lr'], momentum=config['momentum'])\n",
    "    \n",
    "    unlearned_model.train()\n",
    "    for _ in range(config['unlearn_epochs']):\n",
    "        for inputs, targets in relabel_loader:\n",
    "            inputs, targets = inputs.to(config['device']), targets.to(config['device'])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = unlearned_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 마스크를 적용하여 salient 파라미터의 gradient만 유지\n",
    "            for i, param in enumerate(unlearned_model.parameters()):\n",
    "                if param.grad is not None:\n",
    "                    param.grad *= masks[i]\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "    return unlearned_model\n",
    "\n",
    "def unlearn_influence(original_model, forget_indices, train_dataset, config):\n",
    "    \"\"\"\n",
    "    Influence Unlearning: influence 함수를 사용해 forget set의 영향을 추정하고 제거함\n",
    "    이 방법은 재훈련 없이 파라미터의 변화를 근사적으로 계산함\n",
    "    \"\"\"\n",
    "    unlearned_model = copy.deepcopy(original_model)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # 1. InfluenceComputer 초기화\n",
    "    # 이 과정에서 내부적으로 Hessian-vector product 계산 준비\n",
    "    # 이는 계산 비용이 매우 높을 수 있음\n",
    "    computer = WoodFisherInfluenceComputer(\n",
    "        model=unlearned_model,\n",
    "        criterion=criterion,\n",
    "        train_dataset=train_dataset,\n",
    "        device=config['device'],\n",
    "        damp=1e-6, # 계산 안정성을 위한 하이퍼파라미터\n",
    "        gnh=True   # Generalized Gauss-Newton 행렬 사용\n",
    "    )\n",
    "\n",
    "    # 2. Forget set의 영향 계산\n",
    "    # forget_indices에 해당하는 데이터가 모델 파라미터에 미친 영향을 추정\n",
    "    param_changes = computer.compute_influence_on_parameters(\n",
    "        train_indices=forget_indices\n",
    "    )\n",
    "    \n",
    "    # 3. 모델 파라미터 업데이트\n",
    "    # 원본 모델의 파라미터에서 계산된 영향을 빼서 업데이트\n",
    "    with torch.no_grad():\n",
    "        for i, param in enumerate(unlearned_model.parameters()):\n",
    "            param.sub_(param_changes[i] * config['influence_alpha']) # alpha는 스케일링 팩터\n",
    "\n",
    "    return unlearned_model\n",
    "\n",
    "# ===================================================================\n",
    "# 5. 결과 지표 계산 함수\n",
    "# ===================================================================\n",
    "def calculate_tow(unlearned_accs, retrained_accs):\n",
    "    \"\"\"ToW (Tug-of-War) 점수를 계산\"\"\"\n",
    "    la = lambda acc1, acc2: abs(acc1 - acc2) / 100.0\n",
    "    la_s = la(unlearned_accs['forget'], retrained_accs['forget'])\n",
    "    la_r = la(unlearned_accs['retain'], retrained_accs['retain'])\n",
    "    la_t = la(unlearned_accs['test'], retrained_accs['test'])\n",
    "    return (1 - la_s) * (1 - la_r) * (1 - la_t)\n",
    "\n",
    "def calculate_prediction_diff(unlearned_model, retrained_model, full_dataset_loader):\n",
    "    \"\"\"두 모델의 예측이 다른 샘플의 비율을 계산\"\"\"\n",
    "    unlearned_model.eval()\n",
    "    retrained_model.eval()\n",
    "    diff_count = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in full_dataset_loader:\n",
    "            inputs = inputs.to(CONFIG['device'])\n",
    "            unlearned_preds = torch.argmax(unlearned_model(inputs), 1)\n",
    "            retrained_preds = torch.argmax(retrained_model(inputs), 1)\n",
    "            diff_count += (unlearned_preds != retrained_preds).sum().item()\n",
    "            total_count += inputs.size(0)\n",
    "    return (diff_count / total_count) * 100\n",
    "\n",
    "# ===================================================================\n",
    "# 6. 메인 실험 루프 \n",
    "# ===================================================================\n",
    "def main():\n",
    "    # 모델 저장을 위한 디렉토리 생성\n",
    "    save_dir = CONFIG[\"model_save_dir\"]\n",
    "    if CONFIG[\"run_training\"]:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 데이터셋 로드\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "    \n",
    "    full_train_eval_loader = DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_test),\n",
    "        batch_size=CONFIG['batch_size'], shuffle=False\n",
    "    )\n",
    "    \n",
    "    all_methods = [\"Original\", \"Fine-tune\", \"L1-sparse\", \"NegGrad\", \"NegGrad+\", \"SalUn\", \"Random-label\"]\n",
    "    results = {\n",
    "        \"ToW\": {method: {es: [] for es in [\"Low ES\", \"Medium ES\", \"High ES\"]} for method in all_methods},\n",
    "        \"Pred_Diff\": {method: {es: [] for es in [\"Low ES\", \"Medium ES\", \"High ES\"]} for method in all_methods}\n",
    "    }\n",
    "\n",
    "    for run in range(CONFIG['num_runs']):\n",
    "        print(f\"\\n{'='*20} Starting Run {run+1}/{CONFIG['num_runs']} {'='*20}\")\n",
    "\n",
    "        # 훈련 모드와 평가 모드 분리\n",
    "        original_model = get_model()\n",
    "        original_model_path = os.path.join(save_dir, f\"run_{run}_original_model.pth\")\n",
    "        partitions_path = os.path.join(save_dir, f\"run_{run}_es_partitions.pth\")\n",
    "\n",
    "        if CONFIG[\"run_training\"]:\n",
    "            print(\"\\n[TRAINING MODE] Step 1/5: Training original model...\")\n",
    "            run_start_time = time.time()\n",
    "            train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "            train_model(original_model, train_loader, CONFIG['epochs'], CONFIG['lr'])\n",
    "            torch.save(original_model.state_dict(), original_model_path)\n",
    "            print(f\"Original model trained and saved in {time.time() - run_start_time:.2f}s\")\n",
    "            \n",
    "            es_partitions = create_es_partitions(original_model, train_dataset)\n",
    "            torch.save(es_partitions, partitions_path)\n",
    "        else:\n",
    "            print(\"\\n[EVALUATION MODE] Loading pre-trained original model and ES partitions...\")\n",
    "            original_model.load_state_dict(torch.load(original_model_path, map_location=CONFIG['device']))\n",
    "            es_partitions = torch.load(partitions_path)\n",
    "\n",
    "        for es_level, forget_indices in es_partitions.items():\n",
    "            print(f\"\\n--- Processing ES Level: {es_level} ---\")\n",
    "            \n",
    "            all_indices = np.arange(len(train_dataset))\n",
    "            retain_indices = np.setdiff1d(all_indices, forget_indices, assume_unique=True)\n",
    "            \n",
    "            retain_set = Subset(train_dataset, retain_indices)\n",
    "            forget_set = Subset(train_dataset, forget_indices)\n",
    "            \n",
    "            retain_loader = DataLoader(retain_set, batch_size=CONFIG['batch_size'], shuffle=True)\n",
    "            retain_eval_loader = DataLoader(retain_set, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "            forget_loader = DataLoader(forget_set, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "            retrained_model = get_model()\n",
    "            retrained_model_path = os.path.join(save_dir, f\"run_{run}_{es_level.replace(' ', '')}_retrained.pth\")\n",
    "\n",
    "            if CONFIG[\"run_training\"]:\n",
    "                print(f\"\\n[TRAINING MODE] Step 2/5: Training retrained model for {es_level}...\")\n",
    "                retrain_start_time = time.time()\n",
    "                train_model(retrained_model, retain_loader, CONFIG['epochs'], CONFIG['lr'])\n",
    "                torch.save(retrained_model.state_dict(), retrained_model_path)\n",
    "                print(f\"Retrained model trained and saved in {time.time() - retrain_start_time:.2f}s\")\n",
    "            else:\n",
    "                print(f\"\\n[EVALUATION MODE] Loading retrained model for {es_level}...\")\n",
    "                retrained_model.load_state_dict(torch.load(retrained_model_path, map_location=CONFIG['device']))\n",
    "            \n",
    "            print(\"\\n[Step 3/5] Evaluating retrained model...\")\n",
    "            retrained_accs = {\n",
    "                'retain': evaluate_model(retrained_model, retain_eval_loader),\n",
    "                'forget': evaluate_model(retrained_model, forget_loader),\n",
    "                'test': evaluate_model(retrained_model, test_loader)\n",
    "            }\n",
    "            print(f\"  Retrained Accs -> Retain: {retrained_accs['retain']:.2f}%, Forget: {retrained_accs['forget']:.2f}%, Test: {retrained_accs['test']:.2f}%\")\n",
    "            \n",
    "            unlearning_methods = {\n",
    "                \"Original\": lambda: copy.deepcopy(original_model),\n",
    "                \"Fine-tune\": lambda: unlearn_finetune(original_model, retain_loader, CONFIG),\n",
    "                \"L1-sparse\": lambda: unlearn_l1_sparse(original_model, retain_loader, CONFIG),\n",
    "                \"NegGrad\": lambda: unlearn_neggrad(original_model, forget_loader, CONFIG),\n",
    "                \"NegGrad+\": lambda: unlearn_neggrad_plus(original_model, retain_loader, forget_loader, CONFIG),\n",
    "                \"SalUn\": lambda: unlearn_salun(original_model, forget_set, CONFIG),\n",
    "                \"Random-label\": lambda: unlearn_random_label(original_model, forget_set, CONFIG),\n",
    "            }\n",
    "            \n",
    "            print(\"\\n[Step 4/5] Applying and evaluating unlearning methods...\")\n",
    "            for method_name, method_fn in unlearning_methods.items():\n",
    "                unlearned_model_path = os.path.join(save_dir, f\"run_{run}_{es_level.replace(' ', '')}_{method_name}_unlearned.pth\")\n",
    "                \n",
    "                if CONFIG[\"run_training\"]:\n",
    "                    print(f\"  > [TRAINING MODE] Applying method: {method_name}...\")\n",
    "                    method_start_time = time.time()\n",
    "                    # Original은 훈련이 없으므로 바로 저장\n",
    "                    unlearned_model = method_fn()\n",
    "                    torch.save(unlearned_model.state_dict(), unlearned_model_path)\n",
    "                    print(f\"    Method {method_name} applied and saved in {time.time() - method_start_time:.2f}s\")\n",
    "                else:\n",
    "                    print(f\"  > [EVALUATION MODE] Loading unlearned model for method: {method_name}...\")\n",
    "                    unlearned_model = get_model() \n",
    "                    unlearned_model.load_state_dict(torch.load(unlearned_model_path, map_location=CONFIG['device']))\n",
    "\n",
    "                unlearned_accs = {\n",
    "                    'retain': evaluate_model(unlearned_model, retain_eval_loader),\n",
    "                    'forget': evaluate_model(unlearned_model, forget_loader),\n",
    "                    'test': evaluate_model(unlearned_model, test_loader)\n",
    "                }\n",
    "                \n",
    "                tow_score = calculate_tow(unlearned_accs, retrained_accs)\n",
    "                pred_diff = calculate_prediction_diff(unlearned_model, retrained_model, full_train_eval_loader)\n",
    "\n",
    "                results[\"ToW\"][method_name][es_level].append(tow_score)\n",
    "                results[\"Pred_Diff\"][method_name][es_level].append(pred_diff)\n",
    "                \n",
    "                print(f\"    - Unlearned Accs -> Retain: {unlearned_accs['retain']:.2f}%, Forget: {unlearned_accs['forget']:.2f}%, Test: {unlearned_accs['test']:.2f}%\")\n",
    "                print(f\"    - Scores -> ToW: {tow_score:.3f}, Pred Diff: {pred_diff:.3f}%\")\n",
    "\n",
    "    # ===================================================================\n",
    "    # 7. 결과 정리 및 출력\n",
    "    # ===================================================================\n",
    "    print(f\"\\n{'='*20} Final Results {'='*20}\")\n",
    "    \n",
    "    def format_results(data):\n",
    "        mean = np.mean(data)\n",
    "        if len(data) < 2: return f\"{mean:.3f}\"\n",
    "        sem = stats.sem(data)\n",
    "        ci = sem * stats.t.ppf((1 + 0.95) / 2., len(data)-1)\n",
    "        return f\"{mean:.3f} ± {ci:.3f}\"\n",
    "\n",
    "    output_data = []\n",
    "    es_levels = [\"Low ES\", \"Medium ES\", \"High ES\"]\n",
    "    for method in all_methods:\n",
    "        row = {'Method': method}\n",
    "        for es in es_levels:\n",
    "            row[f'ToW_{es}'] = format_results(results[\"ToW\"][method][es])\n",
    "        for es in es_levels:\n",
    "            row[f'Pred_Diff_{es}'] = format_results(results[\"Pred_Diff\"][method][es])\n",
    "        output_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(output_data)\n",
    "    df.columns = pd.MultiIndex.from_tuples([\n",
    "        (\"\", \"Method\"), (\"ToW\", \"Low ES\"), (\"ToW\", \"Medium ES\"), (\"ToW\", \"High ES\"),\n",
    "        (\"Percentage of different predictions (%)\", \"Low ES\"),\n",
    "        (\"Percentage of different predictions (%)\", \"Medium ES\"),\n",
    "        (\"Percentage of different predictions (%)\", \"High ES\")\n",
    "    ])\n",
    "\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
