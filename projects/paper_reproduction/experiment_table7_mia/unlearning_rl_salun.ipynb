{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff91a75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 21 23:39:40 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla P100-PCIE-16GB           Off | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   42C    P0              34W / 250W |  12422MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE-16GB           Off | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              31W / 250W |   6302MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P100-PCIE-16GB           Off | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   46C    P0              34W / 250W |   8370MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P100-PCIE-16GB           Off | 00000000:08:00.0 Off |                    0 |\n",
      "| N/A   54C    P0             107W / 250W |   9384MiB / 16384MiB |     49%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla P100-PCIE-16GB           Off | 00000000:0C:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              42W / 250W |  13760MiB / 16384MiB |     16%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla P100-PCIE-16GB           Off | 00000000:0D:00.0 Off |                    0 |\n",
      "| N/A   44C    P0              53W / 250W |   9076MiB / 16384MiB |     20%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla P100-PCIE-16GB           Off | 00000000:0E:00.0 Off |                    0 |\n",
      "| N/A   70C    P0             172W / 250W |  13486MiB / 16384MiB |    100%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla P100-PCIE-16GB           Off | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              50W / 250W |  14760MiB / 16384MiB |     18%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4b6255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device (GPU if available)\n",
    "DEVICE_NUM = 1\n",
    "ADDITIONAL_GPU = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_device(DEVICE_NUM)\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "print(f\"INFO: Using device - {device}:{DEVICE_NUM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d828e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global random seed set to 42\n",
      "Using device: cuda\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/cifar-10-python.tar.gz to ../../data\n",
      "Files already downloaded and verified\n",
      "\n",
      "==================== Starting Run 1/3 ====================\n",
      "\n",
      "[LOADING] original model and ES partitions\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 32.53s\n",
      "    Epoch 2/10 completed in 31.63s\n",
      "    Epoch 3/10 completed in 35.16s\n",
      "    Epoch 4/10 completed in 31.53s\n",
      "    Epoch 5/10 completed in 32.34s\n",
      "    Epoch 6/10 completed in 32.06s\n",
      "    Epoch 7/10 completed in 31.16s\n",
      "    Epoch 8/10 completed in 31.81s\n",
      "    Epoch 9/10 completed in 31.82s\n",
      "    Epoch 10/10 completed in 31.41s\n",
      "      - SalUn  F:91.43% R:89.83% T:81.99%  MIA:0.766\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 33.26s\n",
      "    Epoch 2/10 completed in 31.74s\n",
      "    Epoch 3/10 completed in 30.23s\n",
      "    Epoch 4/10 completed in 29.98s\n",
      "    Epoch 5/10 completed in 30.52s\n",
      "    Epoch 6/10 completed in 31.66s\n",
      "    Epoch 7/10 completed in 31.51s\n",
      "    Epoch 8/10 completed in 31.34s\n",
      "    Epoch 9/10 completed in 31.38s\n",
      "    Epoch 10/10 completed in 31.63s\n",
      "      - Random-label  F:96.47% R:89.46% T:82.90%  MIA:0.753\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 31.52s\n",
      "    Epoch 2/10 completed in 29.91s\n",
      "    Epoch 3/10 completed in 29.78s\n",
      "    Epoch 4/10 completed in 29.87s\n",
      "    Epoch 5/10 completed in 31.48s\n",
      "    Epoch 6/10 completed in 31.89s\n",
      "    Epoch 7/10 completed in 31.02s\n",
      "    Epoch 8/10 completed in 31.32s\n",
      "    Epoch 9/10 completed in 31.76s\n",
      "    Epoch 10/10 completed in 31.22s\n",
      "      - SalUn  F:98.93% R:89.76% T:83.21%  MIA:0.660\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 32.99s\n",
      "    Epoch 2/10 completed in 31.56s\n",
      "    Epoch 3/10 completed in 31.38s\n",
      "    Epoch 4/10 completed in 32.71s\n",
      "    Epoch 5/10 completed in 32.10s\n",
      "    Epoch 6/10 completed in 33.31s\n",
      "    Epoch 7/10 completed in 31.93s\n",
      "    Epoch 8/10 completed in 32.07s\n",
      "    Epoch 9/10 completed in 31.27s\n",
      "    Epoch 10/10 completed in 31.26s\n",
      "      - Random-label  F:98.70% R:89.72% T:82.72%  MIA:0.674\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 32.97s\n",
      "    Epoch 2/10 completed in 31.58s\n",
      "    Epoch 3/10 completed in 33.04s\n",
      "    Epoch 4/10 completed in 32.11s\n",
      "    Epoch 5/10 completed in 31.74s\n",
      "    Epoch 6/10 completed in 31.59s\n",
      "    Epoch 7/10 completed in 33.29s\n",
      "    Epoch 8/10 completed in 32.01s\n",
      "    Epoch 9/10 completed in 31.58s\n",
      "    Epoch 10/10 completed in 31.81s\n",
      "      - SalUn  F:97.43% R:89.89% T:82.40%  MIA:0.611\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 32.92s\n",
      "    Epoch 2/10 completed in 33.59s\n",
      "    Epoch 3/10 completed in 31.52s\n",
      "    Epoch 4/10 completed in 34.91s\n",
      "    Epoch 5/10 completed in 32.02s\n",
      "    Epoch 6/10 completed in 32.67s\n",
      "    Epoch 7/10 completed in 31.94s\n",
      "    Epoch 8/10 completed in 31.79s\n",
      "    Epoch 9/10 completed in 33.37s\n",
      "    Epoch 10/10 completed in 31.65s\n",
      "      - Random-label  F:97.53% R:89.03% T:82.86%  MIA:0.594\n",
      "\n",
      "==================== Starting Run 2/3 ====================\n",
      "\n",
      "[LOADING] original model and ES partitions\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 33.45s\n",
      "    Epoch 2/10 completed in 31.86s\n",
      "    Epoch 3/10 completed in 31.79s\n",
      "    Epoch 4/10 completed in 31.47s\n",
      "    Epoch 5/10 completed in 31.64s\n",
      "    Epoch 6/10 completed in 31.87s\n",
      "    Epoch 7/10 completed in 31.86s\n",
      "    Epoch 8/10 completed in 32.41s\n",
      "    Epoch 9/10 completed in 31.68s\n",
      "    Epoch 10/10 completed in 31.59s\n",
      "      - SalUn  F:91.93% R:90.18% T:82.77%  MIA:0.758\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 33.03s\n",
      "    Epoch 2/10 completed in 31.84s\n",
      "    Epoch 3/10 completed in 31.08s\n",
      "    Epoch 4/10 completed in 31.89s\n",
      "    Epoch 5/10 completed in 31.54s\n",
      "    Epoch 6/10 completed in 31.74s\n",
      "    Epoch 7/10 completed in 31.69s\n",
      "    Epoch 8/10 completed in 32.31s\n",
      "    Epoch 9/10 completed in 33.39s\n",
      "    Epoch 10/10 completed in 31.88s\n",
      "      - Random-label  F:93.97% R:90.00% T:82.95%  MIA:0.730\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 33.04s\n",
      "    Epoch 2/10 completed in 31.47s\n",
      "    Epoch 3/10 completed in 31.75s\n",
      "    Epoch 4/10 completed in 31.68s\n",
      "    Epoch 5/10 completed in 32.14s\n",
      "    Epoch 6/10 completed in 31.49s\n",
      "    Epoch 7/10 completed in 32.07s\n",
      "    Epoch 8/10 completed in 32.08s\n",
      "    Epoch 9/10 completed in 31.61s\n",
      "    Epoch 10/10 completed in 32.01s\n",
      "      - SalUn  F:98.50% R:90.43% T:83.55%  MIA:0.669\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 33.22s\n",
      "    Epoch 2/10 completed in 31.28s\n",
      "    Epoch 3/10 completed in 31.42s\n",
      "    Epoch 4/10 completed in 31.58s\n",
      "    Epoch 5/10 completed in 31.95s\n",
      "    Epoch 6/10 completed in 32.17s\n",
      "    Epoch 7/10 completed in 31.82s\n",
      "    Epoch 8/10 completed in 32.06s\n",
      "    Epoch 9/10 completed in 31.72s\n",
      "    Epoch 10/10 completed in 31.73s\n",
      "      - Random-label  F:98.53% R:88.59% T:82.15%  MIA:0.669\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 33.77s\n",
      "    Epoch 2/10 completed in 32.31s\n",
      "    Epoch 3/10 completed in 32.43s\n",
      "    Epoch 4/10 completed in 31.82s\n",
      "    Epoch 5/10 completed in 31.74s\n",
      "    Epoch 6/10 completed in 31.62s\n",
      "    Epoch 7/10 completed in 31.43s\n",
      "    Epoch 8/10 completed in 31.60s\n",
      "    Epoch 9/10 completed in 31.31s\n",
      "    Epoch 10/10 completed in 31.63s\n",
      "      - SalUn  F:97.93% R:90.52% T:83.23%  MIA:0.620\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 33.32s\n",
      "    Epoch 2/10 completed in 30.23s\n",
      "    Epoch 3/10 completed in 31.73s\n",
      "    Epoch 4/10 completed in 31.53s\n",
      "    Epoch 5/10 completed in 30.35s\n",
      "    Epoch 6/10 completed in 29.75s\n",
      "    Epoch 7/10 completed in 29.77s\n",
      "    Epoch 8/10 completed in 29.76s\n",
      "    Epoch 9/10 completed in 30.31s\n",
      "    Epoch 10/10 completed in 31.47s\n",
      "      - Random-label  F:97.43% R:89.87% T:82.80%  MIA:0.628\n",
      "\n",
      "==================== Starting Run 3/3 ====================\n",
      "\n",
      "[LOADING] original model and ES partitions\n",
      "\n",
      "--- Processing ES Level: Low ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 32.97s\n",
      "    Epoch 2/10 completed in 31.54s\n",
      "    Epoch 3/10 completed in 31.46s\n",
      "    Epoch 4/10 completed in 31.45s\n",
      "    Epoch 5/10 completed in 31.57s\n",
      "    Epoch 6/10 completed in 31.50s\n",
      "    Epoch 7/10 completed in 31.80s\n",
      "    Epoch 8/10 completed in 31.76s\n",
      "    Epoch 9/10 completed in 31.62s\n",
      "    Epoch 10/10 completed in 31.66s\n",
      "      - SalUn  F:93.77% R:90.31% T:82.77%  MIA:0.761\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 33.07s\n",
      "    Epoch 2/10 completed in 34.89s\n",
      "    Epoch 3/10 completed in 32.96s\n",
      "    Epoch 4/10 completed in 31.35s\n",
      "    Epoch 5/10 completed in 33.61s\n",
      "    Epoch 6/10 completed in 33.29s\n",
      "    Epoch 7/10 completed in 31.45s\n",
      "    Epoch 8/10 completed in 31.84s\n",
      "    Epoch 9/10 completed in 31.55s\n",
      "    Epoch 10/10 completed in 32.59s\n",
      "      - Random-label  F:91.90% R:88.40% T:81.51%  MIA:0.738\n",
      "\n",
      "--- Processing ES Level: Medium ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 33.00s\n",
      "    Epoch 2/10 completed in 32.76s\n",
      "    Epoch 3/10 completed in 31.83s\n",
      "    Epoch 4/10 completed in 31.82s\n",
      "    Epoch 5/10 completed in 31.68s\n",
      "    Epoch 6/10 completed in 32.26s\n",
      "    Epoch 7/10 completed in 31.81s\n",
      "    Epoch 8/10 completed in 31.83s\n",
      "    Epoch 9/10 completed in 31.92s\n",
      "    Epoch 10/10 completed in 31.31s\n",
      "      - SalUn  F:98.93% R:90.31% T:83.51%  MIA:0.695\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 33.73s\n",
      "    Epoch 2/10 completed in 31.74s\n",
      "    Epoch 3/10 completed in 33.08s\n",
      "    Epoch 4/10 completed in 31.61s\n",
      "    Epoch 5/10 completed in 31.31s\n",
      "    Epoch 6/10 completed in 31.83s\n",
      "    Epoch 7/10 completed in 31.69s\n",
      "    Epoch 8/10 completed in 31.52s\n",
      "    Epoch 9/10 completed in 31.59s\n",
      "    Epoch 10/10 completed in 31.77s\n",
      "      - Random-label  F:99.03% R:89.74% T:83.51%  MIA:0.668\n",
      "\n",
      "--- Processing ES Level: High ES ---\n",
      "\n",
      "Applying and evaluating unlearning methods...\n",
      "    > [TRAINING] SalUn\n",
      "    Epoch 1/10 completed in 33.39s\n",
      "    Epoch 2/10 completed in 33.61s\n",
      "    Epoch 3/10 completed in 31.47s\n",
      "    Epoch 4/10 completed in 31.78s\n",
      "    Epoch 5/10 completed in 31.40s\n",
      "    Epoch 6/10 completed in 31.34s\n",
      "    Epoch 7/10 completed in 32.67s\n",
      "    Epoch 8/10 completed in 31.43s\n",
      "    Epoch 9/10 completed in 31.03s\n",
      "    Epoch 10/10 completed in 32.37s\n",
      "      - SalUn  F:98.60% R:90.18% T:83.27%  MIA:0.643\n",
      "    > [TRAINING] Random-label\n",
      "    Epoch 1/10 completed in 32.84s\n",
      "    Epoch 2/10 completed in 31.66s\n",
      "    Epoch 3/10 completed in 31.86s\n",
      "    Epoch 4/10 completed in 31.67s\n",
      "    Epoch 5/10 completed in 31.76s\n",
      "    Epoch 6/10 completed in 33.30s\n",
      "    Epoch 7/10 completed in 33.20s\n",
      "    Epoch 8/10 completed in 32.34s\n",
      "    Epoch 9/10 completed in 31.53s\n",
      "    Epoch 10/10 completed in 31.20s\n",
      "      - Random-label  F:98.17% R:89.65% T:82.91%  MIA:0.615\n",
      "\n",
      "==================== Final Results ====================\n",
      "\n",
      "--- Results for Low ES ---\n",
      "      Method     Forget Acc     Retain Acc       Test Acc           MIA\n",
      "       SalUn 92.378 ± 3.052 90.106 ± 0.608 82.510 ± 1.119 0.762 ± 0.010\n",
      "Random-label 94.111 ± 5.681 89.289 ± 2.027 82.453 ± 2.030 0.740 ± 0.029\n",
      "\n",
      "--- Results for Medium ES ---\n",
      "      Method     Forget Acc     Retain Acc       Test Acc           MIA\n",
      "       SalUn 98.789 ± 0.621 90.166 ± 0.895 83.423 ± 0.462 0.675 ± 0.044\n",
      "Random-label 98.756 ± 0.632 89.348 ± 1.640 82.793 ± 1.697 0.670 ± 0.008\n",
      "\n",
      "--- Results for High ES ---\n",
      "      Method     Forget Acc     Retain Acc       Test Acc           MIA\n",
      "       SalUn 97.989 ± 1.454 90.196 ± 0.791 82.967 ± 1.220 0.625 ± 0.041\n",
      "Random-label 97.711 ± 0.988 89.519 ± 1.084 82.857 ± 0.137 0.612 ± 0.043\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset, ConcatDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import time, copy, itertools, os, random\n",
    "\n",
    "# ===================================================================\n",
    "# 0. 재현성(Reproducibility)을 위한 시드 설정\n",
    "# ===================================================================\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Global random seed set to {SEED}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 1. 실험 환경 설정\n",
    "# ===================================================================\n",
    "CONFIG = {\n",
    "    \"run_training\": True,\n",
    "    \"model_save_dir\": \"saved_models\",\n",
    "    \"num_runs\": 3,\n",
    "    \"epochs\": 30,\n",
    "    \"unlearn_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"lr\": 0.1,\n",
    "    \"unlearn_lr\": 0.01, # 수정된 unlearning을 위한 학습률\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 5e-4,\n",
    "    \"forget_set_size\": 3000,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"salun_sparsity\": 0.5,\n",
    "}\n",
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "\n",
    "# ===================================================================\n",
    "# 2. 모델 및 데이터 헬퍼 (기존과 동일)\n",
    "# ===================================================================\n",
    "def get_model():\n",
    "    return models.resnet18(weights=None, num_classes=10).to(CONFIG[\"device\"])\n",
    "\n",
    "def train_model(model, train_loader, epochs, lr, is_unlearning=False):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = optim.SGD(model.parameters(), lr=lr,\n",
    "                    momentum=CONFIG[\"momentum\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "    sched = None if is_unlearning else optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    model.train()\n",
    "    for ep in range(epochs):\n",
    "        t0 = time.time()\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n",
    "            opt.zero_grad()\n",
    "            loss = crit(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        if sched: sched.step()\n",
    "        print(f\"    Epoch {ep+1}/{epochs} completed in {time.time()-t0:.2f}s\")\n",
    "\n",
    "def evaluate_model(model, loader):\n",
    "    model.eval()\n",
    "    tot = corr = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(CONFIG[\"device\"]), y.to(CONFIG[\"device\"])\n",
    "            pred = model(x).argmax(1)\n",
    "            tot += y.size(0)\n",
    "            corr += (pred == y).sum().item()\n",
    "    return 100 * corr / tot\n",
    "\n",
    "# ===================================================================\n",
    "# 3. 언러닝 알고리즘 (수정된 버전)\n",
    "# ===================================================================\n",
    "class RelabelDataset(Dataset):\n",
    "    def __init__(self, original_dataset, num_classes=10):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.num_classes = num_classes\n",
    "        self.new_labels = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index not in self.new_labels:\n",
    "            _, original_label = self.original_dataset[index]\n",
    "            new_label = torch.randint(0, self.num_classes, (1,)).item()\n",
    "            while new_label == original_label:\n",
    "                new_label = torch.randint(0, self.num_classes, (1,)).item()\n",
    "            self.new_labels[index] = new_label\n",
    "\n",
    "        image, _ = self.original_dataset[index]\n",
    "        return image, self.new_labels[index]\n",
    "\n",
    "# Random Label Unlearning 수정된 함수\n",
    "def unlearn_random_label_fixed(orig, retain_set, forget_set, cfg):\n",
    "    model = copy.deepcopy(orig)\n",
    "    \n",
    "    # Forget 데이터셋의 레이블을 무작위로 변경\n",
    "    relabeled_forget_set = RelabelDataset(forget_set)\n",
    "    \n",
    "    # Retain 데이터셋과 Relabeled Forget 데이터셋을 합침\n",
    "    combined_dataset = ConcatDataset([retain_set, relabeled_forget_set])\n",
    "    \n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "    # 합쳐진 데이터로 모델을 학습\n",
    "    train_model(model, combined_loader, cfg[\"unlearn_epochs\"], cfg[\"unlearn_lr\"], is_unlearning=True)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# SalUn Unlearning 수정된 함수\n",
    "def unlearn_salun_fixed(orig, retain_set, forget_set, cfg):\n",
    "    model = copy.deepcopy(orig)\n",
    "    \n",
    "    # 1. Saliency 계산 (기존과 동일)\n",
    "    saliency = [torch.zeros_like(p) for p in model.parameters()]\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    forget_loader_for_saliency = DataLoader(forget_set, batch_size=cfg[\"batch_size\"])\n",
    "    \n",
    "    model.eval() # Saliency 계산 시에는 eval 모드\n",
    "    for x, y in forget_loader_for_saliency:\n",
    "        x, y = x.to(cfg[\"device\"]), y.to(cfg[\"device\"])\n",
    "        model.zero_grad()\n",
    "        loss = crit(model(x), y)\n",
    "        loss.backward()\n",
    "        for i, p in enumerate(model.parameters()):\n",
    "            if p.grad is not None:\n",
    "                saliency[i] += p.grad.abs()\n",
    "    \n",
    "    flat_saliency = torch.cat([s.flatten() for s in saliency])\n",
    "    k = int(len(flat_saliency) * cfg[\"salun_sparsity\"])\n",
    "    threshold, _ = torch.kthvalue(flat_saliency, k)\n",
    "    masks = [(s > threshold).float() for s in saliency]\n",
    "\n",
    "    # 2. Masked Retraining (수정된 부분)\n",
    "    relabeled_forget_set = RelabelDataset(forget_set)\n",
    "    combined_dataset = ConcatDataset([retain_set, relabeled_forget_set])\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=cfg[\"batch_size\"], shuffle=True)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=cfg[\"unlearn_lr\"], momentum=cfg[\"momentum\"])\n",
    "    \n",
    "    model.train() # 학습을 위해 train 모드로 변경\n",
    "    for ep in range(cfg[\"unlearn_epochs\"]):\n",
    "        t0 = time.time()\n",
    "        for x, y in combined_loader:\n",
    "            x, y = x.to(cfg[\"device\"]), y.to(cfg[\"device\"])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = crit(model(x), y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # 그래디언트에 마스크 적용\n",
    "            for i, p in enumerate(model.parameters()):\n",
    "                if p.grad is not None:\n",
    "                    p.grad *= masks[i]\n",
    "            \n",
    "            optimizer.step()\n",
    "        print(f\"    Epoch {ep+1}/{cfg['unlearn_epochs']} completed in {time.time()-t0:.2f}s\")\n",
    "            \n",
    "    return model\n",
    "\n",
    "# ===================================================================\n",
    "# 4. MIA Score 계산 (수정된 최종 버전)\n",
    "# ===================================================================\n",
    "class black_box_benchmarks:\n",
    "    def __init__(self, s_tr, s_te, t_tr, t_te, num_classes):\n",
    "        self.k = num_classes\n",
    "        self.s_tr_out, self.s_tr_lab = s_tr\n",
    "        self.s_te_out, self.s_te_lab = s_te\n",
    "        self.t_tr_out, self.t_tr_lab = t_tr\n",
    "        self.t_te_out, self.t_te_lab = t_te\n",
    "\n",
    "        self.s_tr_corr = (self.s_tr_out.argmax(1) == self.s_tr_lab).astype(int)\n",
    "        self.s_te_corr = (self.s_te_out.argmax(1) == self.s_te_lab).astype(int)\n",
    "        self.t_tr_corr = (self.t_tr_out.argmax(1) == self.t_tr_lab).astype(int)\n",
    "        self.t_te_corr = (self.t_te_out.argmax(1) == self.t_te_lab).astype(int)\n",
    "\n",
    "        self.s_tr_conf = self.s_tr_out[np.arange(len(self.s_tr_lab)), self.s_tr_lab]\n",
    "        self.s_te_conf = self.s_te_out[np.arange(len(self.s_te_lab)), self.s_te_lab]\n",
    "        self.t_tr_conf = self.t_tr_out[np.arange(len(self.t_tr_lab)), self.t_tr_lab]\n",
    "        self.t_te_conf = self.t_te_out[np.arange(len(self.t_te_lab)), self.t_te_lab]\n",
    "\n",
    "        self.s_tr_entr = self._entr(self.s_tr_out)\n",
    "        self.s_te_entr = self._entr(self.s_te_out)\n",
    "        self.t_tr_entr = self._entr(self.t_tr_out)\n",
    "        self.t_te_entr = self._entr(self.t_te_out)\n",
    "\n",
    "        self.s_tr_m_entr = self._m_entr(self.s_tr_out, self.s_tr_lab)\n",
    "        self.s_te_m_entr = self._m_entr(self.s_te_out, self.s_te_lab)\n",
    "        self.t_tr_m_entr = self._m_entr(self.t_tr_out, self.t_tr_lab)\n",
    "        self.t_te_m_entr = self._m_entr(self.t_te_out, self.t_te_lab)\n",
    "\n",
    "    def _log(self, p, eps=1e-30):\n",
    "        return -np.log(np.maximum(p, eps))\n",
    "\n",
    "    def _entr(self, p):\n",
    "        return (p * self._log(p)).sum(1)\n",
    "\n",
    "    def _m_entr(self, p, l):\n",
    "        lp = self._log(p)\n",
    "        rp = 1 - p\n",
    "        lrp = self._log(rp)\n",
    "        mp = p.copy()\n",
    "        mp[np.arange(l.size), l] = rp[np.arange(l.size), l]\n",
    "        mlp = lrp.copy()\n",
    "        mlp[np.arange(l.size), l] = lp[np.arange(l.size), l]\n",
    "        return (mp * mlp).sum(1)\n",
    "\n",
    "    def _thre(self, tr, te):\n",
    "        vals = np.concatenate((tr, te))\n",
    "        best_acc = 0\n",
    "        best_t = 0\n",
    "        for v in vals:\n",
    "            acc = 0.5 * ((tr >= v).mean() + (te < v).mean())\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_t = v\n",
    "        return best_t\n",
    "\n",
    "    def _via_corr(self):\n",
    "        return 0.5 * (self.t_tr_corr.mean() + (1 - self.t_te_corr).mean())\n",
    "\n",
    "    def _via_feat(self, tr, te, Ttr, Tte):\n",
    "        t_mem = 0\n",
    "        t_non = 0\n",
    "        if len(Ttr) == 0 or len(Tte) == 0:\n",
    "            return 0.5 # 분모가 0이 되는 경우 방지\n",
    "\n",
    "        for c in range(self.k):\n",
    "            class_tr = tr[self.s_tr_lab == c]\n",
    "            class_te = te[self.s_te_lab == c]\n",
    "            if len(class_tr) == 0 or len(class_te) == 0:\n",
    "                continue\n",
    "            \n",
    "            thr = self._thre(class_tr, class_te)\n",
    "            t_mem += (Ttr[self.t_tr_lab == c] >= thr).sum()\n",
    "            t_non += (Tte[self.t_te_lab == c] < thr).sum()\n",
    "        \n",
    "        return 0.5 * (t_mem / len(Ttr) + t_non / len(Tte))\n",
    "\n",
    "    def run(self):\n",
    "        return {\n",
    "            \"correctness\": self._via_corr(),\n",
    "            \"confidence\": self._via_feat(self.s_tr_conf, self.s_te_conf, self.t_tr_conf, self.t_te_conf),\n",
    "            \"entropy\": self._via_feat(-self.s_tr_entr, -self.s_te_entr, -self.t_tr_entr, -self.t_te_entr),\n",
    "            \"m_entropy\": self._via_feat(-self.s_tr_m_entr, -self.s_te_m_entr, -self.t_tr_m_entr, -self.t_te_m_entr)\n",
    "        }\n",
    "\n",
    "\n",
    "def collect_performance(loader, model, device):\n",
    "    outs, labs = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            outs.append(F.softmax(model(x),1).cpu())\n",
    "            labs.append(y.cpu())\n",
    "    if not outs:\n",
    "        return np.array([]), np.array([])\n",
    "    return torch.cat(outs).numpy(), torch.cat(labs).numpy()\n",
    "\n",
    "\n",
    "def calculate_mia_score(model, retain_loader_train, retain_loader_test,\n",
    "                        forget_loader, test_loader):\n",
    "    s_tr = collect_performance(retain_loader_train, model, CONFIG[\"device\"])\n",
    "    s_te = collect_performance(test_loader,         model, CONFIG[\"device\"])\n",
    "    t_tr = collect_performance(retain_loader_test,  model, CONFIG[\"device\"])\n",
    "    t_te = collect_performance(forget_loader,       model, CONFIG[\"device\"])\n",
    "\n",
    "    # 데이터가 비어있는 경우 MIA 점수를 0.5 (무작위 추측)로 반환\n",
    "    if any(arr.size == 0 for arr in [s_tr[0], s_te[0], t_tr[0], t_te[0]]):\n",
    "        return 0.5\n",
    "        \n",
    "    mia = black_box_benchmarks(s_tr, s_te, t_tr, t_te, 10).run()\n",
    "    return mia[\"confidence\"]\n",
    "\n",
    "# ===================================================================\n",
    "# 5. 메인 실험 루프 (수정됨)\n",
    "# ===================================================================\n",
    "def main():\n",
    "    sd = CONFIG[\"model_save_dir\"]\n",
    "    os.makedirs(sd, exist_ok=True)\n",
    "    tf_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32,4), transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),\n",
    "    ])\n",
    "    tf_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914,0.4822,0.4465),(0.2023,0.1994,0.2010)),\n",
    "    ])\n",
    "    \n",
    "    # 데이터셋 경로를 로컬 환경에 맞게 수정하세요.\n",
    "    DATASET = \"../../data\"\n",
    "    \n",
    "    tr_ds = datasets.CIFAR10(root=DATASET, train=True, download=True, transform=tf_train)\n",
    "    te_ds = datasets.CIFAR10(root=DATASET, train=False, download=True, transform=tf_test)\n",
    "\n",
    "    g = torch.Generator(); g.manual_seed(SEED+9999)\n",
    "    te_loader = DataLoader(te_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                           worker_init_fn=lambda i:set_seed(SEED+9999+i), generator=g)\n",
    "\n",
    "    # 실험할 알고리즘만 정의\n",
    "    methods = [\"SalUn\", \"Random-label\"]\n",
    "    res = {m:{es:{\"F\":[],\"R\":[],\"T\":[],\"M\":[]} for es in [\"Low ES\",\"Medium ES\",\"High ES\"]} for m in methods}\n",
    "\n",
    "    for run in range(CONFIG[\"num_runs\"]):\n",
    "        print(f\"\\n{'='*20} Starting Run {run+1}/{CONFIG['num_runs']} {'='*20}\")\n",
    "        orig = get_model()\n",
    "        orig_pth = f\"{sd}/run_{run}_original_model.pth\"\n",
    "        part_pth = f\"{sd}/run_{run}_es_partitions.pth\"\n",
    "\n",
    "        if not os.path.exists(orig_pth) or not os.path.exists(part_pth):\n",
    "            print(f\"Error: Model or partition file not found for run {run}\")\n",
    "            print(f\"Searched for: {orig_pth} and {part_pth}\")\n",
    "            print(\"Please run the original full script first to generate these files.\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n[LOADING] original model and ES partitions\")\n",
    "        orig.load_state_dict(torch.load(orig_pth, map_location=CONFIG[\"device\"]))\n",
    "        parts = torch.load(part_pth)\n",
    "\n",
    "        for es, forget_idx in parts.items():\n",
    "            print(f\"\\n--- Processing ES Level: {es} ---\")\n",
    "            all_idx = np.arange(len(tr_ds))\n",
    "            retain_idx = np.setdiff1d(all_idx, forget_idx, assume_unique=True)\n",
    "            r_set, f_set = Subset(tr_ds, retain_idx), Subset(tr_ds, forget_idx)\n",
    "\n",
    "            # DataLoaders\n",
    "            g_r = torch.Generator(); g_r.manual_seed(SEED+run+ord(es[0]))\n",
    "            retain_loader = DataLoader(r_set, batch_size=CONFIG[\"batch_size\"], shuffle=True,\n",
    "                                       worker_init_fn=lambda i:set_seed(SEED+run+ord(es[0])+i), generator=g_r)\n",
    "            g_re = torch.Generator(); g_re.manual_seed(SEED+run+ord(es[0])+1000)\n",
    "            retain_eval = DataLoader(r_set, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                     worker_init_fn=lambda i:set_seed(SEED+run+ord(es[0])+1000+i), generator=g_re)\n",
    "            g_f = torch.Generator(); g_f.manual_seed(SEED+run+ord(es[0])+2000)\n",
    "            forget_loader = DataLoader(f_set, batch_size=CONFIG[\"batch_size\"], shuffle=False,\n",
    "                                       worker_init_fn=lambda i:set_seed(SEED+run+ord(es[0])+2000+i), generator=g_f)\n",
    "            \n",
    "            unlearn = {\n",
    "                \"SalUn\"       : lambda: unlearn_salun_fixed(orig, r_set, f_set, CONFIG),\n",
    "                \"Random-label\": lambda: unlearn_random_label_fixed(orig, r_set, f_set, CONFIG),\n",
    "            }\n",
    "\n",
    "            print(\"\\nApplying and evaluating unlearning methods...\")\n",
    "            for m_name, fn in unlearn.items():\n",
    "                # 수정된 함수 이름으로 저장 경로 변경\n",
    "                upth = f\"{sd}/run_{run}_{es.replace(' ','')}_{m_name}_fixed_unlearned.pth\"\n",
    "                \n",
    "                if os.path.exists(upth):\n",
    "                    print(f\"    > [LOADING] {m_name}\"); u_model = get_model(); u_model.load_state_dict(torch.load(upth,map_location=CONFIG[\"device\"]))\n",
    "                else:\n",
    "                    print(f\"    > [TRAINING] {m_name}\"); u_model = fn(); torch.save(u_model.state_dict(), upth)\n",
    "\n",
    "                u_f = evaluate_model(u_model, forget_loader)\n",
    "                u_r = evaluate_model(u_model, retain_eval)\n",
    "                u_t = evaluate_model(u_model, te_loader)\n",
    "                u_m = calculate_mia_score(u_model, retain_loader, retain_eval, forget_loader, te_loader)\n",
    "                print(f\"      - {m_name}  F:{u_f:.2f}% R:{u_r:.2f}% T:{u_t:.2f}%  MIA:{u_m:.3f}\")\n",
    "\n",
    "                res[m_name][es][\"F\"].append(u_f); res[m_name][es][\"R\"].append(u_r)\n",
    "                res[m_name][es][\"T\"].append(u_t); res[m_name][es][\"M\"].append(u_m)\n",
    "\n",
    "    # ===================================================================\n",
    "    # 6. 결과 정리\n",
    "    # ===================================================================\n",
    "    print(f\"\\n{'='*20} Final Results {'='*20}\")\n",
    "    def fmt(xs):\n",
    "        xs=np.array(xs); mu=xs.mean()\n",
    "        # SEM 계산 시 샘플 수가 1개일 경우를 대비\n",
    "        if len(xs) < 2:\n",
    "            return f\"{mu:.3f}\"\n",
    "        sem = stats.sem(xs)\n",
    "        # 95% 신뢰구간을 위한 t-분포 값\n",
    "        t_val = stats.t.ppf(0.975, len(xs) - 1) if len(xs) > 1 else 0\n",
    "        return f\"{mu:.3f} ± {(sem * t_val):.3f}\"\n",
    "\n",
    "    for es in [\"Low ES\",\"Medium ES\",\"High ES\"]:\n",
    "        print(f\"\\n--- Results for {es} ---\")\n",
    "        rows=[]\n",
    "        for m in methods:\n",
    "            if not res[m][es][\"F\"]: # 결과가 비어있으면 건너뛰기\n",
    "                continue\n",
    "            row={\"Method\":m,\n",
    "                 \"Forget Acc\":fmt(res[m][es][\"F\"]),\n",
    "                 \"Retain Acc\":fmt(res[m][es][\"R\"]),\n",
    "                 \"Test Acc\"  :fmt(res[m][es][\"T\"]),\n",
    "                 \"MIA\"       :fmt(res[m][es][\"M\"])}\n",
    "            rows.append(row)\n",
    "        if rows:\n",
    "            print(pd.DataFrame(rows).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
