nohup: ignoring input
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
[TRAIN] Original Model
    Epoch 1/1  37.67s
Defining forget set: 5000 random samples.
Partitioning 5000 forget samples using 'memorization' method...
Partition sizes: [1666, 1666, 1668]

===== Running Method: FT =====
[TRAIN] Retrain stage 1 on 48334 samples
    Epoch 1/1  38.07s
[UNLEARN] FT stage 1 (|forget_total|=1666)
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch #0, Learning rate: 0.001
Epoch: [0][99/378]	Loss 1.6456 (1.6171)	Accuracy 39.062 (39.344)	Time 9.29
Epoch: [0][199/378]	Loss 1.5907 (1.6013)	Accuracy 39.062 (40.125)	Time 9.30
Epoch: [0][299/378]	Loss 1.5683 (1.5920)	Accuracy 39.062 (40.578)	Time 9.23
train_accuracy 40.735
one epoch duration:35.00436282157898
        FT | S1 | Ftot=1666 | Ret F/R/T: 65.01/43.00/43.37 | Unl F/R/T: 64.89/43.18/44.37 | ΔF:-0.12 ΔR: 0.18 ΔT: 1.00 | MIA:0.4645 PredDiff:43.21%
[TRAIN] Retrain stage 2 on 46668 samples
    Epoch 1/1  37.67s
[UNLEARN] FT stage 2 (|forget_total|=3332)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/365]	Loss 1.5719 (1.5584)	Accuracy 41.406 (41.461)	Time 10.54
Epoch: [0][199/365]	Loss 1.6176 (1.5604)	Accuracy 41.406 (41.910)	Time 10.70
Epoch: [0][299/365]	Loss 1.5917 (1.5567)	Accuracy 39.844 (41.979)	Time 9.95
train_accuracy 41.990
one epoch duration:38.044201374053955
        FT | S2 | Ftot=3332 | Ret F/R/T: 44.00/36.22/37.30 | Unl F/R/T: 54.50/44.09/45.75 | ΔF:+10.50 ΔR: 7.87 ΔT: 8.45 | MIA:0.4771 PredDiff:46.95%
[TRAIN] Retrain stage 3 on 45000 samples
    Epoch 1/1  35.87s
[UNLEARN] FT stage 3 (|forget_total|=5000)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/352]	Loss 1.4716 (1.5264)	Accuracy 45.312 (43.039)	Time 10.46
Epoch: [0][199/352]	Loss 1.5449 (1.5249)	Accuracy 41.406 (43.191)	Time 10.44
Epoch: [0][299/352]	Loss 1.4070 (1.5242)	Accuracy 42.188 (42.943)	Time 10.37
train_accuracy 43.029
one epoch duration:36.1336886882782
        FT | S3 | Ftot=5000 | Ret F/R/T: 40.52/41.63/41.28 | Unl F/R/T: 43.98/45.37/45.82 | ΔF:+3.46 ΔR: 3.74 ΔT: 4.54 | MIA:0.5010 PredDiff:34.63%

===== Running Method: FT_l1 =====
[TRAIN] Retrain stage 1 on 48334 samples
    Epoch 1/1  38.15s
[UNLEARN] FT_l1 stage 1 (|forget_total|=1666)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/378]	Loss 1.5414 (1.6125)	Accuracy 41.406 (39.609)	Time 11.18
Epoch: [0][199/378]	Loss 1.3987 (1.6048)	Accuracy 51.562 (40.145)	Time 10.70
Epoch: [0][299/378]	Loss 1.6346 (1.5879)	Accuracy 39.844 (40.667)	Time 10.47
train_accuracy 40.830
one epoch duration:40.66604208946228
     FT_l1 | S1 | Ftot=1666 | Ret F/R/T: 65.31/43.01/44.02 | Unl F/R/T: 64.05/43.12/44.36 | ΔF:-1.26 ΔR: 0.11 ΔT: 0.34 | MIA:0.4619 PredDiff:39.65%
[TRAIN] Retrain stage 2 on 46668 samples
    Epoch 1/1  36.79s
[UNLEARN] FT_l1 stage 2 (|forget_total|=3332)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/365]	Loss 1.5375 (1.5694)	Accuracy 41.406 (41.086)	Time 9.86
Epoch: [0][199/365]	Loss 1.6480 (1.5655)	Accuracy 35.938 (41.355)	Time 10.61
Epoch: [0][299/365]	Loss 1.6863 (1.5559)	Accuracy 34.375 (41.992)	Time 10.81
train_accuracy 41.952
one epoch duration:38.41436696052551
     FT_l1 | S2 | Ftot=3332 | Ret F/R/T: 51.14/40.68/41.78 | Unl F/R/T: 54.53/44.07/45.60 | ΔF:+3.39 ΔR: 3.39 ΔT: 3.82 | MIA:0.4831 PredDiff:41.05%
[TRAIN] Retrain stage 3 on 45000 samples
    Epoch 1/1  36.22s
[UNLEARN] FT_l1 stage 3 (|forget_total|=5000)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/352]	Loss 1.4497 (1.5421)	Accuracy 50.000 (42.617)	Time 11.09
Epoch: [0][199/352]	Loss 1.5909 (1.5393)	Accuracy 42.188 (43.004)	Time 10.68
Epoch: [0][299/352]	Loss 1.4066 (1.5334)	Accuracy 49.219 (43.188)	Time 10.98
train_accuracy 43.218
one epoch duration:38.31295990943909
     FT_l1 | S3 | Ftot=5000 | Ret F/R/T: 42.58/43.95/43.33 | Unl F/R/T: 44.54/45.63/46.08 | ΔF:+1.96 ΔR: 1.68 ΔT: 2.75 | MIA:0.5043 PredDiff:37.49%

===== Running Method: GA =====
[TRAIN] Retrain stage 1 on 48334 samples
    Epoch 1/1  38.01s
[UNLEARN] GA stage 1 (|forget_total|=1666)
Epoch #0, Learning rate: 0.001
len(train_loader):  14
train_accuracy 53.301
one epoch duration:1.3485100269317627
        GA | S1 | Ftot=1666 | Ret F/R/T: 65.07/42.70/43.72 | Unl F/R/T: 55.16/37.71/38.94 | ΔF:-9.90 ΔR: 4.98 ΔT: 4.78 | MIA:0.4793 PredDiff:57.44%
[TRAIN] Retrain stage 2 on 46668 samples
    Epoch 1/1  36.58s
[UNLEARN] GA stage 2 (|forget_total|=3332)
Epoch #0, Learning rate: 0.001
len(train_loader):  27
train_accuracy 42.257
one epoch duration:2.510957956314087
        GA | S2 | Ftot=3332 | Ret F/R/T: 49.82/39.79/40.68 | Unl F/R/T: 43.58/36.11/36.51 | ΔF:-6.24 ΔR: 3.69 ΔT: 4.17 | MIA:0.4751 PredDiff:62.74%
[TRAIN] Retrain stage 3 on 45000 samples
    Epoch 1/1  34.21s
[UNLEARN] GA stage 3 (|forget_total|=5000)
Epoch #0, Learning rate: 0.001
len(train_loader):  40
train_accuracy 32.080
one epoch duration:3.7631261348724365
        GA | S3 | Ftot=5000 | Ret F/R/T: 39.74/39.96/40.46 | Unl F/R/T: 33.38/34.14/34.05 | ΔF:-6.36 ΔR: 5.82 ΔT: 6.41 | MIA:0.4994 PredDiff:64.96%

===== Running Method: NG =====
[TRAIN] Retrain stage 1 on 48334 samples
    Epoch 1/1  37.05s
[UNLEARN] NG stage 1 (|forget_total|=1666)
Epoch #0, Learning rate: 0.001
len(r_loader): 378, len(f_loader): 14
Epoch: [0][99/378]	Loss -0.1182 (-0.1573)	Accuracy 17.188 (26.000)	Time 12.63
Epoch: [0][199/378]	Loss -0.1901 (-0.4349)	Accuracy 21.875 (20.133)	Time 12.33
Epoch: [0][299/378]	Loss -3.3561 (-0.8312)	Accuracy 17.188 (18.234)	Time 12.55
train_accuracy 17.135
one epoch duration:47.22970128059387
        NG | S1 | Ftot=1666 | Ret F/R/T: 63.03/42.15/42.52 | Unl F/R/T: 12.00/14.45/14.26 | ΔF:-51.02 ΔR:27.70 ΔT:28.26 | MIA:0.4873 PredDiff:76.99%
[TRAIN] Retrain stage 2 on 46668 samples
    Epoch 1/1  36.58s
[UNLEARN] NG stage 2 (|forget_total|=3332)
Epoch #0, Learning rate: 0.001
len(r_loader): 365, len(f_loader): 27
Epoch: [0][99/365]	Loss 0.8502 (-0.5897)	Accuracy 16.406 (14.438)	Time 13.33
Epoch: [0][199/365]	Loss -4.7420 (-0.9550)	Accuracy 15.625 (14.504)	Time 11.56
Epoch: [0][299/365]	Loss -1.4345 (-1.3141)	Accuracy 8.594 (14.464)	Time 11.58
train_accuracy 14.284
one epoch duration:43.97427845001221
        NG | S2 | Ftot=3332 | Ret F/R/T: 52.55/42.77/42.75 | Unl F/R/T: 11.82/13.94/13.84 | ΔF:-40.73 ΔR:28.83 ΔT:28.91 | MIA:0.5000 PredDiff:91.20%
[TRAIN] Retrain stage 3 on 45000 samples
    Epoch 1/1  33.60s
[UNLEARN] NG stage 3 (|forget_total|=5000)
Epoch #0, Learning rate: 0.001
len(r_loader): 352, len(f_loader): 40
Epoch: [0][99/352]	Loss -0.4438 (-0.4567)	Accuracy 10.938 (13.508)	Time 14.14
Epoch: [0][199/352]	Loss -4.5412 (-0.5430)	Accuracy 14.844 (13.539)	Time 12.56
Epoch: [0][299/352]	Loss 0.8410 (-0.5818)	Accuracy 13.281 (13.456)	Time 12.69
train_accuracy 13.533
one epoch duration:45.936490058898926
        NG | S3 | Ftot=5000 | Ret F/R/T: 39.94/41.69/41.20 | Unl F/R/T: 14.54/14.28/14.25 | ΔF:-25.40 ΔR:27.41 ΔT:26.95 | MIA:0.5017 PredDiff:88.02%

===== Running Method: RL =====
[TRAIN] Retrain stage 1 on 48334 samples
    Epoch 1/1  38.16s
[UNLEARN] RL stage 1 (|forget_total|=1666)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/392]	Loss 1.7435 (1.7011)	Accuracy 39.062 (37.427)	Time 8.24
Epoch: [0][199/392]	Loss 1.6459 (1.6742)	Accuracy 40.625 (38.659)	Time 9.59
Epoch: [0][299/392]	Loss 1.5148 (1.6618)	Accuracy 45.312 (39.147)	Time 9.51
Epoch: [0][399/392]	Loss 1.6594 (1.6533)	Accuracy 42.188 (39.524)	Time 9.59
one epoch duration:37.52798008918762
        RL | S1 | Ftot=1666 | Ret F/R/T: 65.67/40.90/42.01 | Unl F/R/T: 65.55/43.71/45.03 | ΔF:-0.12 ΔR: 2.80 ΔT: 3.02 | MIA:0.4419 PredDiff:40.12%
[TRAIN] Retrain stage 2 on 46668 samples
    Epoch 1/1  36.95s
[UNLEARN] RL stage 2 (|forget_total|=3332)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/392]	Loss 1.6908 (1.6837)	Accuracy 42.969 (39.587)	Time 6.97
Epoch: [0][199/392]	Loss 1.6331 (1.6786)	Accuracy 40.625 (39.595)	Time 9.54
Epoch: [0][299/392]	Loss 1.5351 (1.6760)	Accuracy 46.875 (39.789)	Time 9.55
Epoch: [0][399/392]	Loss 1.6303 (1.6753)	Accuracy 42.188 (39.871)	Time 9.52
one epoch duration:37.39924430847168
        RL | S2 | Ftot=3332 | Ret F/R/T: 50.69/41.80/42.31 | Unl F/R/T: 54.74/44.31/45.77 | ΔF:+4.05 ΔR: 2.50 ΔT: 3.46 | MIA:0.4758 PredDiff:42.33%
[TRAIN] Retrain stage 3 on 45000 samples
    Epoch 1/1  34.04s
[UNLEARN] RL stage 3 (|forget_total|=5000)
Epoch #0, Learning rate: 0.001
Epoch: [0][99/392]	Loss 1.5682 (1.7099)	Accuracy 41.406 (39.180)	Time 5.88
Epoch: [0][199/392]	Loss 1.6828 (1.7020)	Accuracy 42.188 (39.580)	Time 10.55
Epoch: [0][299/392]	Loss 1.6388 (1.7064)	Accuracy 42.188 (39.420)	Time 10.38
Epoch: [0][399/392]	Loss 1.6933 (1.7033)	Accuracy 42.188 (39.564)	Time 10.47
one epoch duration:40.61022067070007
        RL | S3 | Ftot=5000 | Ret F/R/T: 39.02/40.51/40.19 | Unl F/R/T: 44.26/45.34/46.01 | ΔF:+5.24 ΔR: 4.83 ΔT: 5.82 | MIA:0.4998 PredDiff:42.45%

===== Running Method: Wfisher =====
[TRAIN] Retrain stage 1 on 48334 samples
    Epoch 1/1  37.00s
[UNLEARN] Wfisher stage 1 (|forget_total|=1666)

  0%|          | 0/14 [00:00<?, ?it/s]
100%|██████████| 14/14 [00:01<00:00, 10.73it/s]

  0%|          | 0/378 [00:00<?, ?it/s]
100%|██████████| 378/378 [00:36<00:00, 10.35it/s]

  0%|          | 0/48334 [00:00<?, ?it/s]
  2%|▏         | 1001/48334 [00:23<18:14, 43.24it/s]
   Wfisher | S1 | Ftot=1666 | Ret F/R/T: 61.52/41.01/41.56 | Unl F/R/T: 58.46/39.51/40.37 | ΔF:-3.06 ΔR: 1.50 ΔT: 1.19 | MIA:0.4610 PredDiff:54.13%
[TRAIN] Retrain stage 2 on 46668 samples
    Epoch 1/1  36.58s
[UNLEARN] Wfisher stage 2 (|forget_total|=3332)

  0%|          | 0/27 [00:00<?, ?it/s]
  7%|▋         | 2/27 [00:00<00:02, 10.10it/s]
 15%|█▍        | 4/27 [00:00<00:02, 10.14it/s]
 22%|██▏       | 6/27 [00:00<00:02, 10.18it/s]
 30%|██▉       | 8/27 [00:00<00:01, 10.06it/s]
 37%|███▋      | 10/27 [00:00<00:01, 10.14it/s]
 44%|████▍     | 12/27 [00:01<00:01, 10.18it/s]
 52%|█████▏    | 14/27 [00:01<00:01, 10.15it/s]
 59%|█████▉    | 16/27 [00:01<00:01, 10.19it/s]
 67%|██████▋   | 18/27 [00:01<00:00, 10.24it/s]
 74%|███████▍  | 20/27 [00:01<00:00, 10.23it/s]
 81%|████████▏ | 22/27 [00:02<00:00, 10.22it/s]
 89%|████████▉ | 24/27 [00:02<00:00, 10.23it/s]
 96%|█████████▋| 26/27 [00:02<00:00, 10.20it/s]
100%|██████████| 27/27 [00:02<00:00, 10.47it/s]

  0%|          | 0/365 [00:00<?, ?it/s]
100%|██████████| 365/365 [00:35<00:00, 10.35it/s]

  0%|          | 0/46668 [00:00<?, ?it/s]
  2%|▏         | 1001/46668 [00:23<17:33, 43.34it/s]
   Wfisher | S2 | Ftot=3332 | Ret F/R/T: 51.83/40.73/42.10 | Unl F/R/T: 48.17/39.52/40.52 | ΔF:-3.66 ΔR: 1.21 ΔT: 1.58 | MIA:0.4924 PredDiff:53.92%
[TRAIN] Retrain stage 3 on 45000 samples
    Epoch 1/1  32.84s
[UNLEARN] Wfisher stage 3 (|forget_total|=5000)

  0%|          | 0/40 [00:00<?, ?it/s]
  5%|▌         | 2/40 [00:00<00:03, 10.82it/s]
 10%|█         | 4/40 [00:00<00:03, 11.21it/s]
 15%|█▌        | 6/40 [00:00<00:03, 11.25it/s]
 20%|██        | 8/40 [00:00<00:02, 11.35it/s]
 25%|██▌       | 10/40 [00:00<00:02, 11.32it/s]
 30%|███       | 12/40 [00:01<00:02, 11.36it/s]
 35%|███▌      | 14/40 [00:01<00:02, 11.33it/s]
 40%|████      | 16/40 [00:01<00:02, 11.20it/s]
 45%|████▌     | 18/40 [00:01<00:01, 11.30it/s]
 50%|█████     | 20/40 [00:01<00:01, 11.30it/s]
 55%|█████▌    | 22/40 [00:01<00:01, 11.32it/s]
 60%|██████    | 24/40 [00:02<00:01, 11.31it/s]
 65%|██████▌   | 26/40 [00:02<00:01, 11.35it/s]
 70%|███████   | 28/40 [00:02<00:01, 11.31it/s]
 75%|███████▌  | 30/40 [00:02<00:00, 11.31it/s]
 80%|████████  | 32/40 [00:02<00:00, 11.25it/s]
 85%|████████▌ | 34/40 [00:03<00:00, 11.25it/s]
 90%|█████████ | 36/40 [00:03<00:00, 11.20it/s]
 95%|█████████▌| 38/40 [00:03<00:00, 11.06it/s]
100%|██████████| 40/40 [00:03<00:00, 11.88it/s]
100%|██████████| 40/40 [00:03<00:00, 11.36it/s]

  0%|          | 0/352 [00:00<?, ?it/s]
100%|██████████| 352/352 [00:30<00:00, 11.43it/s]

  0%|          | 0/45000 [00:00<?, ?it/s]
  2%|▏         | 1001/45000 [00:23<17:02, 43.03it/s]
   Wfisher | S3 | Ftot=5000 | Ret F/R/T: 41.90/42.00/41.68 | Unl F/R/T: 38.80/40.23/40.44 | ΔF:-3.10 ΔR: 1.77 ΔT: 1.24 | MIA:0.5024 PredDiff:52.52%

===== Running Method: SCRUB =====
[TRAIN] Retrain stage 1 on 48334 samples
    Epoch 1/1  39.90s
[UNLEARN] SCRUB stage 1 (|forget_total|=1666)
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Epoch #0, Learning rate: 0.001
len(r_loader): 378, len(f_loader): 14
/root/miniconda3/envs/myenv/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
*** Maximize step ***
Epoch: [0][13/14]	Time 0.048 (0.125)	Data 0.002 (0.082)	Loss -6.9451 (-0.1124)	Forget_Acc@1 0.000 (54.802)
*** Minimize step ***
Epoch: [0][377/378]	Time 0.077 (0.112)	Data 0.046 (0.075)	Loss 1.5116 (1.6598)	Retain_Acc@1 43.590 (39.891)
Epoch: [0]	 train-acc:	39.891173916421025	 train-loss: 1.659845676026317
one epoch duration:43.962544441223145
     SCRUB | S1 | Ftot=1666 | Ret F/R/T: 65.01/43.00/43.37 | Unl F/R/T: 63.75/42.98/44.62 | ΔF:-1.26 ΔR: 0.02 ΔT: 1.25 | MIA:0.4574 PredDiff:43.18%
[TRAIN] Retrain stage 2 on 46668 samples
    Epoch 1/1  36.09s
[UNLEARN] SCRUB stage 2 (|forget_total|=3332)
Epoch #0, Learning rate: 0.001
len(r_loader): 365, len(f_loader): 27
*** Maximize step ***
Epoch: [0][26/27]	Time 0.037 (0.105)	Data 0.003 (0.069)	Loss -6.7403 (-0.1447)	Forget_Acc@1 50.000 (50.120)
*** Minimize step ***
Epoch: [0][364/365]	Time 0.086 (0.107)	Data 0.050 (0.072)	Loss 1.8084 (1.6437)	Retain_Acc@1 42.105 (41.365)
Epoch: [0]	 train-acc:	41.3645324412763	 train-loss: 1.643687217372686
one epoch duration:42.04708528518677
     SCRUB | S2 | Ftot=3332 | Ret F/R/T: 48.02/38.39/38.96 | Unl F/R/T: 54.80/43.99/45.70 | ΔF:+6.78 ΔR: 5.60 ΔT: 6.74 | MIA:0.4792 PredDiff:42.42%
[TRAIN] Retrain stage 3 on 45000 samples
    Epoch 1/1  34.64s
[UNLEARN] SCRUB stage 3 (|forget_total|=5000)
Epoch #0, Learning rate: 0.001
len(r_loader): 352, len(f_loader): 40
*** Maximize step ***
Epoch: [0][39/40]	Time 0.039 (0.099)	Data 0.005 (0.064)	Loss -1.8634 (-0.0579)	Forget_Acc@1 50.000 (41.780)
*** Minimize step ***
Epoch: [0][351/352]	Time 0.082 (0.104)	Data 0.046 (0.069)	Loss 1.6538 (1.5889)	Retain_Acc@1 45.833 (42.669)
Epoch: [0]	 train-acc:	42.66888888685438	 train-loss: 1.5889015305413141
one epoch duration:40.540966749191284
     SCRUB | S3 | Ftot=5000 | Ret F/R/T: 38.62/39.62/40.13 | Unl F/R/T: 44.32/45.42/46.22 | ΔF:+5.70 ΔR: 5.79 ΔT: 6.09 | MIA:0.5005 PredDiff:45.50%

===== Full Results =====
  method  stage  forget_total  Retrain_F  Retrain_R  Retrain_T  Unlearn_F  Unlearn_R  Unlearn_T      ΔF     ΔR     ΔT      MIA  PredDiff(%)
      FT      1          1666      65.01      43.00      43.37      64.89      43.18      44.37   -0.12   0.18   1.00   0.4645        43.21
      FT      2          3332      44.00      36.22      37.30      54.50      44.09      45.75   10.50   7.87   8.45   0.4771        46.95
      FT      3          5000      40.52      41.63      41.28      43.98      45.37      45.82    3.46   3.74   4.54   0.5010        34.63
   FT_l1      1          1666      65.31      43.01      44.02      64.05      43.12      44.36   -1.26   0.11   0.34   0.4619        39.65
   FT_l1      2          3332      51.14      40.68      41.78      54.53      44.07      45.60    3.39   3.39   3.82   0.4831        41.05
   FT_l1      3          5000      42.58      43.95      43.33      44.54      45.63      46.08    1.96   1.68   2.75   0.5043        37.49
      GA      1          1666      65.07      42.70      43.72      55.16      37.71      38.94   -9.90   4.98   4.78   0.4793        57.44
      GA      2          3332      49.82      39.79      40.68      43.58      36.11      36.51   -6.24   3.69   4.17   0.4751        62.74
      GA      3          5000      39.74      39.96      40.46      33.38      34.14      34.05   -6.36   5.82   6.41   0.4994        64.96
      NG      1          1666      63.03      42.15      42.52      12.00      14.45      14.26  -51.02  27.70  28.26   0.4873        76.99
      NG      2          3332      52.55      42.77      42.75      11.82      13.94      13.84  -40.73  28.83  28.91   0.5000        91.20
      NG      3          5000      39.94      41.69      41.20      14.54      14.28      14.25  -25.40  27.41  26.95   0.5017        88.02
      RL      1          1666      65.67      40.90      42.01      65.55      43.71      45.03   -0.12   2.80   3.02   0.4419        40.12
      RL      2          3332      50.69      41.80      42.31      54.74      44.31      45.77    4.05   2.50   3.46   0.4758        42.33
      RL      3          5000      39.02      40.51      40.19      44.26      45.34      46.01    5.24   4.83   5.82   0.4998        42.45
 Wfisher      1          1666      61.52      41.01      41.56      58.46      39.51      40.37   -3.06   1.50   1.19   0.4610        54.13
 Wfisher      2          3332      51.83      40.73      42.10      48.17      39.52      40.52   -3.66   1.21   1.58   0.4924        53.92
 Wfisher      3          5000      41.90      42.00      41.68      38.80      40.23      40.44   -3.10   1.77   1.24   0.5024        52.52
   SCRUB      1          1666      65.01      43.00      43.37      63.75      42.98      44.62   -1.26   0.02   1.25   0.4574        43.18
   SCRUB      2          3332      48.02      38.39      38.96      54.80      43.99      45.70    6.78   5.60   6.74   0.4792        42.42
   SCRUB      3          5000      38.62      39.62      40.13      44.32      45.42      46.22    5.70   5.79   6.09   0.5005        45.50

Results saved to saved_models/experiment_results.csv
